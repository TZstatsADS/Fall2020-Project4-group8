{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Import Required Packages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the cell below doesn't run then do 'pip install rpy2' or 'conda install -c r rpy2' and 'conda install tzlocal' in Anaconda Prompt\n",
    "#### Change the paths for os.environ below to match your R folder directory and version if you get error messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rpy2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9abdf7e168ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mrpy2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mrpy2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobjects\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrobjects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rpy2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import rpy2\n",
    "\n",
    "try:\n",
    "    import rpy2.robjects as robjects\n",
    "except:\n",
    "    os.environ[\"R_HOME\"] = r\"C:\\Program Files\\R\\R-4.0.2\"\n",
    "    os.environ[\"PATH\"]   = r\"C:\\Program Files\\R\\R-4.0.2\\bin\\x64\" + \";\" + os.environ[\"PATH\"]\n",
    "    import rpy2.robjects as robjects\n",
    "    \n",
    "import rpy2.robjects.packages as rpackages\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "from rpy2.robjects.vectors import StrVector\n",
    "from rpy2.robjects import FloatVector, Formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run pip install tabulate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "import math\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Read the files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first read in the low dimensional dataset and high dimensional dataset from the data folder. These have feature columns with a prefix starting with ‘V’, a treatment/control column, and a continuous response Y. \n",
    "\n",
    "For this project, we know that the true ATE for the low dimensional data is 2.5, and for the high dimensional data it’s -3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowDim_dataset = pd.read_csv('../data/lowDim_dataset.csv')\n",
    "highDim_dataset = pd.read_csv('../data/highDim_dataset.csv')\n",
    "\n",
    "lowDim_true_ATE = 2.5\n",
    "highDim_true_ATE = -3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Calculate Propensity and Linear Propensity Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the propensity scores, we first need to fit a GBM classifier on the features and the binary response A, which indicates if a person is in the control group (0) or the treatment group (1). To get the optimal parameters for the GBM without overfitting, we performed a grid search. The two cells below are commented out due to the long runtime it takes to perform the grid search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#low dim grid search (commented out since it takes a few minutes to run)\n",
    "\n",
    "#X=lowDim_dataset.iloc[:,2:].values\n",
    "#A=lowDim_dataset['A'].values\n",
    "#Y=lowDim_dataset['Y'].values\n",
    "\n",
    "#params = {'learning_rate':[0.01,0.05,0.1,0.5], 'max_depth': [1,2,3,4], 'n_estimators':[50,100,150],\n",
    "#          'min_samples_leaf':[1,3,5],'min_samples_split':[2,4,6]}\n",
    "#gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=5).fit(X,A)\n",
    "#gscv.best_params_\n",
    "\n",
    "#output: {'learning_rate': 0.01,\n",
    "# 'max_depth': 2,\n",
    "# 'min_samples_leaf': 1,\n",
    "# 'min_samples_split': 2,\n",
    "# 'n_estimators': 150}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#high dim grid search (commented out since it takes a few minutes to run)\n",
    "\n",
    "#X=highDim_dataset.iloc[:,2:].values\n",
    "#A=highDim_dataset['A'].values\n",
    "#Y=highDim_dataset['Y'].values\n",
    "\n",
    "#params = {'learning_rate':[0.01,0.05,0.1,0.5], 'max_depth': [1,2,3,4], 'n_estimators':[50,100,150],\n",
    "#          'min_samples_leaf':[1,3,5],'min_samples_split':[2,4,6]}\n",
    "#gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=5).fit(X,A)\n",
    "#gscv.best_params_\n",
    "\n",
    "\n",
    "#output: {'learning_rate': 0.05,\n",
    "# 'max_depth': 1,\n",
    "# 'min_samples_leaf': 5,\n",
    "# 'min_samples_split': 2,\n",
    "# 'n_estimators': 100}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the logit function, which is used to get linear propensity scores from the propensity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(x):\n",
    "    return math.log(x/(1-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the parameters we got from the grid search to get the propensity scores and linear propensity scores from the low and high dimensional datasets. We save the scores to csv files in the output folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Dimensional Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=lowDim_dataset.iloc[:,2:].values\n",
    "A=lowDim_dataset['A'].values\n",
    "Y=lowDim_dataset['Y'].values\n",
    "\n",
    "gbm = GradientBoostingClassifier(learning_rate = 0.01, max_depth = 2, min_samples_leaf = 1,\n",
    "                                min_samples_split = 2, n_estimators = 150).fit(X,A)\n",
    "\n",
    "low_dim_propensity_scores = [x[1] for x in gbm.predict_proba(X)]\n",
    "low_dim_linear_propensity_scores = [logit(x) for x in low_dim_propensity_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowDim_dataset_propensity = lowDim_dataset.copy(deep=True)\n",
    "lowDim_dataset_propensity['propensity_score'] = low_dim_propensity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowDim_dataset_linear_propensity = lowDim_dataset.copy(deep=True)\n",
    "lowDim_dataset_linear_propensity['linear_propensity_score'] = low_dim_linear_propensity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'propensity_scores':low_dim_propensity_scores}).to_csv('../output/low_dim_propensity_scores.csv')\n",
    "pd.DataFrame({'linear_propensity_scores':low_dim_linear_propensity_scores}).to_csv('../output/low_dim_linear_propensity_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Dimensional Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=highDim_dataset.iloc[:,2:].values\n",
    "A=highDim_dataset['A'].values\n",
    "Y=highDim_dataset['Y'].values\n",
    "\n",
    "gbm = GradientBoostingClassifier(learning_rate = 0.05, max_depth = 1, min_samples_leaf = 5,\n",
    "                                min_samples_split = 2, n_estimators = 100).fit(X,A)\n",
    "\n",
    "high_dim_propensity_scores = [x[1] for x in gbm.predict_proba(X)]\n",
    "high_dim_linear_propensity_scores = [logit(x) for x in high_dim_propensity_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "highDim_dataset_propensity = highDim_dataset.copy(deep=True)\n",
    "highDim_dataset_propensity['propensity_score'] = high_dim_propensity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "highDim_dataset_linear_propensity = highDim_dataset.copy(deep=True)\n",
    "highDim_dataset_linear_propensity['linear_propensity_score'] = high_dim_linear_propensity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'propensity_scores':high_dim_propensity_scores}).to_csv('../output/high_dim_propensity_scores.csv')\n",
    "pd.DataFrame({'linear_propensity_scores':high_dim_linear_propensity_scores}).to_csv('../output/high_dim_linear_propensity_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Perform Full Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full matching is a way to break the dataset into subsets so that each subset has at least one treatment member and at least one control member. Subsets are created based on how close people are in terms of a specific distance metric, and the subsets do not have to be the same size. Instead, treated individuals who are close to many comparison individuals will be grouped with many comparison individuals, and treated individuals with few similar comparison individuals will be grouped with fewer comparison individuals.\n",
    "\n",
    "Full matching minimizes the sum of the distances between all pairs of treated and comparison individuals within each matched set, across all matched sets : \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The motivation behind full matching is that people within the same subset are ideally similar enough to each other that their response values can serve as counterfactuals for each other (e.g. if person A received treatment and is in the same subset as person B who was under control, then the response value for B would be close to the response value for A if A had been under control instead of treatment and vice versa). Once we’ve created the subsets, we can estimate the ATE by taking a weighted average of all of the differences between mean treatment response and mean control response within the subsets. \n",
    "\n",
    "One problem with full matching is that it sometimes leads to matched sets with widely varying ratios of treated to comparison individuals, which can lead to large variance of the resulting effect estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up rpy2 (Python Interface to R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement full matching, we use the fullmatch function from the R package optmatch. To use the function, we first set up the Python interface to R which will install the necessary packages from CRAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "utils = importr('utils')\n",
    "utils.chooseCRANmirror(ind=1)\n",
    "packnames = ('optmatch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_to_install = [x for x in packnames if not rpackages.isinstalled(x)]\n",
    "if len(names_to_install) > 0:\n",
    "    utils.install_packages(StrVector(names_to_install))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "utils.chooseCRANmirror(ind=1)\n",
    "robjects.r(f'install.packages(\"{\"optmatch\"}\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optmatch = rpackages.importr('optmatch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we convert the pandas dataframes to R dataframes which are compatible with the fullmatch function and also keep track of the runtime it takes to do the conversion. \n",
    "\n",
    "The try-except block is to take into account of the fact that Windows and Mac do not have the same latest version of the rpy2 package, and the two different versions have different syntax for dataframe conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with localconverter(robjects.default_converter + pandas2ri.converter):\n",
    "    try:\n",
    "        lowDim_R_runtime = time.time()\n",
    "        lowDim_dataset_R = robjects.conversion.py2rpy(lowDim_dataset)\n",
    "        lowDim_R_runtime = time.time()-lowDim_R_runtime\n",
    "        \n",
    "        lowDim_propensity_R_runtime = time.time()\n",
    "        lowDim_dataset_propensity_R = robjects.conversion.py2rpy(lowDim_dataset_propensity)\n",
    "        lowDim_propensity_R_runtime = time.time()-lowDim_propensity_R_runtime\n",
    "        \n",
    "        lowDim_linear_propensity_R_runtime = time.time()\n",
    "        lowDim_dataset_linear_propensity_R = robjects.conversion.py2rpy(lowDim_dataset_linear_propensity)\n",
    "        lowDim_linear_propensity_R_runtime = time.time()-lowDim_linear_propensity_R_runtime\n",
    "        \n",
    "    except:\n",
    "        lowDim_R_runtime = time.time()\n",
    "        lowDim_dataset_R = pandas2ri.py2ri(lowDim_dataset)\n",
    "        lowDim_R_runtime = time.time()-lowDim_R_runtime\n",
    "        \n",
    "        lowDim_propensity_R_runtime = time.time()\n",
    "        lowDim_dataset_propensity_R = pandas2ri.py2ri(lowDim_dataset_propensity)\n",
    "        lowDim_propensity_R_runtime = time.time()-lowDim_propensity_R_runtime\n",
    "        \n",
    "        lowDim_linear_propensity_R_runtime = time.time()\n",
    "        lowDim_dataset_linear_propensity_R = pandas2ri.py2ri(lowDim_dataset_linear_propensity)\n",
    "        lowDim_linear_propensity_R_runtime = time.time()-lowDim_linear_propensity_R_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with localconverter(robjects.default_converter + pandas2ri.converter):\n",
    "    try:\n",
    "        highDim_R_runtime = time.time()\n",
    "        highDim_dataset_R = robjects.conversion.py2rpy(highDim_dataset)\n",
    "        highDim_R_runtime = time.time()-highDim_R_runtime\n",
    "        \n",
    "        highDim_propensity_R_runtime = time.time()\n",
    "        highDim_dataset_propensity_R = robjects.conversion.py2rpy(highDim_dataset_propensity)\n",
    "        highDim_propensity_R_runtime = time.time()-highDim_propensity_R_runtime\n",
    "        \n",
    "        highDim_linear_propensity_R_runtime = time.time()\n",
    "        highDim_dataset_linear_propensity_R = robjects.conversion.py2rpy(highDim_dataset_linear_propensity)\n",
    "        highDim_linear_propensity_R_runtime = time.time()-highDim_linear_propensity_R_runtime\n",
    "        \n",
    "    except:\n",
    "        highDim_R_runtime = time.time()\n",
    "        highDim_dataset_R = pandas2ri.py2ri(highDim_dataset)\n",
    "        highDim_R_runtime = time.time()-highDim_R_runtime\n",
    "        \n",
    "        highDim_propensity_R_runtime = time.time()\n",
    "        highDim_dataset_propensity_R = pandas2ri.py2ri(highDim_dataset_propensity)\n",
    "        highDim_propensity_R_runtime = time.time()-highDim_propensity_R_runtime\n",
    "        \n",
    "        highDim_linear_propensity_R_runtime = time.time()\n",
    "        highDim_dataset_linear_propensity_R = pandas2ri.py2ri(highDim_dataset_linear_propensity)\n",
    "        highDim_linear_propensity_R_runtime = time.time()-highDim_linear_propensity_R_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Mahalanobis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mahalanobis distance matrix is given by\n",
    "$$D_{ij} = (X_i-X_j)^T\\Sigma^{-1}(X_i-X_j)$$\n",
    "where $\\Sigma$ is the covariance matrix of $X$ in the pooled treatment and full control groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mahalanobis does not require propensity scores and instead uses the features and covariance matrix of the pooled treatment and full control groups to create a distance matrix. Intuitively, the Mahalanobis distance measures the distance of two points relative to the centroid of all of the data points with the axes being determined by the direction of greatest variance in the cloud of points. That is, we let the data itself determine the coordinate system. For uncorrelated variables, the covariance matrix becomes a diagonal matrix, so the Mahalanobis distance between two points is equal to their standardized Euclidean distance in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Low Dim Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mahalanbois generally works well, both in terms of runtime and having a low error, when there are relatively few covariates because the covariance matrix is easier to invert. In this case, it works well because it takes advantage of the correlations between different features for its distance calculation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "full_match_Mahalanobis_factor = optmatch.fullmatch(optmatch.match_on(Formula('A~.-Y'),data=lowDim_dataset_R,method='mahalanobis'),data=lowDim_dataset_R)\n",
    "lowDim_dataset['assign'] = list(full_match_Mahalanobis_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute ATE\n",
    "ATE_vec = []\n",
    "weights = []\n",
    "\n",
    "for i in range(max(list(full_match_Mahalanobis_factor))):\n",
    "    temp = lowDim_dataset.loc[lowDim_dataset['assign']==i+1]\n",
    "    \n",
    "    treatment_Y = temp.loc[temp['A']==1]['Y'].values\n",
    "    control_Y = temp.loc[temp['A']==0]['Y'].values\n",
    "    \n",
    "    ATE_vec.append(np.mean(treatment_Y)-np.mean(control_Y))\n",
    "    weights.append(len(treatment_Y)+len(control_Y))\n",
    "\n",
    "lowDim_mahalanobis_est_ATE = np.average(ATE_vec, weights = weights)\n",
    "\n",
    "end = time.time()\n",
    "lowDim_mahalanobis_match_runtime = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.461'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#runtime is time to convert to R data frame + time to do matching\n",
    "lowDim_mahalanobis_runtime = \"{:,.3f}\".format(lowDim_R_runtime+lowDim_mahalanobis_match_runtime)\n",
    "lowDim_mahalanobis_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.406\n"
     ]
    }
   ],
   "source": [
    "lowDim_mahalanobis_error = abs(lowDim_true_ATE-lowDim_mahalanobis_est_ATE)\n",
    "lowDim_mahalanobis_error =\"{:,.3f}\".format(lowDim_mahalanobis_error)\n",
    "print(lowDim_mahalanobis_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. High Dim Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mahalanobis has a higher error on the high dimensional dataset because the creation of the distance matrix views all of the interactions between features as equally important, so full matching with Mahalanobis tries to capture more of the multi way interactions, so it does not perform well when there are too many interactions to keep track of for the matching criteria. In terms of runtime, Mahalanobis also takes a significantly long time on high dimensional data due to the complexity of inverting the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optmatch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-47d47c8df7d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfull_match_Mahalanobis_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfullmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_on\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFormula\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'A~.-Y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhighDim_dataset_R\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mahalanobis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhighDim_dataset_R\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mhighDim_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'assign'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_match_Mahalanobis_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optmatch' is not defined"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "full_match_Mahalanobis_factor = optmatch.fullmatch(optmatch.match_on(Formula('A~.-Y'),data=highDim_dataset_R,method='mahalanobis'),data=highDim_dataset_R)\n",
    "highDim_dataset['assign'] = list(full_match_Mahalanobis_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute ATE\n",
    "ATE_vec = []\n",
    "weights = []\n",
    "\n",
    "for i in range(max(list(full_match_Mahalanobis_factor))):\n",
    "    temp = highDim_dataset.loc[highDim_dataset['assign']==i+1]\n",
    "    \n",
    "    treatment_Y = temp.loc[temp['A']==1]['Y'].values\n",
    "    control_Y = temp.loc[temp['A']==0]['Y'].values\n",
    "    \n",
    "    ATE_vec.append(np.mean(treatment_Y)-np.mean(control_Y))\n",
    "    weights.append(len(treatment_Y)+len(control_Y))\n",
    "    \n",
    "highDim_mahalanobis_est_ATE = np.average(ATE_vec, weights=weights)\n",
    "    \n",
    "end = time.time()\n",
    "highDim_mahalanobis_match_runtime = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'51.508'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highDim_mahalanobis_runtime = \"{:,.3f}\".format(highDim_R_runtime+highDim_mahalanobis_match_runtime)\n",
    "highDim_mahalanobis_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.447\n"
     ]
    }
   ],
   "source": [
    "highDim_mahalanobis_error = abs(highDim_true_ATE-highDim_mahalanobis_est_ATE)\n",
    "highDim_mahalanobis_error= \"{:,.3f}\".format(highDim_mahalanobis_error)\n",
    "print(highDim_mahalanobis_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Propensity Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Low Dim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "full_match_propensity_factor = optmatch.fullmatch(optmatch.match_on(Formula('A~propensity_score'),data=lowDim_dataset_propensity_R,method='euclidean'),data=lowDim_dataset_propensity_R)\n",
    "lowDim_dataset_propensity['assign'] = list(full_match_propensity_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute ATE\n",
    "ATE_vec = []\n",
    "weights = []\n",
    "\n",
    "for i in range(max(list(full_match_propensity_factor))):\n",
    "    temp = lowDim_dataset_propensity.loc[lowDim_dataset_propensity['assign']==i+1]\n",
    "    \n",
    "    treatment_Y = temp.loc[temp['A']==1]['Y'].values\n",
    "    control_Y = temp.loc[temp['A']==0]['Y'].values\n",
    "    \n",
    "    ATE_vec.append(np.mean(treatment_Y)-np.mean(control_Y))\n",
    "    weights.append(len(treatment_Y)+len(control_Y))\n",
    "\n",
    "lowDim_propensity_est_ATE = np.average(ATE_vec, weights=weights)\n",
    "    \n",
    "end = time.time()\n",
    "lowDim_propensity_match_runtime = end-start\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.307'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowDim_propensity_runtime = \"{:,.3f}\".format(lowDim_propensity_R_runtime+lowDim_propensity_match_runtime)\n",
    "lowDim_propensity_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.888\n"
     ]
    }
   ],
   "source": [
    "lowDim_propensity_error = abs(lowDim_true_ATE-lowDim_propensity_est_ATE)\n",
    "lowDim_propensity_error =\"{:,.3f}\".format(lowDim_propensity_error)\n",
    "print(lowDim_propensity_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. High Dim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "full_match_propensity_factor = optmatch.fullmatch(optmatch.match_on(Formula('A~propensity_score'),data=highDim_dataset_propensity_R,method='euclidean'),data=highDim_dataset_propensity_R)\n",
    "highDim_dataset_propensity['assign'] = list(full_match_propensity_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute ATE\n",
    "ATE_vec = []\n",
    "weights = []\n",
    "\n",
    "for i in range(max(list(full_match_propensity_factor))):\n",
    "    temp = highDim_dataset_propensity.loc[highDim_dataset_propensity['assign']==i+1]\n",
    "    \n",
    "    treatment_Y = temp.loc[temp['A']==1]['Y'].values\n",
    "    control_Y = temp.loc[temp['A']==0]['Y'].values\n",
    "    \n",
    "    ATE_vec.append(np.mean(treatment_Y)-np.mean(control_Y))\n",
    "    weights.append(len(treatment_Y)+len(control_Y))\n",
    "\n",
    "highDim_propensity_est_ATE = np.average(ATE_vec, weights=weights)    \n",
    "\n",
    "end = time.time()\n",
    "highDim_propensity_match_runtime = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.227'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highDim_propensity_runtime = \"{:,.3f}\".format(highDim_propensity_R_runtime+highDim_propensity_match_runtime)\n",
    "highDim_propensity_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.292\n"
     ]
    }
   ],
   "source": [
    "highDim_propensity_error = abs(highDim_true_ATE-highDim_propensity_est_ATE)\n",
    "highDim_propensity_error =\"{:,.3f}\".format(highDim_propensity_error)\n",
    "print(highDim_propensity_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: Linear Propensity Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Low Dim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "full_match_linear_propensity_factor = optmatch.fullmatch(optmatch.match_on(Formula('A~linear_propensity_score'),data=lowDim_dataset_linear_propensity_R,method='euclidean'),data=lowDim_dataset_linear_propensity_R)\n",
    "lowDim_dataset_linear_propensity['assign'] = list(full_match_linear_propensity_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute ATE\n",
    "ATE_vec = []\n",
    "weights = []\n",
    "\n",
    "for i in range(max(list(full_match_linear_propensity_factor))):\n",
    "    temp = lowDim_dataset_linear_propensity.loc[lowDim_dataset_linear_propensity['assign']==i+1]\n",
    "    \n",
    "    treatment_Y = temp.loc[temp['A']==1]['Y'].values\n",
    "    control_Y = temp.loc[temp['A']==0]['Y'].values\n",
    "    \n",
    "    ATE_vec.append(np.mean(treatment_Y)-np.mean(control_Y))\n",
    "    weights.append(len(treatment_Y)+len(control_Y))\n",
    "\n",
    "lowDim_linear_propensity_est_ATE = np.average(ATE_vec, weights=weights)\n",
    "\n",
    "end = time.time()\n",
    "lowDim_linear_propensity_match_runtime = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.304'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowDim_linear_propensity_runtime = \"{:,.3f}\".format(lowDim_linear_propensity_R_runtime+lowDim_linear_propensity_match_runtime)\n",
    "lowDim_linear_propensity_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.976\n"
     ]
    }
   ],
   "source": [
    "lowDim_linear_propensity_error = abs(lowDim_true_ATE-lowDim_linear_propensity_est_ATE)\n",
    "lowDim_linear_propensity_error =\"{:,.3f}\".format(lowDim_linear_propensity_error)\n",
    "print(lowDim_linear_propensity_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. High Dim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "full_match_linear_propensity_factor = optmatch.fullmatch(optmatch.match_on(Formula('A~linear_propensity_score'),data=highDim_dataset_linear_propensity_R,\n",
    "                                                                           method='euclidean'),data=highDim_dataset_linear_propensity_R)\n",
    "highDim_dataset_linear_propensity['assign'] = list(full_match_linear_propensity_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute ATE\n",
    "ATE_vec = []\n",
    "weights = []\n",
    "\n",
    "for i in range(max(list(full_match_linear_propensity_factor))):\n",
    "    temp = highDim_dataset_linear_propensity.loc[highDim_dataset_linear_propensity['assign']==i+1]\n",
    "    \n",
    "    treatment_Y = temp.loc[temp['A']==1]['Y'].values\n",
    "    control_Y = temp.loc[temp['A']==0]['Y'].values\n",
    "    \n",
    "    ATE_vec.append(np.mean(treatment_Y)-np.mean(control_Y))\n",
    "    weights.append(len(treatment_Y)+len(control_Y))\n",
    "    \n",
    "highDim_linear_propensity_est_ATE=np.average(ATE_vec, weights=weights)\n",
    "\n",
    "end = time.time()\n",
    "highDim_linear_propensity_match_runtime = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.231'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highDim_linear_propensity_runtime = \"{:,.3f}\".format(highDim_linear_propensity_R_runtime+highDim_linear_propensity_match_runtime)\n",
    "highDim_linear_propensity_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.232\n"
     ]
    }
   ],
   "source": [
    "highDim_linear_propensity_error = abs(highDim_true_ATE-highDim_linear_propensity_est_ATE)\n",
    "highDim_linear_propensity_error =\"{:,.3f}\".format(highDim_linear_propensity_error)\n",
    "print(highDim_linear_propensity_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Inverse Propensity Weighting Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing the exposure effects between treatment groups, if we ignored those confounding factors, the effect estimates will be biased. Inverse probability weighting (IPW) based on the marginal structure model is an important method that can be used to estimate the effect of observational data processing and can address a very large number of confounding variables. Applying this weight when conducting statistical tests or regression models reduces or removes the impact of confounders. For inverse probability of treatment weighting (IPTW), we use propensity score as inverse weights in estimates of the ATE. \n",
    "\n",
    "The weight $w_i$ is \n",
    "$$w_i = \\frac{T_i}{\\hat{e_i}} + \\frac{1 - T_i}{1 - \\hat{e_i}} $$\n",
    "where $\\hat{e_i}$ is the estimated propensity score for individual $i$; $T_i$ is the treatment groups: $T_0$ is the controlled group and $T_1$ is after treatment group\n",
    "\n",
    "In this project, IPW does not work well in both low and high dimensional datasets.The reason might be the limitations of the Inverse Probability Weighted Estimator (IPWE). It can be unstable if estimated propensities are small. If the probability of either treatment assignment is small, then the logistic regression model can become unstable around the tails causing the IPWE to also be less stable. IPW needs to meet some prerequisites when applying, such as no omissions and unobserved confounding factors, non-negativity assumptions, stable unit processing value assumptions, and correct weight estimation models.\n",
    "\n",
    "The estimate ATE using IPW is \n",
    "$$\\hat{\\Delta}_{IPW} = N^{-1} (\\sum_{i \\in treated}{w_i Y_i} -\\sum_{i\\in controlled}{w_i Y_i} )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reset data & Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowDim_dataset = pd.read_csv('../data/lowDim_dataset.csv')\n",
    "highDim_dataset = pd.read_csv('../data/highDim_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipw_ate(dataset):\n",
    "    treated = 0\n",
    "    controlled = 0\n",
    "    for i in range(dataset.shape[0]):\n",
    "        if dataset['A'][i] == 1:\n",
    "            treated += dataset['Y'][i] * dataset['weight'][i]\n",
    "        else:\n",
    "            controlled += dataset['Y'][i] * dataset['weight'][i]\n",
    "\n",
    "    print(treated - controlled)\n",
    "    ate = (treated - controlled)/dataset.shape[0]\n",
    "    return ate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Low Dim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398.383166304905\n",
      "ATE for low dimension is:  0.8387014027471684\n",
      "Runtime for low dimension is:  0.28824400901794434\n",
      "ATE error for low dimension is:  1.661\n"
     ]
    }
   ],
   "source": [
    "runtime = time.time()\n",
    "X=lowDim_dataset.iloc[:,2:].values\n",
    "A=lowDim_dataset['A'].values\n",
    "gbm = GradientBoostingClassifier(learning_rate = 0.01, max_depth = 2, min_samples_leaf = 1,\n",
    "                                min_samples_split = 2, n_estimators = 150).fit(X,A)\n",
    "low_dim_propensity_scores = [x[1] for x in gbm.predict_proba(X)]\n",
    "lowDim_dataset_ipw = lowDim_dataset\n",
    "lowDim_dataset_ipw['score'] = low_dim_propensity_scores\n",
    "lowDim_dataset_ipw['weight'] = lowDim_dataset_ipw['A']/lowDim_dataset_ipw['score'] + (1 - lowDim_dataset_ipw['A'])/(1 - lowDim_dataset_ipw['score'])\n",
    "ate_low = ipw_ate(lowDim_dataset_ipw)\n",
    "runtime_low_ipw = time.time()-runtime\n",
    "lowDim_ipw_error = abs(ate_low - lowDim_true_ATE)\n",
    "lowDim_ipw_error = \"{:,.3f}\".format(lowDim_ipw_error)\n",
    "print(\"ATE for low dimension is: \", ate_low)\n",
    "print(\"Runtime for low dimension is: \", runtime_low_ipw)\n",
    "print(\"ATE error for low dimension is: \", lowDim_ipw_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. High Dim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3693.534034205346\n",
      "ATE for high dimension is:  -1.8467670171026729\n",
      "Runtime for high dimension is:  0.6256771087646484\n",
      "ATE error for high dimension is:  1.153\n"
     ]
    }
   ],
   "source": [
    "runtime = time.time()\n",
    "X=highDim_dataset.iloc[:,2:].values\n",
    "A=highDim_dataset['A'].values\n",
    "Y=highDim_dataset['Y'].values\n",
    "\n",
    "gbm = GradientBoostingClassifier(learning_rate = 0.05, max_depth = 1, min_samples_leaf = 5,\n",
    "                                min_samples_split = 2, n_estimators = 100).fit(X,A)\n",
    "high_dim_propensity_scores = [x[1] for x in gbm.predict_proba(X)]\n",
    "highDim_dataset_ipw = highDim_dataset\n",
    "highDim_dataset_ipw['score'] = high_dim_propensity_scores\n",
    "highDim_dataset_ipw['weight'] = highDim_dataset_ipw['A']/highDim_dataset_ipw['score'] + (1 - highDim_dataset_ipw['A'])/(1 - highDim_dataset_ipw['score'])\n",
    "ate_high = ipw_ate(highDim_dataset_ipw)\n",
    "runtime_high_ipw = time.time()-runtime\n",
    "highDim_ipw_error = abs(ate_high-highDim_true_ATE)\n",
    "highDim_ipw_error = \"{:,.3f}\".format(highDim_ipw_error)\n",
    "print(\"ATE for high dimension is: \", ate_high)\n",
    "print(\"Runtime for high dimension is: \", runtime_high_ipw)\n",
    "print(\"ATE error for high dimension is: \", highDim_ipw_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to stratify data \n",
    "def stratify(df):\n",
    "    \n",
    "    Y = df['Y']\n",
    "    D = df['A']\n",
    "    scores = df['propensity_scores']\n",
    "    \n",
    "    # Create stratum and stratum limits\n",
    "    Q1 = np.quantile(scores, .20)\n",
    "    Q2 = np.quantile(scores, .40)\n",
    "    Q3 = np.quantile(scores, .60)\n",
    "    Q4 = np.quantile(scores, .80)\n",
    "    Q5 = np.quantile(scores, 1.0)\n",
    "    \n",
    "    quin1 = df[df['propensity_scores']<= Q1]\n",
    "    quin2 = df[(df['propensity_scores']> Q1) & (df['propensity_scores']<= Q2)]\n",
    "    quin3 = df[(df['propensity_scores']> Q2) & (df['propensity_scores']<= Q3)]\n",
    "    quin4 = df[(df['propensity_scores']> Q3) & (df['propensity_scores']<= Q4)]\n",
    "    quin5 = df[df['propensity_scores']> Q4]\n",
    "\n",
    "    quintiles = [quin1, quin2, quin3, quin4, quin5]\n",
    "    Q_ranges = [None, Q1, Q2, Q3, Q4, Q5]\n",
    "\n",
    "    return [quintiles, Q_ranges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to calc ATE\n",
    "def strat_ATE(quintiles, Q_ranges):\n",
    "    results = []\n",
    "    N = sum([len(quintiles[0]),len(quintiles[1]),len(quintiles[2]),len(quintiles[3]),len(quintiles[4])])\n",
    "    \n",
    "    for i, stratum in enumerate(quintiles): \n",
    "        i+=1\n",
    "        \n",
    "        Nj = len(stratum)                      # Number of ind in stratum\n",
    "        N1j = stratum['A'].value_counts()[1]   # Number of treated ind\n",
    "        N0j = stratum['A'].value_counts()[0]   # Number of control ind\n",
    "        \n",
    "        sum1 = 0\n",
    "        sum2 = 0\n",
    "\n",
    "        # Summation of treated samples within strata\n",
    "        sum1 = sum([Y*T for Y,T in zip(stratum['Y'],stratum['A'])])\n",
    "        # Summation of untreated samples within strata\n",
    "        sum2 = sum([(1-T)*Y for Y,T in zip(stratum['Y'],stratum['A'])]) \n",
    "\n",
    "        results.append(Nj/N * ((sum1/N1j)-(sum2/N0j)))\n",
    "\n",
    "    return sum(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowDim_dataset = pd.read_csv('../data/lowDim_dataset.csv')\n",
    "highDim_dataset = pd.read_csv('../data/highDim_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Low Dim data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowDim_scores = pd.read_csv('../output/low_dim_propensity_scores.csv') \n",
    "lowDim_scores.insert( 1 , \"Y\" , lowDim_dataset['Y']) \n",
    "lowDim_scores.insert( 2 , \"A\" , lowDim_dataset['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated ATE:  2.463529123502176\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "quintiles , Q_ranges = stratify(lowDim_scores) \n",
    "\n",
    "lowDim_stratification_ATE = strat_ATE(quintiles ,Q_ranges)\n",
    "end = time.time()\n",
    "lowdim_strat_runtime = end - start\n",
    "print( \"Estimated ATE: \" , lowDim_stratification_ATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.036\n",
      "0.015\n"
     ]
    }
   ],
   "source": [
    "lowdim_strat_runtime = \"{:,.3f}\".format(lowdim_strat_runtime)\n",
    "#lowDim_stratification_ATE = \"{:,.3f}\".format(lowDim_stratification_ATE)\n",
    "#print(lowDim_stratification_ATE)\n",
    "lowDim_stratification_error = abs(lowDim_stratification_ATE-lowDim_true_ATE)\n",
    "lowDim_stratification_error = \"{:,.3f}\".format(lowDim_stratification_error)\n",
    "print(lowDim_stratification_error)\n",
    "print(lowdim_strat_runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. High Dim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Calculated Propensity Scores \n",
    "highDim_scores = pd.read_csv('../output/high_dim_propensity_scores.csv') \n",
    "highDim_scores.insert( 1 , \"Y\" , highDim_dataset['Y']) \n",
    "highDim_scores.insert( 2 , \"A\" , highDim_dataset['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "quintiles , Q_ranges = stratify(highDim_scores)\n",
    "\n",
    "highDim_stratification_ATE = strat_ATE(quintiles ,Q_ranges)\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010\n",
      "0.014\n"
     ]
    }
   ],
   "source": [
    "highdim_strat_runtime=\"{:,.3f}\".format(end-start)\n",
    "#highDim_stratification_ATE = \"{:,.3f}\".format(highDim_stratification_ATE)\n",
    "#print(highDim_stratification_ATE)\n",
    "highDim_stratification_error = abs(highDim_stratification_ATE-highDim_true_ATE)\n",
    "highDim_stratification_error = \"{:,.3f}\".format(highDim_stratification_error)\n",
    "print(highDim_stratification_error)\n",
    "print(highdim_strat_runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have a table of all of the absolute errors and the runtimes for each method across the low dimensional and high dimensional datasets. Note that this notebook was run on a Mac with 64 GB of RAM and an i9 CPU, so the runtimes may be different if you run this notebook on your system.\n",
    "\n",
    "We can see that the overall best method on the low dimensional dataset and the high dimensional dataset in terms of runtime and performance is stratification. The reason is because…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Metric          </th><th>Dimension  </th><th>Full Matching-\n",
       "Mahalanobis       </th><th>Full Matching-\n",
       "Propensity score       </th><th>Full Matching-\n",
       "Linear Propensity Score       </th><th>Inverse Propensity\n",
       "Weighting        </th><th>Stratification  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>Best ATE score  </td><td>Low Dim    </td><td>0.406 </td><td>0.888 </td><td>0.976 </td><td>0.705  </td><td>0.036           </td></tr>\n",
       "<tr><td>                </td><td>High Dim   </td><td>1.447 </td><td>0.292 </td><td>0.232 </td><td>0.947  </td><td>0.010           </td></tr>\n",
       "<tr><td>Run Time (sec)  </td><td>Low Dim    </td><td>0.461 </td><td>0.307 </td><td>0.304 </td><td>52.604 </td><td>0.015           </td></tr>\n",
       "<tr><td>                </td><td>High Dim   </td><td>51.508</td><td>5.227 </td><td>5.231 </td><td>372.688</td><td>0.014           </td></tr>\n",
       "<tr><td>Computer Used   </td><td>           </td><td>PC    </td><td>PC    </td><td>PC    </td><td>PC     </td><td>PC              </td></tr>\n",
       "<tr><td>Stable/Nonstable</td><td>           </td><td>Stable</td><td>Stable</td><td>Stable</td><td>Stable </td><td>Stable          </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table = [[\"ATE Absolute Error\",'Low Dim',lowDim_mahalanobis_error,lowDim_propensity_error,\n",
    "         lowDim_linear_propensity_error,lowDim_ipw_error,lowDim_stratification_error],\n",
    "        [\"\",'High Dim',highDim_mahalanobis_error,highDim_propensity_error,\n",
    "         highDim_linear_propensity_error,highDim_ipw_error,highDim_stratification_error],\n",
    "        [\"Run Time (sec)\",'Low Dim',lowDim_mahalanobis_runtime,lowDim_propensity_runtime,lowDim_linear_propensity_runtime,\n",
    "         runtime_low_ipw, lowdim_strat_runtime],\n",
    "        [\"\",'High Dim',highDim_mahalanobis_runtime,highDim_propensity_runtime,highDim_linear_propensity_runtime,\n",
    "         runtime_high_ipw, highdim_strat_runtime],\n",
    "        [\"Computer Used\",'','PC','PC','PC','PC','PC'],\n",
    "        [\"Stable/Nonstable\",'','Stable','Stable','Stable','Stable','Stable']]\n",
    "\n",
    "display(HTML(tabulate.tabulate(table, headers=[\"Metric\",\"Dimension\", \"Full Matching-\\nMahalanobis\",\n",
    "                                               \"Full Matching-\\nPropensity score\", \"Full Matching-\\nLinear Propensity Score\",\n",
    "                                               \"Inverse Propensity\\nWeighting\", 'Stratification'],\n",
    "                                tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Papers (alphabetize later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stuart E. A. (2010). Matching methods for causal inference: A review and a look forward. Statistical science : a review journal of the Institute of Mathematical Statistics, 25(1), 1–21. https://doi.org/10.1214/09-STS313\n",
    "\n",
    "Chan, D., Ge, R., Gershony, O., Hesterberg, T., & Lambert, D. (2010). Evaluating online ad campaigns in a pipeline: causal models at scale. KDD '10.\n",
    "\n",
    "Stuart, E. A., & Green, K. M. (2008). Using full matching to estimate causal effects in nonexperimental studies: examining the relationship between adolescent marijuana use and adult outcomes. Developmental psychology, 44(2), 395–406. https://doi.org/10.1037/0012-1649.44.2.395\n",
    "\n",
    "McCaffrey, D. F., Ridgeway, G., & Morral, A. R. (2004). Propensity score estimation with boosted regression for evaluating causal effects in observational studies. Psychological methods, 9(4), 403–425. https://doi.org/10.1037/1082-989X.9.4.403\n",
    "\n",
    "Raad, H., Cornelius, V., Chan, S., Williamson, E. and Cro, S., 2020. An evaluation of inverse probability weighting using the propensity score for baseline covariate adjustment in smaller population randomised controlled trials with a continuous outcome. BMC Medical Research Methodology, 20(1), pp.1-12. https://link.springer.com/article/10.1186/s12874-020-00947-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
