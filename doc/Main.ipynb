{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Import Required Packages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the cell below doesn't run then do 'pip install rpy2' \n",
    "#### Change the paths for os.environ below to match your R folder directory and version if you get error messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rpy2\n",
    "\n",
    "try:\n",
    "    import rpy2.robjects as robjects\n",
    "except:\n",
    "    os.environ[\"R_HOME\"] = r\"C:\\Program Files\\R\\R-4.0.2\"\n",
    "    os.environ[\"PATH\"]   = r\"C:\\Program Files\\R\\R-4.0.2\\bin\\x64\" + \";\" + os.environ[\"PATH\"]\n",
    "    import rpy2.robjects as robjects\n",
    "    \n",
    "import rpy2.robjects.packages as rpackages\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "from rpy2.robjects.vectors import StrVector\n",
    "from rpy2.robjects import FloatVector, Formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run pip install tabulate if the cell below does not run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import math\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Read the files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first read in the low dimensional dataset and high dimensional dataset from the data folder. These have feature columns with a prefix ‘V’, a treatment/control column 'A', and a continuous response column 'Y'. \n",
    "\n",
    "For this project, we know that the true ATE for the low dimensional data is 2.5, and for the high dimensional data it’s -3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowDim_dataset = pd.read_csv('../data/lowDim_dataset.csv')\n",
    "highDim_dataset = pd.read_csv('../data/highDim_dataset.csv')\n",
    "\n",
    "lowDim_true_ATE = 2.5\n",
    "highDim_true_ATE = -3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Calculate Propensity and Linear Propensity Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the propensity scores, we first need to fit a GBM classifier on the features and the binary response A, which indicates if a person is in the control group (0) or the treatment group (1). To get the optimal parameters for the GBM without overfitting, we performed a grid search. The two cells below are commented out due to the long runtime it takes to perform the grid search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#low dim grid search (commented out since it takes a few minutes to run)\n",
    "\n",
    "#X=lowDim_dataset.iloc[:,2:].values\n",
    "#A=lowDim_dataset['A'].values\n",
    "#Y=lowDim_dataset['Y'].values\n",
    "\n",
    "#params = {'learning_rate':[0.01,0.05,0.1,0.5], 'max_depth': [1,2,3,4], 'n_estimators':[50,100,150],\n",
    "#          'min_samples_leaf':[1,3,5],'min_samples_split':[2,4,6]}\n",
    "#gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=5).fit(X,A)\n",
    "#gscv.best_params_\n",
    "\n",
    "#output: {'learning_rate': 0.01,\n",
    "# 'max_depth': 2,\n",
    "# 'min_samples_leaf': 1,\n",
    "# 'min_samples_split': 2,\n",
    "# 'n_estimators': 150}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#high dim grid search (commented out since it takes a few minutes to run)\n",
    "\n",
    "#X=highDim_dataset.iloc[:,2:].values\n",
    "#A=highDim_dataset['A'].values\n",
    "#Y=highDim_dataset['Y'].values\n",
    "\n",
    "#params = {'learning_rate':[0.01,0.05,0.1,0.5], 'max_depth': [1,2,3,4], 'n_estimators':[50,100,150],\n",
    "#          'min_samples_leaf':[1,3,5],'min_samples_split':[2,4,6]}\n",
    "#gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=5).fit(X,A)\n",
    "#gscv.best_params_\n",
    "\n",
    "\n",
    "#output: {'learning_rate': 0.05,\n",
    "# 'max_depth': 1,\n",
    "# 'min_samples_leaf': 5,\n",
    "# 'min_samples_split': 2,\n",
    "# 'n_estimators': 100}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the logit function, which is used to get linear propensity scores from the propensity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(x):\n",
    "    return math.log(x/(1-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the parameters we got from the grid search to get the propensity scores and linear propensity scores from the low and high dimensional datasets. We save the scores to csv files in the output folder.\n",
    "\n",
    "Note that the propensity score of an individual is the class probability that the individual is in the treatment group, and the probabilities are determined by the GBM classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Dimensional Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=lowDim_dataset.iloc[:,2:].values\n",
    "A=lowDim_dataset['A'].values\n",
    "Y=lowDim_dataset['Y'].values\n",
    "\n",
    "gbm = GradientBoostingClassifier(learning_rate = 0.01, max_depth = 2, min_samples_leaf = 1,\n",
    "                                min_samples_split = 2, n_estimators = 150).fit(X,A)\n",
    "\n",
    "low_dim_propensity_scores = [x[1] for x in gbm.predict_proba(X)]\n",
    "low_dim_linear_propensity_scores = [logit(x) for x in low_dim_propensity_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowDim_dataset_propensity = lowDim_dataset.copy(deep=True)\n",
    "lowDim_dataset_propensity['propensity_score'] = low_dim_propensity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowDim_dataset_linear_propensity = lowDim_dataset.copy(deep=True)\n",
    "lowDim_dataset_linear_propensity['linear_propensity_score'] = low_dim_linear_propensity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'propensity_scores':low_dim_propensity_scores}).to_csv('../output/low_dim_propensity_scores.csv')\n",
    "pd.DataFrame({'linear_propensity_scores':low_dim_linear_propensity_scores}).to_csv('../output/low_dim_linear_propensity_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Dimensional Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=highDim_dataset.iloc[:,2:].values\n",
    "A=highDim_dataset['A'].values\n",
    "Y=highDim_dataset['Y'].values\n",
    "\n",
    "gbm = GradientBoostingClassifier(learning_rate = 0.05, max_depth = 1, min_samples_leaf = 5,\n",
    "                                min_samples_split = 2, n_estimators = 100).fit(X,A)\n",
    "\n",
    "high_dim_propensity_scores = [x[1] for x in gbm.predict_proba(X)]\n",
    "high_dim_linear_propensity_scores = [logit(x) for x in high_dim_propensity_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "highDim_dataset_propensity = highDim_dataset.copy(deep=True)\n",
    "highDim_dataset_propensity['propensity_score'] = high_dim_propensity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "highDim_dataset_linear_propensity = highDim_dataset.copy(deep=True)\n",
    "highDim_dataset_linear_propensity['linear_propensity_score'] = high_dim_linear_propensity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'propensity_scores':high_dim_propensity_scores}).to_csv('../output/high_dim_propensity_scores.csv')\n",
    "pd.DataFrame({'linear_propensity_scores':high_dim_linear_propensity_scores}).to_csv('../output/high_dim_linear_propensity_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Perform Full Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full matching is a way to break the dataset into subsets so that each subset has at least one treatment member and at least one control member. Subsets are created based on how close people are in terms of a specific distance metric, and the subsets do not have to be the same size. Every individual must be placed into a subset.\n",
    "\n",
    "In particular, full matching creates subsets so that the sum of the distances between all pairs of treated and control individuals within each matched set, across all matched sets, is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more formal explanation of how full matching works, let $T$ and $C$ denote the treatment and control sets of individuals respectively. Suppose that for every pair $(i,j)$ with $i \\in T$ and $j \\in C$, a distance $\\delta_{ij} \\in \\mathbb{R}^+$ is given, where smaller values of $\\delta$ would indicate more desirable matches. \n",
    "\n",
    "Let $S$ be a mapping of $T \\cup C$ into $\\{1,...,K\\}$ ($K$ is the number of subsets that $S$ maps values to) where for each matched subset $M$, $M=S^{-1}[k]$ $(1 \\leq k \\leq K)$ is such that $min(|M \\cap T|, |M \\cap C|)=1$ (at least one treatment and at least one control individual in each subset) and $\\delta_{ij} <\\infty$ for all i,j.\n",
    "\n",
    "Then given a dataset of control individuals $C$, treatment individuals $T$, and distances $\\delta$, a full match $S'$ minimizes the below sum, which is the sum of distances between all pairs of treated and control individuals across all matches for a fixed matching function $S$.\n",
    "\n",
    "$$ \\sum_{i \\in T, S(i) > 0}^{}  \\sum_{j \\in C, S(i)=S(j)}^{}  \\delta_{ij}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The motivation behind full matching is that people within the same subset are ideally similar enough to each other that their response values can serve as counterfactuals for each other (e.g. if person A received treatment and is in the same subset as person B who was under control, then the response value for B would be close to the response value for A if A had been under control instead of treatment and vice versa). Once we’ve created the subsets, we can estimate the ATE by taking a weighted average of all of the differences between mean treatment response and mean control response within the subsets. \n",
    "\n",
    "Note that one problem with full matching is that it sometimes leads to subsets with widely varying ratios of treatment to control since we only require that at least one treatment or control be in each set. For example, it is possible to have one subset with five treatment and five control individuals, and another subset to have one treatment and nine control individuals. Hence, this possible large variance in ratios can lead to a large variance of differences between mean treatment response and mean control response across all of the subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up rpy2 (Python Interface to R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement full matching, we use the fullmatch function from the R package optmatch. To use the function, we first set up the Python interface to R which will install the necessary packages from CRAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "utils = importr('utils')\n",
    "utils.chooseCRANmirror(ind=1)\n",
    "packnames = ('optmatch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_to_install = [x for x in packnames if not rpackages.isinstalled(x)]\n",
    "if len(names_to_install) > 0:\n",
    "    utils.install_packages(StrVector(names_to_install))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "utils.chooseCRANmirror(ind=1)\n",
    "robjects.r(f'install.packages(\"{\"optmatch\"}\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optmatch = rpackages.importr('optmatch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we convert the pandas dataframes to R dataframes which are compatible with the fullmatch function and also keep track of the runtime it takes to do the conversion. \n",
    "\n",
    "The try-except block is to take into account of the fact that Windows and Mac do not have the same latest version of the rpy2 package, and the two different versions have different syntax for dataframe conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with localconverter(robjects.default_converter + pandas2ri.converter):\n",
    "    try:\n",
    "        lowDim_R_runtime = time.time()\n",
    "        lowDim_dataset_R = robjects.conversion.py2rpy(lowDim_dataset)\n",
    "        lowDim_R_runtime = time.time()-lowDim_R_runtime\n",
    "        \n",
    "        lowDim_propensity_R_runtime = time.time()\n",
    "        lowDim_dataset_propensity_R = robjects.conversion.py2rpy(lowDim_dataset_propensity)\n",
    "        lowDim_propensity_R_runtime = time.time()-lowDim_propensity_R_runtime\n",
    "        \n",
    "        lowDim_linear_propensity_R_runtime = time.time()\n",
    "        lowDim_dataset_linear_propensity_R = robjects.conversion.py2rpy(lowDim_dataset_linear_propensity)\n",
    "        lowDim_linear_propensity_R_runtime = time.time()-lowDim_linear_propensity_R_runtime\n",
    "        \n",
    "    except:\n",
    "        lowDim_R_runtime = time.time()\n",
    "        lowDim_dataset_R = pandas2ri.py2ri(lowDim_dataset)\n",
    "        lowDim_R_runtime = time.time()-lowDim_R_runtime\n",
    "        \n",
    "        lowDim_propensity_R_runtime = time.time()\n",
    "        lowDim_dataset_propensity_R = pandas2ri.py2ri(lowDim_dataset_propensity)\n",
    "        lowDim_propensity_R_runtime = time.time()-lowDim_propensity_R_runtime\n",
    "        \n",
    "        lowDim_linear_propensity_R_runtime = time.time()\n",
    "        lowDim_dataset_linear_propensity_R = pandas2ri.py2ri(lowDim_dataset_linear_propensity)\n",
    "        lowDim_linear_propensity_R_runtime = time.time()-lowDim_linear_propensity_R_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with localconverter(robjects.default_converter + pandas2ri.converter):\n",
    "    try:\n",
    "        highDim_R_runtime = time.time()\n",
    "        highDim_dataset_R = robjects.conversion.py2rpy(highDim_dataset)\n",
    "        highDim_R_runtime = time.time()-highDim_R_runtime\n",
    "        \n",
    "        highDim_propensity_R_runtime = time.time()\n",
    "        highDim_dataset_propensity_R = robjects.conversion.py2rpy(highDim_dataset_propensity)\n",
    "        highDim_propensity_R_runtime = time.time()-highDim_propensity_R_runtime\n",
    "        \n",
    "        highDim_linear_propensity_R_runtime = time.time()\n",
    "        highDim_dataset_linear_propensity_R = robjects.conversion.py2rpy(highDim_dataset_linear_propensity)\n",
    "        highDim_linear_propensity_R_runtime = time.time()-highDim_linear_propensity_R_runtime\n",
    "        \n",
    "    except:\n",
    "        highDim_R_runtime = time.time()\n",
    "        highDim_dataset_R = pandas2ri.py2ri(highDim_dataset)\n",
    "        highDim_R_runtime = time.time()-highDim_R_runtime\n",
    "        \n",
    "        highDim_propensity_R_runtime = time.time()\n",
    "        highDim_dataset_propensity_R = pandas2ri.py2ri(highDim_dataset_propensity)\n",
    "        highDim_propensity_R_runtime = time.time()-highDim_propensity_R_runtime\n",
    "        \n",
    "        highDim_linear_propensity_R_runtime = time.time()\n",
    "        highDim_dataset_linear_propensity_R = pandas2ri.py2ri(highDim_dataset_linear_propensity)\n",
    "        highDim_linear_propensity_R_runtime = time.time()-highDim_linear_propensity_R_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin the evaluation of each of the metrics, note that the evaluation criteria that we will be using for all of the different methods in the notebook will be measuring performance and runtime.\n",
    "\n",
    "Since the ATE is a scalar, performance will be measured with the absolute error of our estimated ATE and the true ATE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric 1: Mahalanobis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mahalanobis distance matrix is given by\n",
    "$$D_{ij} = (X_i-X_j)^T\\Sigma^{-1}(X_i-X_j)$$\n",
    "where $\\Sigma$ is the covariance matrix of $X$ in the pooled treatment and full control groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mahalanobis does not require propensity scores and instead uses the features and covariance matrix of the pooled treatment and full control groups to create a distance matrix. Intuitively, the Mahalanobis distance measures the distance of two points relative to the centroid of all of the data points with the axes being determined by the direction of greatest variance in the cloud of points. That is, we let the data itself determine the coordinate system. For uncorrelated variables, the covariance matrix becomes a diagonal matrix, so the Mahalanobis distance between two points is equal to their standardized Euclidean distance in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Low Dim Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mahalanbois generally works well, both in terms of runtime and having a low error, when there are relatively few features because the covariance matrix is easier to invert. In this case, it works well because it takes advantage of the correlations between different features for its distance calculation. \n",
    "\n",
    "In the cell below we use the optmatch package's fullmatch function to create a list of assignment labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "full_match_Mahalanobis_factor = optmatch.fullmatch(optmatch.match_on(Formula('A~.-Y'),data=lowDim_dataset_R,method='mahalanobis'),data=lowDim_dataset_R)\n",
    "lowDim_dataset['assign'] = list(full_match_Mahalanobis_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the ATE using the assignment labels to denote different subsets. As mentioned in the full matching description, we do so by going through each subset, taking the difference between the mean treatment response and mean control response, and then taking a weighted average of all of those differences where the weights are the length of the subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute ATE\n",
    "ATE_vec = []\n",
    "weights = []\n",
    "\n",
    "for i in range(max(list(full_match_Mahalanobis_factor))):\n",
    "    temp = lowDim_dataset.loc[lowDim_dataset['assign']==i+1]\n",
    "    \n",
    "    treatment_Y = temp.loc[temp['A']==1]['Y'].values\n",
    "    control_Y = temp.loc[temp['A']==0]['Y'].values\n",
    "    \n",
    "    ATE_vec.append(np.mean(treatment_Y)-np.mean(control_Y))\n",
    "    weights.append(len(treatment_Y)+len(control_Y))\n",
    "\n",
    "lowDim_mahalanobis_est_ATE = np.average(ATE_vec, weights = weights)\n",
    "\n",
    "end = time.time()\n",
    "lowDim_mahalanobis_match_runtime = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runtime is time to convert to R data frame + time to do matching\n",
    "lowDim_mahalanobis_runtime = \"{:,.3f}\".format(lowDim_R_runtime+lowDim_mahalanobis_match_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE for low dimension is:  2.906\n",
      "Runtime for low dimension is:  0.437\n",
      "ATE error for low dimension is:  0.406\n"
     ]
    }
   ],
   "source": [
    "lowDim_mahalanobis_error = abs(lowDim_true_ATE-lowDim_mahalanobis_est_ATE)\n",
    "lowDim_mahalanobis_error =\"{:,.3f}\".format(lowDim_mahalanobis_error)\n",
    "lowDim_mahalanobis_est_ATE =\"{:,.3f}\".format(lowDim_mahalanobis_est_ATE)\n",
    "\n",
    "print(\"ATE for low dimension is: \", lowDim_mahalanobis_est_ATE)\n",
    "print(\"Runtime for low dimension is: \", lowDim_mahalanobis_runtime)\n",
    "print(\"ATE error for low dimension is: \", lowDim_mahalanobis_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. High Dim Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mahalanobis has a higher error on the high dimensional dataset because the creation of the distance matrix views all of the interactions between features as equally important. Hence, full matching with Mahalanobis tries to capture more of the multi way interactions, so it does not perform well when there are too many interactions to keep track of for the matching criteria. Moreover, the complexity of inverting a high dimensional matrix also contributes to a decrease in performance as well as an increase in runtime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "full_match_Mahalanobis_factor = optmatch.fullmatch(optmatch.match_on(Formula('A~.-Y'),data=highDim_dataset_R,method='mahalanobis'),data=highDim_dataset_R)\n",
    "highDim_dataset['assign'] = list(full_match_Mahalanobis_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute ATE\n",
    "ATE_vec = []\n",
    "weights = []\n",
    "\n",
    "for i in range(max(list(full_match_Mahalanobis_factor))):\n",
    "    temp = highDim_dataset.loc[highDim_dataset['assign']==i+1]\n",
    "    \n",
    "    treatment_Y = temp.loc[temp['A']==1]['Y'].values\n",
    "    control_Y = temp.loc[temp['A']==0]['Y'].values\n",
    "    \n",
    "    ATE_vec.append(np.mean(treatment_Y)-np.mean(control_Y))\n",
    "    weights.append(len(treatment_Y)+len(control_Y))\n",
    "    \n",
    "highDim_mahalanobis_est_ATE = np.average(ATE_vec, weights=weights)\n",
    "    \n",
    "end = time.time()\n",
    "highDim_mahalanobis_match_runtime = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "highDim_mahalanobis_runtime = \"{:,.3f}\".format(highDim_R_runtime+highDim_mahalanobis_match_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE for high dimension is:  -1.553\n",
      "Runtime for high dimension is:  52.327\n",
      "ATE error for high dimension is:  1.447\n"
     ]
    }
   ],
   "source": [
    "highDim_mahalanobis_error = abs(highDim_true_ATE-highDim_mahalanobis_est_ATE)\n",
    "highDim_mahalanobis_error =\"{:,.3f}\".format(highDim_mahalanobis_error)\n",
    "highDim_mahalanobis_est_ATE =\"{:,.3f}\".format(highDim_mahalanobis_est_ATE)\n",
    "\n",
    "print(\"ATE for high dimension is: \", highDim_mahalanobis_est_ATE)\n",
    "print(\"Runtime for high dimension is: \", highDim_mahalanobis_runtime)\n",
    "print(\"ATE error for high dimension is: \", highDim_mahalanobis_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric 2: Propensity Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance of Propensity Score is defined as:\n",
    "                                            $$D_{ij}=|e_i-e_j|$$\n",
    "where $e_{k}$ is the propensity score for individual k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propensity scores are created with a GBM classifier fitted on the features and treatment/control column. The propensity scores are class probabilities of each individual being in the treatment group. Since GBM adaptively chooses variables to include as part of its algorithm, we fit it on all of the features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Motivation behind propensity scores: \n",
    "Considering an idealized situation in which the treatment and comparison groups are similar on all background characteristics (as is attained in a randomized experiment). In nonexperimental studies, researchers might aim to find for each treated individual a comparison individual who looks exactly the same as the treated individual on all observed pretreatment covariates. Thus, assuming no hidden bias, any difference in outcomes within these pairs could be attributed to the treatment and not to any other differences between the treated and comparison individuals. <br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benefits of propensity score matching:\n",
    "(a) Facilitate this matching by collapsing the set of observed background covariates into a single summary measure (the propensity score), representing an estimate of the probability of receiving the treatment. Then, instead of trying to find treated and comparison individuals with the same values of all covariates, one can match each treated individual to a comparison individual with a similar value of the propensity score. \n",
    "\n",
    "If treatment assignment is independent of the potential outcomes given the full set of covariates (treatment assignment is unconfounded), then it is also independent of the potential outcomes given the propensity score. This implies that the benefits of matching on all covariates individually are also attained when matching on the propensity score. In other words, within a set of treated and comparison individuals with similar propensity scores, the treatment and comparison groups will also have similar distributions of all the covariates that went into the propensity score. The success of the matching procedure is then examined by comparing the distributions of the covariates in the resulting matched treatment and comparison groups.\n",
    "\n",
    "(b) Reduced bias in the estimation of causal effects using nonexperimental data, partly through reduced reliance on the outcome model itself (e.g. violations of the assumption of a normal distribution or linearity) <br>\n",
    "(c) Intuitive and easy explanation to nontechnical audiences <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Low Dim Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propensity score didn't perform as well on low dimensional data set as high dimensional one. <br> \n",
    "The problem may be that full matching sometimes leads to matched sets with widely varying ratios of treated to comparison individuals, which can lead to large variance of the resulting effect estimates.<br>\n",
    "With regular propensity scores the ratio of the subsets for the low dim datasets have a somewhat high variance than for high dim datasets, as seen in the treatment/control ratio list below: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "full_match_propensity_factor = optmatch.fullmatch(optmatch.match_on(Formula('A~propensity_score'),data=lowDim_dataset_propensity_R,method='euclidean'),data=lowDim_dataset_propensity_R)\n",
    "lowDim_dataset_propensity['assign'] = list(full_match_propensity_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [18, 1], [1, 1], [1, 1], [1, 8], [1, 16], [1, 1], [2, 1], [1, 1], [1, 2]]\n"
     ]
    }
   ],
   "source": [
    "#compute ATE\n",
    "ATE_vec = []\n",
    "weights = []\n",
    "ratio_list = []\n",
    "\n",
    "for i in range(max(list(full_match_propensity_factor))):\n",
    "    temp = lowDim_dataset_propensity.loc[lowDim_dataset_propensity['assign']==i+1]\n",
    "    \n",
    "    treatment_Y = temp.loc[temp['A']==1]['Y'].values\n",
    "    control_Y = temp.loc[temp['A']==0]['Y'].values\n",
    "    \n",
    "    ATE_vec.append(np.mean(treatment_Y)-np.mean(control_Y))\n",
    "    weights.append(len(treatment_Y)+len(control_Y))\n",
    "    ratio_list.append([len(treatment_Y),len(control_Y)])\n",
    "    \n",
    "lowDim_propensity_est_ATE = np.average(ATE_vec, weights=weights)\n",
    "    \n",
    "end = time.time()\n",
    "lowDim_propensity_match_runtime = end-start\n",
    "print(ratio_list[0:10])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowDim_propensity_runtime = \"{:,.3f}\".format(lowDim_propensity_R_runtime+lowDim_propensity_match_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE for low dimension is:  3.388\n",
      "Runtime for low dimension is:  0.364\n",
      "ATE error for low dimension is:  0.888\n"
     ]
    }
   ],
   "source": [
    "lowDim_propensity_error = abs(lowDim_true_ATE-lowDim_propensity_est_ATE)\n",
    "lowDim_propensity_error =\"{:,.3f}\".format(lowDim_propensity_error)\n",
    "lowDim_propensity_est_ATE =\"{:,.3f}\".format(lowDim_propensity_est_ATE)\n",
    "\n",
    "print(\"ATE for low dimension is: \", lowDim_propensity_est_ATE)\n",
    "print(\"Runtime for low dimension is: \", lowDim_propensity_runtime)\n",
    "print(\"ATE error for low dimension is: \", lowDim_propensity_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. High Dim Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propensity scores performed well on the High dim data set. \n",
    "\n",
    "This is because, as mentioned in the benefits section above, propensity scores facilitate the matching by collapsing the set of observed background covariates into a single summary measure (the propensity score), representing an estimate of the probability of receiving the treatment. Since this is the high dimensional data set, it works particularly well to collapse all these covariates into the propensity score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "full_match_propensity_factor = optmatch.fullmatch(optmatch.match_on(Formula('A~propensity_score'),data=highDim_dataset_propensity_R,method='euclidean'),data=highDim_dataset_propensity_R)\n",
    "highDim_dataset_propensity['assign'] = list(full_match_propensity_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratio list below shows ratios of treated to comparison individuals, which is more consistent around 1 than those in the low dim data set, which should result in less variance and is possibly why propensity scores performed better for high dim than low dim. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1]]\n"
     ]
    }
   ],
   "source": [
    "#compute ATE\n",
    "ATE_vec = []\n",
    "weights = []\n",
    "ratio_list = []\n",
    "\n",
    "for i in range(max(list(full_match_propensity_factor))):\n",
    "    temp = highDim_dataset_propensity.loc[highDim_dataset_propensity['assign']==i+1]\n",
    "    \n",
    "    treatment_Y = temp.loc[temp['A']==1]['Y'].values\n",
    "    control_Y = temp.loc[temp['A']==0]['Y'].values\n",
    "    \n",
    "    ATE_vec.append(np.mean(treatment_Y)-np.mean(control_Y))\n",
    "    weights.append(len(treatment_Y)+len(control_Y))\n",
    "    ratio_list.append([len(treatment_Y),len(control_Y)])\n",
    "\n",
    "highDim_propensity_est_ATE = np.average(ATE_vec, weights=weights)    \n",
    "\n",
    "end = time.time()\n",
    "highDim_propensity_match_runtime = end-start\n",
    "print(ratio_list[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "highDim_propensity_runtime = \"{:,.3f}\".format(highDim_propensity_R_runtime+highDim_propensity_match_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE for high dimension is:  -3.292\n",
      "Runtime for high dimension is:  5.720\n",
      "ATE error for high dimension is:  0.292\n"
     ]
    }
   ],
   "source": [
    "highDim_propensity_error = abs(highDim_true_ATE-highDim_propensity_est_ATE)\n",
    "highDim_propensity_error =\"{:,.3f}\".format(highDim_propensity_error)\n",
    "highDim_propensity_est_ATE =\"{:,.3f}\".format(highDim_propensity_est_ATE)\n",
    "\n",
    "print(\"ATE for high dimension is: \", highDim_propensity_est_ATE)\n",
    "print(\"Runtime for high dimension is: \", highDim_propensity_runtime)\n",
    "print(\"ATE error for high dimension is: \", highDim_propensity_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric 3: Linear Propensity Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defined as:                         \n",
    "                        $$D_{ij}=|logit(e_i)-logit(e_j)|$$\n",
    "Obtained by applying the logit function on the propensity scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching on the linear propensity score can be particularly effective in terms of reducing bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Low Dim Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Propensity Score didn't perform as well for low dimension as high dimensional data set, possibly for the same reason discussed above for standard Propensity Score. <br>\n",
    "We can see from the ratio_list print out below that the ratio between treatment and controls in each group is not very consistent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "full_match_linear_propensity_factor = optmatch.fullmatch(optmatch.match_on(Formula('A~linear_propensity_score'),data=lowDim_dataset_linear_propensity_R,method='euclidean'),data=lowDim_dataset_linear_propensity_R)\n",
    "lowDim_dataset_linear_propensity['assign'] = list(full_match_linear_propensity_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1], [1, 1], [1, 2], [1, 7], [1, 2], [1, 17], [1, 1], [1, 1], [1, 1], [1, 2]]\n"
     ]
    }
   ],
   "source": [
    "#compute ATE\n",
    "ATE_vec = []\n",
    "weights = []\n",
    "ratio_list = []\n",
    "\n",
    "for i in range(max(list(full_match_linear_propensity_factor))):\n",
    "    temp = lowDim_dataset_linear_propensity.loc[lowDim_dataset_linear_propensity['assign']==i+1]\n",
    "    \n",
    "    treatment_Y = temp.loc[temp['A']==1]['Y'].values\n",
    "    control_Y = temp.loc[temp['A']==0]['Y'].values\n",
    "    \n",
    "    ATE_vec.append(np.mean(treatment_Y)-np.mean(control_Y))\n",
    "    weights.append(len(treatment_Y)+len(control_Y))\n",
    "    ratio_list.append([len(treatment_Y),len(control_Y)])\n",
    "    \n",
    "lowDim_linear_propensity_est_ATE = np.average(ATE_vec, weights=weights)\n",
    "\n",
    "end = time.time()\n",
    "lowDim_linear_propensity_match_runtime = end-start\n",
    "print(ratio_list[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowDim_linear_propensity_runtime = \"{:,.3f}\".format(lowDim_linear_propensity_R_runtime+lowDim_linear_propensity_match_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE for low dimension is:  3.476\n",
      "Runtime for low dimension is:  0.331\n",
      "ATE error for low dimension is:  0.976\n"
     ]
    }
   ],
   "source": [
    "lowDim_linear_propensity_error = abs(lowDim_true_ATE-lowDim_linear_propensity_est_ATE)\n",
    "lowDim_linear_propensity_error =\"{:,.3f}\".format(lowDim_linear_propensity_error)\n",
    "lowDim_linear_propensity_est_ATE =\"{:,.3f}\".format(lowDim_linear_propensity_est_ATE)\n",
    "\n",
    "print(\"ATE for low dimension is: \", lowDim_linear_propensity_est_ATE)\n",
    "print(\"Runtime for low dimension is: \", lowDim_linear_propensity_runtime)\n",
    "print(\"ATE error for low dimension is: \", lowDim_linear_propensity_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. High Dim Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Propensity score performed well for high dimensional data set as explained above for standard propensity score. In addition, linear propensity score is effective in reducing bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "full_match_linear_propensity_factor = optmatch.fullmatch(optmatch.match_on(Formula('A~linear_propensity_score'),data=highDim_dataset_linear_propensity_R,\n",
    "                                                                           method='euclidean'),data=highDim_dataset_linear_propensity_R)\n",
    "highDim_dataset_linear_propensity['assign'] = list(full_match_linear_propensity_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1], [2, 1], [1, 1], [1, 1], [2, 1], [1, 1], [2, 1], [1, 1], [2, 1], [1, 1]]\n"
     ]
    }
   ],
   "source": [
    "#compute ATE\n",
    "ATE_vec = []\n",
    "weights = []\n",
    "ratio_list = []\n",
    "for i in range(max(list(full_match_linear_propensity_factor))):\n",
    "    temp = highDim_dataset_linear_propensity.loc[highDim_dataset_linear_propensity['assign']==i+1]\n",
    "    \n",
    "    treatment_Y = temp.loc[temp['A']==1]['Y'].values\n",
    "    control_Y = temp.loc[temp['A']==0]['Y'].values\n",
    "    \n",
    "    ATE_vec.append(np.mean(treatment_Y)-np.mean(control_Y))\n",
    "    weights.append(len(treatment_Y)+len(control_Y))\n",
    "    ratio_list.append([len(treatment_Y),len(control_Y)])\n",
    "\n",
    "highDim_linear_propensity_est_ATE=np.average(ATE_vec, weights=weights)\n",
    "\n",
    "end = time.time()\n",
    "highDim_linear_propensity_match_runtime = end-start\n",
    "print(ratio_list[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "highDim_linear_propensity_runtime = \"{:,.3f}\".format(highDim_linear_propensity_R_runtime+highDim_linear_propensity_match_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE for high dimension is:  -3.232\n",
      "Runtime for high dimension is:  5.390\n",
      "ATE error for high dimension is:  0.232\n"
     ]
    }
   ],
   "source": [
    "highDim_linear_propensity_error = abs(highDim_true_ATE-highDim_linear_propensity_est_ATE)\n",
    "highDim_linear_propensity_error =\"{:,.3f}\".format(highDim_linear_propensity_error)\n",
    "highDim_linear_propensity_est_ATE =\"{:,.3f}\".format(highDim_linear_propensity_est_ATE)\n",
    "\n",
    "print(\"ATE for high dimension is: \", highDim_linear_propensity_est_ATE)\n",
    "print(\"Runtime for high dimension is: \", highDim_linear_propensity_runtime)\n",
    "print(\"ATE error for high dimension is: \", highDim_linear_propensity_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Inverse Propensity Weighting Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing the exposure effects between treatment groups, if we ignored those confounding factors, the effect estimates will be biased. Inverse probability weighting (IPW) based on the marginal structure model is an important method that can be used to estimate the effect of observational data processing and can address a very large number of confounding variables. Applying this weight when conducting statistical tests or regression models reduces or removes the impact of confounders. For inverse probability of treatment weighting (IPTW), we use propensity score as inverse weights in estimates of the ATE. \n",
    "\n",
    "The weight $w_i$ is \n",
    "$$w_i = \\frac{T_i}{\\hat{e_i}} + \\frac{1 - T_i}{1 - \\hat{e_i}} $$\n",
    "where $\\hat{e_i}$ is the estimated propensity score for individual $i$; $T_i$ is the treatment groups: $T_0$ is the controlled group and $T_1$ is after treatment group\n",
    "\n",
    "In this project, IPW does not work well in both low and high dimensional datasets. The reason might be the limitations of the Inverse Probability Weighted Estimator (IPWE). It can be unstable if estimated propensities are small.\n",
    "The estimated ATE using IPW is \n",
    "$$\\hat{\\Delta}_{IPW} = N^{-1} (\\sum_{i \\in treated}{w_i Y_i} -\\sum_{i\\in controlled}{w_i Y_i} )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reset data & Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowDim_dataset = pd.read_csv('../data/lowDim_dataset.csv')\n",
    "highDim_dataset = pd.read_csv('../data/highDim_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipw_ate(dataset):\n",
    "    treated = 0\n",
    "    controlled = 0\n",
    "    for i in range(dataset.shape[0]):\n",
    "        if dataset['A'][i] == 1:\n",
    "            treated += dataset['Y'][i] * dataset['weight'][i]\n",
    "        else:\n",
    "            controlled += dataset['Y'][i] * dataset['weight'][i]\n",
    "    \n",
    "    #print(treated - controlled)\n",
    "    ate = (treated - controlled)/dataset.shape[0]\n",
    "    return ate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Low Dim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE for low dimension is:  0.839\n",
      "Runtime for low dimension is:  0.407\n",
      "ATE error for low dimension is:  1.661\n"
     ]
    }
   ],
   "source": [
    "runtime = time.time()\n",
    "X=lowDim_dataset.iloc[:,2:].values\n",
    "A=lowDim_dataset['A'].values\n",
    "gbm = GradientBoostingClassifier(learning_rate = 0.01, max_depth = 2, min_samples_leaf = 1,\n",
    "                                min_samples_split = 2, n_estimators = 150).fit(X,A)\n",
    "low_dim_propensity_scores = [x[1] for x in gbm.predict_proba(X)]\n",
    "lowDim_dataset_ipw = lowDim_dataset\n",
    "lowDim_dataset_ipw['score'] = low_dim_propensity_scores\n",
    "lowDim_dataset_ipw['weight'] = lowDim_dataset_ipw['A']/lowDim_dataset_ipw['score'] + (1 - lowDim_dataset_ipw['A'])/(1 - lowDim_dataset_ipw['score'])\n",
    "ate_low = ipw_ate(lowDim_dataset_ipw)\n",
    "runtime_low_ipw = \"{:,.3f}\".format(time.time()-runtime)\n",
    "lowDim_ipw_error = abs(ate_low - lowDim_true_ATE)\n",
    "lowDim_ipw_error = \"{:,.3f}\".format(lowDim_ipw_error)\n",
    "ate_low = \"{:,.3f}\".format(ate_low)\n",
    "print(\"ATE for low dimension is: \", ate_low)\n",
    "print(\"Runtime for low dimension is: \", runtime_low_ipw)\n",
    "print(\"ATE error for low dimension is: \", lowDim_ipw_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. High Dim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE for high dimension is:  -1.847\n",
      "Runtime for high dimension is:  0.751\n",
      "ATE error for high dimension is:  1.153\n"
     ]
    }
   ],
   "source": [
    "runtime = time.time()\n",
    "X=highDim_dataset.iloc[:,2:].values\n",
    "A=highDim_dataset['A'].values\n",
    "Y=highDim_dataset['Y'].values\n",
    "\n",
    "gbm = GradientBoostingClassifier(learning_rate = 0.05, max_depth = 1, min_samples_leaf = 5,\n",
    "                                min_samples_split = 2, n_estimators = 100).fit(X,A)\n",
    "high_dim_propensity_scores = [x[1] for x in gbm.predict_proba(X)]\n",
    "highDim_dataset_ipw = highDim_dataset\n",
    "highDim_dataset_ipw['score'] = high_dim_propensity_scores\n",
    "highDim_dataset_ipw['weight'] = highDim_dataset_ipw['A']/highDim_dataset_ipw['score'] + (1 - highDim_dataset_ipw['A'])/(1 - highDim_dataset_ipw['score'])\n",
    "ate_high = ipw_ate(highDim_dataset_ipw)\n",
    "runtime_high_ipw = \"{:,.3f}\".format(time.time()-runtime)\n",
    "highDim_ipw_error = abs(ate_high-highDim_true_ATE)\n",
    "highDim_ipw_error = \"{:,.3f}\".format(highDim_ipw_error)\n",
    "ate_high = \"{:,.3f}\".format(ate_high)\n",
    "print(\"ATE for high dimension is: \", ate_high)\n",
    "print(\"Runtime for high dimension is: \", runtime_high_ipw)\n",
    "print(\"ATE error for high dimension is: \", highDim_ipw_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an observational study, the treatment effects are often hard to estimate because of potential confounding elements. Therefore, a method called Principal Stratification is used to divide data into several stratas and compute causal effects within each strata. Principal effects are defined as a comparison of potential outcomes on a set of common units which is the union of principal strata. Individuals are stratified based on estimated propensity scores and the difference estimated as the average of within-stratum effects. Stratification attempts to achieve groups where individuals share the same propensity scores which is unrealistic in practice. \n",
    "\n",
    "One problem with stratification in estimating propensity scores is that when working under a large sample size, the estimation will become biased because of residual confounding. Under this circumstance, increasing the number of stratas used might be one possible solution to consider. In order to reduce bias, the trade-off between bias and variation can be examined using a careful choice of number of stratas. \n",
    "In this project, stratification works well with both low and high dimensional datasets and the default number of stratums was five, which worked appropriately.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for the Stratification algorithm is the following where K is the number of strata (we use $K=5$), $N_{1j}$ is number of treated individuals in stratum j, $N_{0j}$ is the number of control individuals in stratum j, and $\\hat{Q_j}=(\\hat{q_{j-1}},\\hat{q_j}]$, where $\\hat{q_j}$ is the $j^{th}$ sample quantile of the propensity scores:\n",
    "\n",
    "$$\\hat{\\Delta}_S = \\sum_{j=1}^{K} \\frac{N_j}{N}\\{N_{1j}^{-1}\\sum_{i=1}^{N}T_iY_iI(\\hat{e_i} \\in \\hat{Q_j})-N_{0j}^{-1}\\sum_{i=1}^{N}(1-T_i)Y_iI(\\hat{e_i} \\in \\hat{Q_j})\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to stratify data \n",
    "def stratify(df):\n",
    "    \n",
    "    Y = df['Y']\n",
    "    D = df['A']\n",
    "    scores = df['propensity_scores']\n",
    "    \n",
    "    # Create stratum and stratum limits\n",
    "    Q1 = np.quantile(scores, .20)\n",
    "    Q2 = np.quantile(scores, .40)\n",
    "    Q3 = np.quantile(scores, .60)\n",
    "    Q4 = np.quantile(scores, .80)\n",
    "    Q5 = np.quantile(scores, 1.0)\n",
    "    \n",
    "    quin1 = df[df['propensity_scores']<= Q1]\n",
    "    quin2 = df[(df['propensity_scores']> Q1) & (df['propensity_scores']<= Q2)]\n",
    "    quin3 = df[(df['propensity_scores']> Q2) & (df['propensity_scores']<= Q3)]\n",
    "    quin4 = df[(df['propensity_scores']> Q3) & (df['propensity_scores']<= Q4)]\n",
    "    quin5 = df[df['propensity_scores']> Q4]\n",
    "\n",
    "    quintiles = [quin1, quin2, quin3, quin4, quin5]\n",
    "    Q_ranges = [None, Q1, Q2, Q3, Q4, Q5]\n",
    "\n",
    "    return [quintiles, Q_ranges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to calc ATE\n",
    "def strat_ATE(quintiles, Q_ranges):\n",
    "    results = []\n",
    "    N = sum([len(quintiles[0]),len(quintiles[1]),len(quintiles[2]),len(quintiles[3]),len(quintiles[4])])\n",
    "    \n",
    "    for i, stratum in enumerate(quintiles): \n",
    "        i+=1\n",
    "        \n",
    "        Nj = len(stratum)                      # Number of ind in stratum\n",
    "        N1j = stratum['A'].value_counts()[1]   # Number of treated ind\n",
    "        N0j = stratum['A'].value_counts()[0]   # Number of control ind\n",
    "        \n",
    "        sum1 = 0\n",
    "        sum2 = 0\n",
    "\n",
    "        # Summation of treated samples within strata\n",
    "        sum1 = sum([Y*T for Y,T in zip(stratum['Y'],stratum['A'])])\n",
    "        # Summation of untreated samples within strata\n",
    "        sum2 = sum([(1-T)*Y for Y,T in zip(stratum['Y'],stratum['A'])]) \n",
    "\n",
    "        results.append(Nj/N * ((sum1/N1j)-(sum2/N0j)))\n",
    "\n",
    "    return sum(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowDim_dataset = pd.read_csv('../data/lowDim_dataset.csv')\n",
    "highDim_dataset = pd.read_csv('../data/highDim_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Low Dim data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowDim_scores = pd.read_csv('../output/low_dim_propensity_scores.csv') \n",
    "lowDim_scores.insert( 1 , \"Y\" , lowDim_dataset['Y']) \n",
    "lowDim_scores.insert( 2 , \"A\" , lowDim_dataset['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "quintiles , Q_ranges = stratify(lowDim_scores) \n",
    "\n",
    "lowDim_stratification_ATE = strat_ATE(quintiles ,Q_ranges)\n",
    "end = time.time()\n",
    "lowdim_strat_runtime = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE for low dimension is:  2.464\n",
      "Runtime for low dimension is:  0.016\n",
      "ATE error for low dimension is:  0.036\n"
     ]
    }
   ],
   "source": [
    "lowdim_strat_runtime = \"{:,.3f}\".format(lowdim_strat_runtime)\n",
    "\n",
    "lowDim_stratification_error = abs(lowDim_stratification_ATE-lowDim_true_ATE)\n",
    "lowDim_stratification_error = \"{:,.3f}\".format(lowDim_stratification_error)\n",
    "lowDim_stratification_ATE = \"{:,.3f}\".format(lowDim_stratification_ATE)\n",
    "\n",
    "print(\"ATE for low dimension is: \", lowDim_stratification_ATE)\n",
    "print(\"Runtime for low dimension is: \", lowdim_strat_runtime)\n",
    "print(\"ATE error for low dimension is: \", lowDim_stratification_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. High Dim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Calculated Propensity Scores \n",
    "highDim_scores = pd.read_csv('../output/high_dim_propensity_scores.csv') \n",
    "highDim_scores.insert( 1 , \"Y\" , highDim_dataset['Y']) \n",
    "highDim_scores.insert( 2 , \"A\" , highDim_dataset['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "quintiles , Q_ranges = stratify(highDim_scores)\n",
    "\n",
    "highDim_stratification_ATE = strat_ATE(quintiles ,Q_ranges)\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE for high dimension is:  -3.010\n",
      "Runtime for high dimension is:  0.015\n",
      "ATE error for high dimension is:  0.010\n"
     ]
    }
   ],
   "source": [
    "highdim_strat_runtime=\"{:,.3f}\".format(end-start)\n",
    "\n",
    "highDim_stratification_error = abs(highDim_stratification_ATE-highDim_true_ATE)\n",
    "highDim_stratification_error = \"{:,.3f}\".format(highDim_stratification_error)\n",
    "highDim_stratification_ATE = \"{:,.3f}\".format(highDim_stratification_ATE)\n",
    "\n",
    "print(\"ATE for high dimension is: \", highDim_stratification_ATE)\n",
    "print(\"Runtime for high dimension is: \", highdim_strat_runtime)\n",
    "print(\"ATE error for high dimension is: \", highDim_stratification_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have a table of all of the absolute errors and the runtimes for each method across the low dimensional and high dimensional datasets. Note that this notebook was run on a Mac with 64 GB of RAM and an i9 CPU, so the runtimes may be different if you run this notebook on your system.\n",
    "\n",
    "We can see that the overall best method on the low dimensional dataset and the high dimensional dataset in terms of runtime and performance is stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Metric            </th><th>Dimension  </th><th style=\"text-align: right;\">       Full Matching-\n",
       "Mahalanobis</th><th style=\"text-align: right;\">      Full Matching-\n",
       "Propensity score</th><th style=\"text-align: right;\">      Full Matching-\n",
       "Linear Propensity Score</th><th style=\"text-align: right;\">      Inverse Propensity\n",
       "Weighting</th><th style=\"text-align: right;\">  Stratification</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>ATE Absolute Error</td><td>Low Dim    </td><td style=\"text-align: right;\"> 0.406</td><td style=\"text-align: right;\">0.888</td><td style=\"text-align: right;\">0.976</td><td style=\"text-align: right;\">1.661</td><td style=\"text-align: right;\">           0.036</td></tr>\n",
       "<tr><td>                  </td><td>High Dim   </td><td style=\"text-align: right;\"> 1.447</td><td style=\"text-align: right;\">0.292</td><td style=\"text-align: right;\">0.232</td><td style=\"text-align: right;\">1.153</td><td style=\"text-align: right;\">           0.01 </td></tr>\n",
       "<tr><td>Run Time (sec)    </td><td>Low Dim    </td><td style=\"text-align: right;\"> 0.437</td><td style=\"text-align: right;\">0.364</td><td style=\"text-align: right;\">0.331</td><td style=\"text-align: right;\">0.407</td><td style=\"text-align: right;\">           0.016</td></tr>\n",
       "<tr><td>                  </td><td>High Dim   </td><td style=\"text-align: right;\">52.327</td><td style=\"text-align: right;\">5.72 </td><td style=\"text-align: right;\">5.39 </td><td style=\"text-align: right;\">0.751</td><td style=\"text-align: right;\">           0.015</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table = [[\"ATE Absolute Error\",'Low Dim',lowDim_mahalanobis_error,lowDim_propensity_error,\n",
    "         lowDim_linear_propensity_error,lowDim_ipw_error,lowDim_stratification_error],\n",
    "        [\"\",'High Dim',highDim_mahalanobis_error,highDim_propensity_error,\n",
    "         highDim_linear_propensity_error,highDim_ipw_error,highDim_stratification_error],\n",
    "        [\"Run Time (sec)\",'Low Dim',lowDim_mahalanobis_runtime,lowDim_propensity_runtime,lowDim_linear_propensity_runtime,\n",
    "         runtime_low_ipw, lowdim_strat_runtime],\n",
    "        [\"\",'High Dim',highDim_mahalanobis_runtime,highDim_propensity_runtime,highDim_linear_propensity_runtime,\n",
    "         runtime_high_ipw, highdim_strat_runtime]]\n",
    "\n",
    "display(HTML(tabulate.tabulate(table, headers=[\"Metric\",\"Dimension\", \"Full Matching-\\nMahalanobis\",\n",
    "                                               \"Full Matching-\\nPropensity score\", \"Full Matching-\\nLinear Propensity Score\",\n",
    "                                               \"Inverse Propensity\\nWeighting\", 'Stratification'],\n",
    "                                tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chan, D., Ge, R., Gershony, O., Hesterberg, T., & Lambert, D. (2010). Evaluating online ad campaigns in a pipeline: causal models at scale. KDD '10.\n",
    "\n",
    "Hansen, Ben B. Full matching in an observational study of coaching for the SAT. J. Amer. Statist. Assoc. 99 (2004), no. 467, 609–618.\n",
    "\n",
    "Lunceford, Jared K, and Marie Davidian. 2004. “Stratification and Weighting via the Propensity Score in\n",
    "Estimation of Causal Treatment Effects a Comparative Study.” Statistics in Medicine 23 (19): 2937–60.\n",
    "\n",
    "McCaffrey, D. F., Ridgeway, G., & Morral, A. R. (2004). Propensity score estimation with boosted regression for evaluating causal effects in observational studies. Psychological methods, 9(4), 403–425. https://doi.org/10.1037/1082-989X.9.4.403\n",
    "\n",
    "Raad, H., Cornelius, V., Chan, S., Williamson, E. and Cro, S., 2020. An evaluation of inverse probability weighting using the propensity score for baseline covariate adjustment in smaller population randomised controlled trials with a continuous outcome. BMC Medical Research Methodology, 20(1), pp.1-12. https://link.springer.com/article/10.1186/s12874-020-00947-7\n",
    "\n",
    "Stuart E. A. (2010). Matching methods for causal inference: A review and a look forward. Statistical science : a review journal of the Institute of Mathematical Statistics, 25(1), 1–21. https://doi.org/10.1214/09-STS313\n",
    "\n",
    "Stuart, E. A., & Green, K. M. (2008). Using full matching to estimate causal effects in nonexperimental studies: examining the relationship between adolescent marijuana use and adult outcomes. Developmental psychology, 44(2), 395–406. https://doi.org/10.1037/0012-1649.44.2.395"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
