{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Import Required Packages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the cell below doesn't run then do 'pip install rpy2' \n",
    "#### Change the paths for os.environ below to match your R folder directory and version if you get error messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rpy2\n",
    "\n",
    "try:\n",
    "    import rpy2.robjects as robjects\n",
    "except:\n",
    "    os.environ[\"R_HOME\"] = r\"C:\\Program Files\\R\\R-4.0.2\"\n",
    "    os.environ[\"PATH\"]   = r\"C:\\Program Files\\R\\R-4.0.2\\bin\\x64\" + \";\" + os.environ[\"PATH\"]\n",
    "    import rpy2.robjects as robjects\n",
    "    \n",
    "import rpy2.robjects.packages as rpackages\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "from rpy2.robjects.vectors import StrVector\n",
    "from rpy2.robjects import FloatVector, Formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run pip install tabulate if the cell below does not run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import math\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Read the files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first read in the low dimensional dataset and high dimensional dataset from the data folder. These have feature columns with a prefix ‘V’, a treatment/control column, and a continuous response Y. \n",
    "\n",
    "For this project, we know that the true ATE for the low dimensional data is 2.5, and for the high dimensional data it’s -3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowDim_dataset = pd.read_csv('../data/lowDim_dataset.csv')\n",
    "highDim_dataset = pd.read_csv('../data/highDim_dataset.csv')\n",
    "\n",
    "lowDim_true_ATE = 2.5\n",
    "highDim_true_ATE = -3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Calculate Propensity and Linear Propensity Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the propensity scores, we first need to fit a GBM classifier on the features and the binary response A, which indicates if a person is in the control group (0) or the treatment group (1). To get the optimal parameters for the GBM without overfitting, we performed a grid search. The two cells below are commented out due to the long runtime it takes to perform the grid search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#low dim grid search (commented out since it takes a few minutes to run)\n",
    "\n",
    "#X=lowDim_dataset.iloc[:,2:].values\n",
    "#A=lowDim_dataset['A'].values\n",
    "#Y=lowDim_dataset['Y'].values\n",
    "\n",
    "#params = {'learning_rate':[0.01,0.05,0.1,0.5], 'max_depth': [1,2,3,4], 'n_estimators':[50,100,150],\n",
    "#          'min_samples_leaf':[1,3,5],'min_samples_split':[2,4,6]}\n",
    "#gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=5).fit(X,A)\n",
    "#gscv.best_params_\n",
    "\n",
    "#output: {'learning_rate': 0.01,\n",
    "# 'max_depth': 2,\n",
    "# 'min_samples_leaf': 1,\n",
    "# 'min_samples_split': 2,\n",
    "# 'n_estimators': 150}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#high dim grid search (commented out since it takes a few minutes to run)\n",
    "\n",
    "#X=highDim_dataset.iloc[:,2:].values\n",
    "#A=highDim_dataset['A'].values\n",
    "#Y=highDim_dataset['Y'].values\n",
    "\n",
    "#params = {'learning_rate':[0.01,0.05,0.1,0.5], 'max_depth': [1,2,3,4], 'n_estimators':[50,100,150],\n",
    "#          'min_samples_leaf':[1,3,5],'min_samples_split':[2,4,6]}\n",
    "#gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=5).fit(X,A)\n",
    "#gscv.best_params_\n",
    "\n",
    "\n",
    "#output: {'learning_rate': 0.05,\n",
    "# 'max_depth': 1,\n",
    "# 'min_samples_leaf': 5,\n",
    "# 'min_samples_split': 2,\n",
    "# 'n_estimators': 100}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the logit function, which is used to get linear propensity scores from the propensity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(x):\n",
    "    return math.log(x/(1-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the parameters we got from the grid search to get the propensity scores and linear propensity scores from the low and high dimensional datasets. We save the scores to csv files in the output folder.\n",
    "\n",
    "Note that the propensity score of an individual is the class probability that it is the individual is in the treatment group, and the probabilities are determined by the GBM classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Dimensional Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=lowDim_dataset.iloc[:,2:].values\n",
    "A=lowDim_dataset['A'].values\n",
    "Y=lowDim_dataset['Y'].values\n",
    "\n",
    "gbm = GradientBoostingClassifier(learning_rate = 0.01, max_depth = 2, min_samples_leaf = 1,\n",
    "                                min_samples_split = 2, n_estimators = 150).fit(X,A)\n",
    "\n",
    "low_dim_propensity_scores = [x[1] for x in gbm.predict_proba(X)]\n",
    "low_dim_linear_propensity_scores = [logit(x) for x in low_dim_propensity_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowDim_dataset_propensity = lowDim_dataset.copy(deep=True)\n",
    "lowDim_dataset_propensity['propensity_score'] = low_dim_propensity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowDim_dataset_linear_propensity = lowDim_dataset.copy(deep=True)\n",
    "lowDim_dataset_linear_propensity['linear_propensity_score'] = low_dim_linear_propensity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'propensity_scores':low_dim_propensity_scores}).to_csv('../output/low_dim_propensity_scores.csv')\n",
    "pd.DataFrame({'linear_propensity_scores':low_dim_linear_propensity_scores}).to_csv('../output/low_dim_linear_propensity_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Dimensional Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=highDim_dataset.iloc[:,2:].values\n",
    "A=highDim_dataset['A'].values\n",
    "Y=highDim_dataset['Y'].values\n",
    "\n",
    "gbm = GradientBoostingClassifier(learning_rate = 0.05, max_depth = 1, min_samples_leaf = 5,\n",
    "                                min_samples_split = 2, n_estimators = 100).fit(X,A)\n",
    "\n",
    "high_dim_propensity_scores = [x[1] for x in gbm.predict_proba(X)]\n",
    "high_dim_linear_propensity_scores = [logit(x) for x in high_dim_propensity_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "highDim_dataset_propensity = highDim_dataset.copy(deep=True)\n",
    "highDim_dataset_propensity['propensity_score'] = high_dim_propensity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "highDim_dataset_linear_propensity = highDim_dataset.copy(deep=True)\n",
    "highDim_dataset_linear_propensity['linear_propensity_score'] = high_dim_linear_propensity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'propensity_scores':high_dim_propensity_scores}).to_csv('../output/high_dim_propensity_scores.csv')\n",
    "pd.DataFrame({'linear_propensity_scores':high_dim_linear_propensity_scores}).to_csv('../output/high_dim_linear_propensity_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Perform Full Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full matching is a way to break the dataset into subsets so that each subset has at least one treatment member and at least one control member. Subsets are created based on how close people are in terms of a specific distance metric, and the subsets do not have to be the same size. Every individual must be placed into a subset.\n",
    "\n",
    "In particular, full matching creates subsets so that the sum of the distances between all pairs of treated and control individuals within each matched set, across all matched sets, is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more formal explanation of how full matching works, let $T$ and $C$ denote the treatment and control sets of individuals respectively. Suppose that for every pair $(i,j)$ with $i \\in T$ and $j \\in C$, a distance $\\delta_{ij} \\in \\mathbb{R}^+$ is given, where smaller values of $\\delta$ would indicate more desirable matches. \n",
    "\n",
    "Let $S$ be a mapping of $T \\cup C$ into $\\{1,...,K\\}$ ($K$ is the number of subsets that $S$ maps values to) where for each matched subset $M$, $M=S^{-1}[k]$ $(1 \\leq k \\leq K)$ is such that $min(|M \\cap T|, |M \\cap C|)=1$ and $\\delta_{ij} <\\infty$ for all i,j.\n",
    "\n",
    "Then given a dataset of control individuals $C$, treatment individuals $T$, and distances $\\delta$, a full match $S'$ minimizes the below sum, which is the sum of distances between all pairs of treated and control individuals across all matches for a fixed matching function $S$.\n",
    "\n",
    "$$ \\sum_{i \\in T, S(i) > 0}^{}  \\sum_{j \\in C, S(i)=S(j)}^{}  \\delta_{ij}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The motivation behind full matching is that people within the same subset are ideally similar enough to each other that their response values can serve as counterfactuals for each other (e.g. if person A received treatment and is in the same subset as person B who was under control, then the response value for B would be close to the response value for A if A had been under control instead of treatment and vice versa). Once we’ve created the subsets, we can estimate the ATE by taking a weighted average of all of the differences between mean treatment response and mean control response within the subsets. \n",
    "\n",
    "Note that one problem with full matching is that it sometimes leads to subsets with widely varying ratios of treatment to control since we only require that at least one treatment or control be in each set. For example, it is possible to have one subset with five treatment and five control individuals, and another subset to have one treatment and eight control individuals. Hence, this possible large variance in ratios can lead to a large variance of differences between mean treatment response and mean control response across all of the subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up rpy2 (Python Interface to R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement full matching, we use the fullmatch function from the R package optmatch. To use the function, we first set up the Python interface to R which will install the necessary packages from CRAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "utils = importr('utils')\n",
    "utils.chooseCRANmirror(ind=1)\n",
    "packnames = ('optmatch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Installing packages into 'C:/Users/Elise Nguyen/OneDrive/Documents/R/win-library/4.0'\n",
      "(as 'lib' is unspecified)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names_to_install = [x for x in packnames if not rpackages.isinstalled(x)]\n",
    "if len(names_to_install) > 0:\n",
    "    utils.install_packages(StrVector(names_to_install))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "utils.chooseCRANmirror(ind=1)\n",
    "robjects.r(f'install.packages(\"{\"optmatch\"}\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "optmatch = rpackages.importr('optmatch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we convert the pandas dataframes to R dataframes which are compatible with the fullmatch function and also keep track of the runtime it takes to do the conversion. \n",
    "\n",
    "The try-except block is to take into account of the fact that Windows and Mac do not have the same latest version of the rpy2 package, and the two different versions have different syntax for dataframe conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "with localconverter(robjects.default_converter + pandas2ri.converter):\n",
    "    try:\n",
    "        lowDim_R_runtime = time.time()\n",
    "        lowDim_dataset_R = robjects.conversion.py2rpy(lowDim_dataset)\n",
    "        lowDim_R_runtime = time.time()-lowDim_R_runtime\n",
    "        \n",
    "        lowDim_propensity_R_runtime = time.time()\n",
    "        lowDim_dataset_propensity_R = robjects.conversion.py2rpy(lowDim_dataset_propensity)\n",
    "        lowDim_propensity_R_runtime = time.time()-lowDim_propensity_R_runtime\n",
    "        \n",
    "        lowDim_linear_propensity_R_runtime = time.time()\n",
    "        lowDim_dataset_linear_propensity_R = robjects.conversion.py2rpy(lowDim_dataset_linear_propensity)\n",
    "        lowDim_linear_propensity_R_runtime = time.time()-lowDim_linear_propensity_R_runtime\n",
    "        \n",
    "    except:\n",
    "        lowDim_R_runtime = time.time()\n",
    "        lowDim_dataset_R = pandas2ri.py2ri(lowDim_dataset)\n",
    "        lowDim_R_runtime = time.time()-lowDim_R_runtime\n",
    "        \n",
    "        lowDim_propensity_R_runtime = time.time()\n",
    "        lowDim_dataset_propensity_R = pandas2ri.py2ri(lowDim_dataset_propensity)\n",
    "        lowDim_propensity_R_runtime = time.time()-lowDim_propensity_R_runtime\n",
    "        \n",
    "        lowDim_linear_propensity_R_runtime = time.time()\n",
    "        lowDim_dataset_linear_propensity_R = pandas2ri.py2ri(lowDim_dataset_linear_propensity)\n",
    "        lowDim_linear_propensity_R_runtime = time.time()-lowDim_linear_propensity_R_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "with localconverter(robjects.default_converter + pandas2ri.converter):\n",
    "    try:\n",
    "        highDim_R_runtime = time.time()\n",
    "        highDim_dataset_R = robjects.conversion.py2rpy(highDim_dataset)\n",
    "        highDim_R_runtime = time.time()-highDim_R_runtime\n",
    "        \n",
    "        highDim_propensity_R_runtime = time.time()\n",
    "        highDim_dataset_propensity_R = robjects.conversion.py2rpy(highDim_dataset_propensity)\n",
    "        highDim_propensity_R_runtime = time.time()-highDim_propensity_R_runtime\n",
    "        \n",
    "        highDim_linear_propensity_R_runtime = time.time()\n",
    "        highDim_dataset_linear_propensity_R = robjects.conversion.py2rpy(highDim_dataset_linear_propensity)\n",
    "        highDim_linear_propensity_R_runtime = time.time()-highDim_linear_propensity_R_runtime\n",
    "        \n",
    "    except:\n",
    "        highDim_R_runtime = time.time()\n",
    "        highDim_dataset_R = pandas2ri.py2ri(highDim_dataset)\n",
    "        highDim_R_runtime = time.time()-highDim_R_runtime\n",
    "        \n",
    "        highDim_propensity_R_runtime = time.time()\n",
    "        highDim_dataset_propensity_R = pandas2ri.py2ri(highDim_dataset_propensity)\n",
    "        highDim_propensity_R_runtime = time.time()-highDim_propensity_R_runtime\n",
    "        \n",
    "        highDim_linear_propensity_R_runtime = time.time()\n",
    "        highDim_dataset_linear_propensity_R = pandas2ri.py2ri(highDim_dataset_linear_propensity)\n",
    "        highDim_linear_propensity_R_runtime = time.time()-highDim_linear_propensity_R_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Mahalanobis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mahalanobis distance matrix is given by\n",
    "$$D_{ij} = (X_i-X_j)^T\\Sigma^{-1}(X_i-X_j)$$\n",
    "where $\\Sigma$ is the covariance matrix of $X$ in the pooled treatment and full control groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mahalanobis does not require propensity scores and instead uses the features and covariance matrix of the pooled treatment and full control groups to create a distance matrix. Intuitively, the Mahalanobis distance measures the distance of two points relative to the centroid of all of the data points with the axes being determined by the direction of greatest variance in the cloud of points. That is, we let the data itself determine the coordinate system. For uncorrelated variables, the covariance matrix becomes a diagonal matrix, so the Mahalanobis distance between two points is equal to their standardized Euclidean distance in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Low Dim Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mahalanbois generally works well, both in terms of runtime and having a low error, when there are relatively few covariates because the covariance matrix is easier to invert. In this case, it works well because it takes advantage of the correlations between different features for its distance calculation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "full_match_Mahalanobis_factor = optmatch.fullmatch(optmatch.match_on(Formula('A~.-Y'),data=lowDim_dataset_R,method='mahalanobis'),data=lowDim_dataset_R)\n",
    "lowDim_dataset['assign'] = list(full_match_Mahalanobis_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute ATE\n",
    "ATE_vec = []\n",
    "weights = []\n",
    "\n",
    "for i in range(max(list(full_match_Mahalanobis_factor))):\n",
    "    temp = lowDim_dataset.loc[lowDim_dataset['assign']==i+1]\n",
    "    \n",
    "    treatment_Y = temp.loc[temp['A']==1]['Y'].values\n",
    "    control_Y = temp.loc[temp['A']==0]['Y'].values\n",
    "    \n",
    "    ATE_vec.append(np.mean(treatment_Y)-np.mean(control_Y))\n",
    "    weights.append(len(treatment_Y)+len(control_Y))\n",
    "\n",
    "lowDim_mahalanobis_est_ATE = np.average(ATE_vec, weights = weights)\n",
    "\n",
    "end = time.time()\n",
    "lowDim_mahalanobis_match_runtime = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.042'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#runtime is time to convert to R data frame + time to do matching\n",
    "lowDim_mahalanobis_runtime = \"{:,.3f}\".format(lowDim_R_runtime+lowDim_mahalanobis_match_runtime)\n",
    "lowDim_mahalanobis_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.406\n"
     ]
    }
   ],
   "source": [
    "lowDim_mahalanobis_error = abs(lowDim_true_ATE-lowDim_mahalanobis_est_ATE)\n",
    "lowDim_mahalanobis_error =\"{:,.3f}\".format(lowDim_mahalanobis_error)\n",
    "print(lowDim_mahalanobis_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. High Dim Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mahalanobis has a higher error on the high dimensional dataset because the creation of the distance matrix views all of the interactions between features as equally important. Hence, full matching with Mahalanobis tries to capture more of the multi way interactions, so it does not perform well when there are too many interactions to keep track of for the matching criteria. In terms of runtime, Mahalanobis also takes a significantly long time on high dimensional data due to the complexity of inverting the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "full_match_Mahalanobis_factor = optmatch.fullmatch(optmatch.match_on(Formula('A~.-Y'),data=highDim_dataset_R,method='mahalanobis'),data=highDim_dataset_R)\n",
    "highDim_dataset['assign'] = list(full_match_Mahalanobis_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute ATE\n",
    "ATE_vec = []\n",
    "weights = []\n",
    "\n",
    "for i in range(max(list(full_match_Mahalanobis_factor))):\n",
    "    temp = highDim_dataset.loc[highDim_dataset['assign']==i+1]\n",
    "    \n",
    "    treatment_Y = temp.loc[temp['A']==1]['Y'].values\n",
    "    control_Y = temp.loc[temp['A']==0]['Y'].values\n",
    "    \n",
    "    ATE_vec.append(np.mean(treatment_Y)-np.mean(control_Y))\n",
    "    weights.append(len(treatment_Y)+len(control_Y))\n",
    "    \n",
    "highDim_mahalanobis_est_ATE = np.average(ATE_vec, weights=weights)\n",
    "    \n",
    "end = time.time()\n",
    "highDim_mahalanobis_match_runtime = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'87.579'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highDim_mahalanobis_runtime = \"{:,.3f}\".format(highDim_R_runtime+highDim_mahalanobis_match_runtime)\n",
    "highDim_mahalanobis_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.447\n"
     ]
    }
   ],
   "source": [
    "highDim_mahalanobis_error = abs(highDim_true_ATE-highDim_mahalanobis_est_ATE)\n",
    "highDim_mahalanobis_error= \"{:,.3f}\".format(highDim_mahalanobis_error)\n",
    "print(highDim_mahalanobis_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Propensity Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance of Propensity Score is defined as:\n",
    "                                            $$D_{ij} = |e_{i} − e_{j} |$$\n",
    "where $e_{k}$ is the propensity score for individual k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propensity scores are created with a GBM classifier fitted on the features and treatment/control column. The propensity scores are class probabilities of each individual being in the treatment group. Since GBM adaptively chooses variables to include as part of its algorithm, we fit it on all of the features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Motivation behind propensity scores: \n",
    "Considering an idealized situation in which the treatment and comparison groups are similar on all background characteristics (as is attained in a randomized experiment). In nonexperimental studies, researchers might aim to find for each treated individual a comparison individual who looks exactly the same as the treated individual on all observed pretreatment covariates. Thus, assuming no hidden bias, any difference in outcomes within these pairs could be attributed to the treatment and not to any other differences between the treated and comparison individuals. <br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benefits of propensity score matching:\n",
    "(a) Facilitate this matching by collapsing the set of observed background covariates into a single summary measure (the propensity score), representing an estimate of the probability of receiving the treatment. Then, instead of trying to find treated and comparison individuals with the same values of all covariates, one can match each treated individual to a comparison individual with a similar value of the propensity score. \n",
    "\n",
    "If treatment assignment is independent of the potential outcomes given the full set of covariates (treatment assignment is unconfounded), then it is also independent of the potential outcomes given the propensity score. This implies that the benefits of matching on all covariates individually are also attained when matching on the propensity score. In other words, within a set of treated and comparison individuals with similar propensity scores, the treatment and comparison groups will also have similar distributions of all the covariates that went into the propensity score. The success of the matching procedure is then examined by comparing the distributions of the covariates in the resulting matched treatment and comparison groups.\n",
    "\n",
    "(b) Reduced bias in the estimation of causal effects using nonexperimental data, partly through reduced reliance on the outcome model itself (e.g., violations of the assumption of a normal distribution or linearity) <br>\n",
    "(c) Intuitive and easy explanation to nontechnical audiences <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Low Dim Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propensity score didn't perform as well on low dimensional data set as high dimensional one. <br> \n",
    "The problem maybe that full matching sometimes leads to matched sets with widely varying ratios of treated to comparison individuals, which can lead to large variance of the resulting effect estimates.<br>\n",
    "With regular propensity scores the ratio of the subsets for the low dim datasets have a somewhat high variance than for high dim datasets, as seen in the treatment/control ratio list below: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "full_match_propensity_factor = optmatch.fullmatch(optmatch.match_on(Formula('A~propensity_score'),data=lowDim_dataset_propensity_R,method='euclidean'),data=lowDim_dataset_propensity_R)\n",
    "lowDim_dataset_propensity['assign'] = list(full_match_propensity_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [18, 1], [1, 1], [1, 1], [1, 8], [1, 16], [1, 1], [2, 1], [1, 1], [1, 2]]\n"
     ]
    }
   ],
   "source": [
    "#compute ATE\n",
    "ATE_vec = []\n",
    "weights = []\n",
    "ratio_list = []\n",
    "\n",
    "for i in range(max(list(full_match_propensity_factor))):\n",
    "    temp = lowDim_dataset_propensity.loc[lowDim_dataset_propensity['assign']==i+1]\n",
    "    \n",
    "    treatment_Y = temp.loc[temp['A']==1]['Y'].values\n",
    "    control_Y = temp.loc[temp['A']==0]['Y'].values\n",
    "    \n",
    "    ATE_vec.append(np.mean(treatment_Y)-np.mean(control_Y))\n",
    "    weights.append(len(treatment_Y)+len(control_Y))\n",
    "    ratio_list.append([len(treatment_Y),len(control_Y)])\n",
    "    \n",
    "lowDim_propensity_est_ATE = np.average(ATE_vec, weights=weights)\n",
    "    \n",
    "end = time.time()\n",
    "lowDim_propensity_match_runtime = end-start\n",
    "print(ratio_list[0:10])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.902'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowDim_propensity_runtime = \"{:,.3f}\".format(lowDim_propensity_R_runtime+lowDim_propensity_match_runtime)\n",
    "lowDim_propensity_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.888\n"
     ]
    }
   ],
   "source": [
    "lowDim_propensity_error = abs(lowDim_true_ATE-lowDim_propensity_est_ATE)\n",
    "lowDim_propensity_error =\"{:,.3f}\".format(lowDim_propensity_error)\n",
    "print(lowDim_propensity_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. High Dim Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propensity scores performed well on High dim data set. \n",
    "\n",
    "This is because, as mentioned in the Benefits section above, propensity scores facilitate the matching by collapsing the set of observed background covariates into a single summary measure (the propensity score), representing an estimate of the probability of receiving the treatment. Since this is the high dimensional data set, it works particularly well to collapse all these covariates into the propensity score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "full_match_propensity_factor = optmatch.fullmatch(optmatch.match_on(Formula('A~propensity_score'),data=highDim_dataset_propensity_R,method='euclidean'),data=highDim_dataset_propensity_R)\n",
    "highDim_dataset_propensity['assign'] = list(full_match_propensity_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratio list below shows ratios of treated to comparison individuals, which is more consistent around 1 than those in the low dim data set, which should result in less variance and is possibly why propensity scores performed better for high dim than low dim. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1]]\n"
     ]
    }
   ],
   "source": [
    "#compute ATE\n",
    "ATE_vec = []\n",
    "weights = []\n",
    "ratio_list = []\n",
    "\n",
    "for i in range(max(list(full_match_propensity_factor))):\n",
    "    temp = highDim_dataset_propensity.loc[highDim_dataset_propensity['assign']==i+1]\n",
    "    \n",
    "    treatment_Y = temp.loc[temp['A']==1]['Y'].values\n",
    "    control_Y = temp.loc[temp['A']==0]['Y'].values\n",
    "    \n",
    "    ATE_vec.append(np.mean(treatment_Y)-np.mean(control_Y))\n",
    "    weights.append(len(treatment_Y)+len(control_Y))\n",
    "    ratio_list.append([len(treatment_Y),len(control_Y)])\n",
    "\n",
    "highDim_propensity_est_ATE = np.average(ATE_vec, weights=weights)    \n",
    "\n",
    "end = time.time()\n",
    "highDim_propensity_match_runtime = end-start\n",
    "print(ratio_list[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8.703'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highDim_propensity_runtime = \"{:,.3f}\".format(highDim_propensity_R_runtime+highDim_propensity_match_runtime)\n",
    "highDim_propensity_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.292\n"
     ]
    }
   ],
   "source": [
    "highDim_propensity_error = abs(highDim_true_ATE-highDim_propensity_est_ATE)\n",
    "highDim_propensity_error =\"{:,.3f}\".format(highDim_propensity_error)\n",
    "print(highDim_propensity_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: Linear Propensity Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defined as:                         \n",
    "                        $$D_{ij} = |logit(e_{i})−logit(e_{j})|$$\n",
    "Obtained by applying the logit function on the propensity scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching on the linear propensity score can be particularly effective in terms of reducing bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Low Dim Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Propensity Score didn't perform as well for low dimension as high dimensional data set, possibly for the same reason discussed above for standard Propensity Score <br>\n",
    "We can see ratio_list print out below that the ratio between treatment and controls in each group is not very consistent around 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "full_match_linear_propensity_factor = optmatch.fullmatch(optmatch.match_on(Formula('A~linear_propensity_score'),data=lowDim_dataset_linear_propensity_R,method='euclidean'),data=lowDim_dataset_linear_propensity_R)\n",
    "lowDim_dataset_linear_propensity['assign'] = list(full_match_linear_propensity_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1], [1, 1], [1, 2], [1, 7], [1, 2], [1, 17], [1, 1], [1, 1], [1, 1], [1, 2]]\n"
     ]
    }
   ],
   "source": [
    "#compute ATE\n",
    "ATE_vec = []\n",
    "weights = []\n",
    "ratio_list = []\n",
    "\n",
    "for i in range(max(list(full_match_linear_propensity_factor))):\n",
    "    temp = lowDim_dataset_linear_propensity.loc[lowDim_dataset_linear_propensity['assign']==i+1]\n",
    "    \n",
    "    treatment_Y = temp.loc[temp['A']==1]['Y'].values\n",
    "    control_Y = temp.loc[temp['A']==0]['Y'].values\n",
    "    \n",
    "    ATE_vec.append(np.mean(treatment_Y)-np.mean(control_Y))\n",
    "    weights.append(len(treatment_Y)+len(control_Y))\n",
    "    ratio_list.append([len(treatment_Y),len(control_Y)])\n",
    "    \n",
    "lowDim_linear_propensity_est_ATE = np.average(ATE_vec, weights=weights)\n",
    "\n",
    "end = time.time()\n",
    "lowDim_linear_propensity_match_runtime = end-start\n",
    "print(ratio_list[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.587'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowDim_linear_propensity_runtime = \"{:,.3f}\".format(lowDim_linear_propensity_R_runtime+lowDim_linear_propensity_match_runtime)\n",
    "lowDim_linear_propensity_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.976\n"
     ]
    }
   ],
   "source": [
    "lowDim_linear_propensity_error = abs(lowDim_true_ATE-lowDim_linear_propensity_est_ATE)\n",
    "lowDim_linear_propensity_error =\"{:,.3f}\".format(lowDim_linear_propensity_error)\n",
    "print(lowDim_linear_propensity_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. High Dim Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Propensity score performed well for high dimensional data set as explained above for standard propensity score. In addition, linear propensity score is effective in reduceing bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "full_match_linear_propensity_factor = optmatch.fullmatch(optmatch.match_on(Formula('A~linear_propensity_score'),data=highDim_dataset_linear_propensity_R,\n",
    "                                                                           method='euclidean'),data=highDim_dataset_linear_propensity_R)\n",
    "highDim_dataset_linear_propensity['assign'] = list(full_match_linear_propensity_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1], [2, 1], [1, 1], [1, 1], [2, 1], [1, 1], [2, 1], [1, 1], [2, 1], [1, 1]]\n"
     ]
    }
   ],
   "source": [
    "#compute ATE\n",
    "ATE_vec = []\n",
    "weights = []\n",
    "ratio_list = []\n",
    "for i in range(max(list(full_match_linear_propensity_factor))):\n",
    "    temp = highDim_dataset_linear_propensity.loc[highDim_dataset_linear_propensity['assign']==i+1]\n",
    "    \n",
    "    treatment_Y = temp.loc[temp['A']==1]['Y'].values\n",
    "    control_Y = temp.loc[temp['A']==0]['Y'].values\n",
    "    \n",
    "    ATE_vec.append(np.mean(treatment_Y)-np.mean(control_Y))\n",
    "    weights.append(len(treatment_Y)+len(control_Y))\n",
    "    ratio_list.append([len(treatment_Y),len(control_Y)])\n",
    "\n",
    "highDim_linear_propensity_est_ATE=np.average(ATE_vec, weights=weights)\n",
    "\n",
    "end = time.time()\n",
    "highDim_linear_propensity_match_runtime = end-start\n",
    "print(ratio_list[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12.074'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highDim_linear_propensity_runtime = \"{:,.3f}\".format(highDim_linear_propensity_R_runtime+highDim_linear_propensity_match_runtime)\n",
    "highDim_linear_propensity_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.232\n"
     ]
    }
   ],
   "source": [
    "highDim_linear_propensity_error = abs(highDim_true_ATE-highDim_linear_propensity_est_ATE)\n",
    "highDim_linear_propensity_error =\"{:,.3f}\".format(highDim_linear_propensity_error)\n",
    "print(highDim_linear_propensity_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Inverse Propensity Weighting Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing the exposure effects between treatment groups, if we ignored those confounding factors, the effect estimates will be biased. Inverse probability weighting (IPW) based on the marginal structure model is an important method that can be used to estimate the effect of observational data processing and can address a very large number of confounding variables. Applying this weight when conducting statistical tests or regression models reduces or removes the impact of confounders. For inverse probability of treatment weighting (IPTW), we use propensity score as inverse weights in estimates of the ATE. \n",
    "\n",
    "The weight $w_i$ is \n",
    "$$w_i = \\frac{T_i}{\\hat{e_i}} + \\frac{1 - T_i}{1 - \\hat{e_i}} $$\n",
    "where $\\hat{e_i}$ is the estimated propensity score for individual $i$; $T_i$ is the treatment groups: $T_0$ is the controlled group and $T_1$ is after treatment group\n",
    "\n",
    "In this project, IPW does not work well in both low and high dimensional datasets.The reason might be the limitations of the Inverse Probability Weighted Estimator (IPWE). It can be unstable if estimated propensities are small. If the probability of either treatment assignment is small, then the logistic regression model can become unstable around the tails causing the IPWE to also be less stable. IPW needs to meet some prerequisites when applying, such as no omissions and unobserved confounding factors, non-negativity assumptions, stable unit processing value assumptions, and correct weight estimation models.\n",
    "\n",
    "The estimate ATE using IPW is \n",
    "$$\\hat{\\Delta}_{IPW} = N^{-1} (\\sum_{i \\in treated}{w_i Y_i} -\\sum_{i\\in controlled}{w_i Y_i} )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reset data & Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowDim_dataset = pd.read_csv('../data/lowDim_dataset.csv')\n",
    "highDim_dataset = pd.read_csv('../data/highDim_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipw_ate(dataset):\n",
    "    treated = 0\n",
    "    controlled = 0\n",
    "    for i in range(dataset.shape[0]):\n",
    "        if dataset['A'][i] == 1:\n",
    "            treated += dataset['Y'][i] * dataset['weight'][i]\n",
    "        else:\n",
    "            controlled += dataset['Y'][i] * dataset['weight'][i]\n",
    "\n",
    "    print(treated - controlled)\n",
    "    ate = (treated - controlled)/dataset.shape[0]\n",
    "    return ate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Low Dim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398.383166304905\n",
      "ATE for low dimension is:  0.8387014027471684\n",
      "Runtime for low dimension is:  0.145\n",
      "ATE error for low dimension is:  1.661\n"
     ]
    }
   ],
   "source": [
    "runtime = time.time()\n",
    "X=lowDim_dataset.iloc[:,2:].values\n",
    "A=lowDim_dataset['A'].values\n",
    "gbm = GradientBoostingClassifier(learning_rate = 0.01, max_depth = 2, min_samples_leaf = 1,\n",
    "                                min_samples_split = 2, n_estimators = 150).fit(X,A)\n",
    "low_dim_propensity_scores = [x[1] for x in gbm.predict_proba(X)]\n",
    "lowDim_dataset_ipw = lowDim_dataset\n",
    "lowDim_dataset_ipw['score'] = low_dim_propensity_scores\n",
    "lowDim_dataset_ipw['weight'] = lowDim_dataset_ipw['A']/lowDim_dataset_ipw['score'] + (1 - lowDim_dataset_ipw['A'])/(1 - lowDim_dataset_ipw['score'])\n",
    "ate_low = ipw_ate(lowDim_dataset_ipw)\n",
    "runtime_low_ipw = \"{:,.3f}\".format(time.time()-runtime)\n",
    "lowDim_ipw_error = abs(ate_low - lowDim_true_ATE)\n",
    "lowDim_ipw_error = \"{:,.3f}\".format(lowDim_ipw_error)\n",
    "print(\"ATE for low dimension is: \", ate_low)\n",
    "print(\"Runtime for low dimension is: \", runtime_low_ipw)\n",
    "print(\"ATE error for low dimension is: \", lowDim_ipw_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. High Dim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3693.534034205346\n",
      "ATE for high dimension is:  -1.8467670171026729\n",
      "Runtime for high dimension is:  0.779\n",
      "ATE error for high dimension is:  1.153\n"
     ]
    }
   ],
   "source": [
    "runtime = time.time()\n",
    "X=highDim_dataset.iloc[:,2:].values\n",
    "A=highDim_dataset['A'].values\n",
    "Y=highDim_dataset['Y'].values\n",
    "\n",
    "gbm = GradientBoostingClassifier(learning_rate = 0.05, max_depth = 1, min_samples_leaf = 5,\n",
    "                                min_samples_split = 2, n_estimators = 100).fit(X,A)\n",
    "high_dim_propensity_scores = [x[1] for x in gbm.predict_proba(X)]\n",
    "highDim_dataset_ipw = highDim_dataset\n",
    "highDim_dataset_ipw['score'] = high_dim_propensity_scores\n",
    "highDim_dataset_ipw['weight'] = highDim_dataset_ipw['A']/highDim_dataset_ipw['score'] + (1 - highDim_dataset_ipw['A'])/(1 - highDim_dataset_ipw['score'])\n",
    "ate_high = ipw_ate(highDim_dataset_ipw)\n",
    "runtime_high_ipw = \"{:,.3f}\".format(time.time()-runtime)\n",
    "highDim_ipw_error = abs(ate_high-highDim_true_ATE)\n",
    "highDim_ipw_error = \"{:,.3f}\".format(highDim_ipw_error)\n",
    "print(\"ATE for high dimension is: \", ate_high)\n",
    "print(\"Runtime for high dimension is: \", runtime_high_ipw)\n",
    "print(\"ATE error for high dimension is: \", highDim_ipw_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an observational study, the treatment effects are often hard to estimate because of potential confounding elements. Therefore a method called Principal Stratification is used to divide data into several stratas and compute causal effects within each strata. Principal effects are defined as a comparison of potential outcomes on a set of common units which is the union of principal strata. Individuals are stratified based on estimated propensity scores and the difference estimated as the average of within-stratum effects. Stratification attempts to achieve groups where individuals share the same propensity scores which is unrealistic in practice. \n",
    "One problem with stratification in estimating propensity scores is that when working under a large sample size, estimation will become biased because of residual confounding. Under this circumstance, increasing the number of stratas used might be one possible solution to consider. In order to reduce bias, the trade-off between bias and variation can be examined using a careful choice of number of stratas. \n",
    "In this project, stratification works well with both low and high dimensional datasets and the default number of stratums was five, which worked appropriately.\n",
    "\n"
   ]
  },
  {
   "attachments": {
    "Screen%20Shot%202020-12-01%20at%204.25.37%20PM.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAC2CAYAAAAIqYNAAAAKrGlDQ1BJQ0MgUHJvZmlsZQAASImVlgdUU9kWhs+9N73QEiIgJfSOFOlSQg9FehWVkFBCiTEQFGyIiCMwFlSkqQM4CKLgqBQZRcSCbVAsYB+QQUEdBwugovIu8AjvvfVmvfV21s751l77/tn35Jy1fgAoDzhCYQosA0CqIF0U5OnCjIiMYuIHAQJwgAJ0gTaHmyZkBQT4gr+N8V4ATa93jae1/r7vv4YsLy6NCwAUgHIsL42bivJpNNu5QlE6AAiaQHNNunCaS1Cmi9ABUT42zQmz3DHNsbN8b6YnJMgV5WEACBQOR5QAAPkjWmdmcBNQHQodZVMBjy9A2Q1lR24ih4dyLspGqamrpvkEynqx/6KT8G+asRJNDidBwrPvMhMEN36aMIWT+X9ux/+O1BTx3G9ooElJFHkFoSsD3bPa5FU+EhbELvWfYz5vpn+GE8VeoXPMTXONmmMex81njsXJoaw55ojmn+Wns0PmWLQqSKIfl+YeLNGPY/tKZkhZKuF4vgd7jrMSQ8LnOIMftnSO05KDfeZ7XCV1kThIMnO8yEPyjqlp87NxOfMzpCeGeM3PFiGZgRfn5i6pC0Il/cJ0F4mmMCVA0h+X4impp2UES55NRw/YHCdxvAPmdQIk+wPcgDvwRT9MYAZsgSmwBB4A7UyPWzt9poHrKmGmiJ+QmM5kobcmjskWcE2MmOamZrYATN/B2b/4w4OZuwUxCPM1Iapvi55bpHq+FqsEQAt6LhSJ8zWtIwBIRwDQnM0VizJma5jpLywgAWlAB4pAFWgCPWAMzIEVsAfO6MTewB+EgEiwAnBBIkgFIrAGrAebQR4oALvAPlAGDoFqUAuOg5OgBZwFF8AVcAPcBvfBY9APhsBrMArGwSQEQXiICtEgRUgN0oYMIXPIBnKE3CFfKAiKhGKgBEgAiaH10BaoACqCyqBKqA76BToDXYCuQT3QQ2gAGoHeQ19gBKbAdFgF1oEXwTYwC/aBQ+DlcAK8Gs6Cc+EdcAlcBR+Dm+EL8A34PtwPv4bHEICQEQaijhgjNogr4o9EIfGICNmI5CPFSBXSgLQhXchdpB95g3zG4DA0DBNjjLHHeGFCMVzMasxGTCGmDFOLacZcwtzFDGBGMd+xVKwy1hBrh2VjI7AJ2DXYPGwxtgbbhL2MvY8dwo7jcDgGThdnjfPCReKScOtwhbgDuEZcB64HN4gbw+PxinhDvAPeH8/Bp+Pz8KX4Y/jz+Dv4IfwnApmgRjAneBCiCAJCDqGYcJTQTrhDeEmYJMoQtYl2RH8ij5hJ3Ek8TGwj3iIOESdJsiRdkgMphJRE2kwqITWQLpOekD6QyWQNsi05kMwnZ5NLyCfIV8kD5M8UOYoBxZUSTRFTdlCOUDooDykfqFSqDtWZGkVNp+6g1lEvUp9RP0nRpEyk2FI8qU1S5VLNUnek3koTpbWlWdIrpLOki6VPSd+SfiNDlNGRcZXhyGyUKZc5I9MnMyZLkzWT9ZdNlS2UPSp7TXZYDi+nI+cux5PLlauWuyg3SENomjRXGpe2hXaYdpk2RMfRdelsehK9gH6c3k0flZeTXywfJr9Wvlz+nHw/A2HoMNiMFMZOxklGL+PLApUFrAVxC7YvaFhwZ8GEwkIFZ4U4hXyFRoX7Cl8UmYruismKuxVbFJ8qYZQMlAKV1igdVLqs9GYhfaH9Qu7C/IUnFz5ShpUNlIOU1ylXK99UHlNRVfFUEaqUqlxUeaPKUHVWTVLdq9quOqJGU3NU46vtVTuv9oopz2QxU5glzEvMUXVldS91sXqlerf6pIauRqhGjkajxlNNkqaNZrzmXs1OzVEtNS0/rfVa9VqPtInaNtqJ2vu1u7QndHR1wnW26bToDOsq6LJ1s3TrdZ/oUfWc9FbrVend08fp2+gn6x/Qv20AG1gaJBqUG9wyhA2tDPmGBwx7jLBGtkYCoyqjPmOKMcs4w7jeeMCEYeJrkmPSYvJ2kdaiqEW7F3Ut+m5qaZpietj0sZmcmbdZjlmb2XtzA3Ouebn5PQuqhYfFJotWi3eLDRfHLT64+IElzdLPcptlp+U3K2srkVWD1Yi1lnWMdYV1nw3dJsCm0OaqLdbWxXaT7Vnbz3ZWdul2J+3+sje2T7Y/aj+8RHdJ3JLDSwYdNBw4DpUO/Y5MxxjHnxz7ndSdOE5VTs+dNZ15zjXOL1n6rCTWMdZbF1MXkUuTy4SrnesG1w43xM3TLd+t213OPdS9zP2Zh4ZHgke9x6inpec6zw4vrJeP126vPrYKm8uuY496W3tv8L7kQ/EJ9inzee5r4CvybfOD/bz99vg9Waq9VLC0xR/4s/33+D8N0A1YHfBrIC4wILA88EWQWdD6oK5gWvDK4KPB4yEuITtDHofqhYpDO8Okw6LD6sImwt3Ci8L7IxZFbIi4EakUyY9sjcJHhUXVRI0tc1+2b9lQtGV0XnTvct3la5dfW6G0ImXFuZXSKzkrT8VgY8JjjsZ85fhzqjhjsezYithRrit3P/c1z5m3lzcS5xBXFPcy3iG+KH44wSFhT8JIolNiceIbviu/jP8uySvpUNJEsn/ykeSplPCUxlRCakzqGYGcIFlwaZXqqrWreoSGwjxh/2q71ftWj4p8RDVpUNrytNZ0Omp2bor1xFvFAxmOGeUZn9aErTm1VnatYO3NTIPM7Zkvszyyfl6HWcdd17leff3m9QMbWBsqN0IbYzd2btLclLtpKNszu3YzaXPy5t9yTHOKcj5uCd/SlquSm507uNVza32eVJ4or2+b/bZDP2B+4P/Qvd1ie+n27/m8/OsFpgXFBV8LuYXXfzT7seTHqR3xO7p3Wu08uAu3S7Crd7fT7toi2aKsosE9fnua9zL35u/9uG/lvmvFi4sP7SftF+/vL/EtaS3VKt1V+rUssex+uUt5Y4VyxfaKiQO8A3cOOh9sOKRyqODQl5/4Pz2o9KxsrtKpKq7GVWdUvzgcdrjrZ5uf62qUagpqvh0RHOmvDaq9VGddV3dU+ejOerheXD9yLPrY7eNux1sbjBsqGxmNBSfACfGJV7/E/NJ70udk5ymbUw2ntU9XNNGa8puh5szm0ZbElv7WyNaeM95nOtvs25p+Nfn1yFn1s+Xn5M/tbCe157ZPnc86P9Yh7HhzIeHCYOfKzscXIy7euxR4qfuyz+WrVzyuXOxidZ2/6nD17DW7a2eu21xvuWF1o/mm5c2m3yx/a+q26m6+ZX2r9bbt7baeJT3td5zuXLjrdvfKPfa9G/eX3u/pDe190Bfd1/+A92D4YcrDd48yHk0+zn6CfZL/VOZp8TPlZ1W/6//e2G/Vf27AbeDm8+Dnjwe5g6//SPvj61DuC+qL4pdqL+uGzYfPjniM3H617NXQa+HryTd5f8r+WfFW7+3pv5z/ujkaMTr0TvRu6n3hB8UPRz4u/tg5FjD2bDx1fHIi/5Pip9rPNp+7voR/eTm55iv+a8k3/W9t332+P5lKnZoSckScGSuAoAnHxwPwHvUJ1EgAaLcBIEnNeuSZgGZ9/QyBv+NZHz0TVgBUdwAQkg2AL7qWoqsOmtLOAASgGeIMYAsLSf4z0uItzGe1yC2oNSmemvqAekO8PgDf+qamJlumpr7VoMM+AqBjfNabT4cM6v+dqSw/r7DzeFvwn/EPcjMC2tHtwkQAAACKZVhJZk1NACoAAAAIAAQBGgAFAAAAAQAAAD4BGwAFAAAAAQAAAEYBKAADAAAAAQACAACHaQAEAAAAAQAAAE4AAAAAAAAAkAAAAAEAAACQAAAAAQADkoYABwAAABIAAAB4oAIABAAAAAEAAAJeoAMABAAAAAEAAAC2AAAAAEFTQ0lJAAAAU2NyZWVuc2hvdGTHNUcAAAAJcEhZcwAAFiUAABYlAUlSJPAAAAHWaVRYdFhNTDpjb20uYWRvYmUueG1wAAAAAAA8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIiB4OnhtcHRrPSJYTVAgQ29yZSA1LjQuMCI+CiAgIDxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+CiAgICAgIDxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSIiCiAgICAgICAgICAgIHhtbG5zOmV4aWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20vZXhpZi8xLjAvIj4KICAgICAgICAgPGV4aWY6UGl4ZWxYRGltZW5zaW9uPjYwNjwvZXhpZjpQaXhlbFhEaW1lbnNpb24+CiAgICAgICAgIDxleGlmOlVzZXJDb21tZW50PlNjcmVlbnNob3Q8L2V4aWY6VXNlckNvbW1lbnQ+CiAgICAgICAgIDxleGlmOlBpeGVsWURpbWVuc2lvbj4xODI8L2V4aWY6UGl4ZWxZRGltZW5zaW9uPgogICAgICA8L3JkZjpEZXNjcmlwdGlvbj4KICAgPC9yZGY6UkRGPgo8L3g6eG1wbWV0YT4KsEnVDwAAABxpRE9UAAAAAgAAAAAAAABbAAAAKAAAAFsAAABbAAAl6tN4pXYAACW2SURBVHgB7J1pzGVF0cfPgCHBoJ9UjPpFIZqo0UAQt7ghxBg1SMAlLoiQIUoAERCBwIwogjKOGHUUHYQ4TgBR3JeooKIYlFGjyGJENkVFFLdx3679O2/qvHX7nn2753mefyU3fe5Zuqv/p093dXVV9bpZoEQkBISAEBACQkAICAEhMDgC6yR4DY6xChACQkAICAEhIASEQIqABC81BCEgBISAEBACQkAIjISABK+RgFYxQkAICAEhIASEgBCQ4KU2IASEgBAQAkJACAiBkRCQ4DUS0CpGCAgBISAEhIAQEAISvNQGhIAQEAJCQAgIASEwEgISvEYCWsUIASEgBISAEBACQkCCl9qAEBACQkAICAEhIARGQkCC10hAqxghIASEgBAQAkJACEjwUhsQAkJACAgBISAEhMBICEjwGgloFSMEhIAQEAJCQAgIAQleagNCQAgIASEgBISAEBgJAQleIwGtYoSAEBACQkAICAEhIMFLbUAICAEhIASEgBAQAiMhIMFrJKBVjBAQAkJACAgBISAEJHipDQgBISAEhIAQEAJCYCQEJHiNBLSKEQJCQAgIASEgBISABC+1ASEgBISAEBACQkAIjISABK+RgFYxQkAICAEhIASEgBCQ4KU2IASEgBAQAkJACAiBkRCQ4DUS0CpGCAgBISAEhIAQEAISvNQGhIAQEAJCQAgIASEwEgISvEYCWsUIASEgBISAEBACQkCCl9qAEBACQkAICAEhIARGQkCC10hAqxghIASEgBAQAkJACEjwUhsQAkJACAgBISAEhMBICEjwGgloFSMEhIAQEAJCQAgIAQleagNCQAgIASEgBISAEBgJAQleIwGtYoSAEBACQkAICAEhIMFLbUAICAEhIASEgBAQAiMhIMFrJKBVjBAQAkJACAgBISAEJHipDQgBISAEhIAQEAJCYCQEJHiNBLSKEQJCQAgIASEgBISABC+1ASEgBISAEBACQkAIjISABK+RgFYxQkAICAEhIASEgBCQ4KU2IASEgBAQAkJACAiBkRCQ4DUS0FMv5re//W3y73//O5fNPffcM5nNZsk999yzcP3+979/ct/73nfhvE4IgWUg8N///nehnT7wgQ9Mdt111zl28u7jhgc/+MFz9+mPEBACQqBvBCR49Y3oCs1vw4YNyRVXXJHcdNNNWQ3ud7/7Jc997nOTbdu2Jb/4xS+SM888M7nkkkuy64985COTiy++OHnKU56SndOBEFgmAn/961+T17zmNclHPvKRjI1PfOITySGHHJL95+DPf/5zcswxx8zdt++++yY7duxIdtlll7l79UcICAEh0CcCErz6RHOF5/Wb3/wmedCDHpTV4vvf/36yzz77ZP9vvfXWZO+9907/n3POOclJJ52U7Lbbbtl1HQiBqSDw5S9/OXnOc56TsnPAAQckV111VS5rGzduTN785jcnZ511VnLGGWdI6MpFSSeFgBDoEwEJXn2iucLzQtv1mMc8Jq3FiSeemGzevDmr0Y9//OPkGc94RvofDcJTn/rU7JoO+kOAJd1169b1l+Eazenzn/988vznPz+r/Y033pg8+tGPzv7bwRvf+MbkvPPOS3bu3JnssccedlqpEBACawCBZfW3ErzWQOOqW8UtW7Ykxx57bHr71VdfnTz96U9Pjz/60Y8mL33pS7Nlxwc84AF1s9R9FQj85z//SW644YbkG9/4RvKVr3wl+frXv56mT3ziEyue1OUyBF73utcll112WWbvdcIJJyTnn3/+wiOPf/zjk0c96lHJ5ZdfvnBNJ4SAEFi9CGDTjKLhPve5T/LkJz852W+//ZIXvOAFyUMf+tDBKy3Ba3CIV04BaAjQFED/+Mc/UmP7k08+OXn/+9+fnHvuuckpp5yipZieXucdd9yRsFyLcIC2xRMC2IEHHuhP6bghAnvttVc6WbjllluSj33sY+nTf/rTnxLsFo1+9atfJQ95yENSO69XvOIVdlqpEBACawCBv//978nuu+++UFOEr/Xr16ca88FWH4KqTSQEZsEoeRZaYPp70YteNLvuuutmwXh+Fmy+ZkEbI4R6QiDY0c2CbVyGtWFOevDBB88uuuii2R//+MeeSlub2fz0pz9N8b3yyitnX/va1zKsP/CBD8wBcumll6bXggA2d15/hIAQWBsIfPvb354df/zxszAhy/oJ65Of9rSnzYKJwiBAECZAJARmwfg4a3gIWzS+Jz3pSbNf//rXQqcnBL761a8ufODBk272uc99bha87HoqRdl88IMfTNvvX/7yl1kIG5FOIGjPwcYr/W8IHXnkkTPwFwkBIbC2EQgrPLPPfvazM4QtE7wsfctb3jLXb/SBlASvPlBcBXmcdtppCw3uCU94wizYIK2C2i2/Clu3bl3A96ijjko1jcvnbnVxcOihh84OOuigrFJhqTzD3rS3CGRMMII3Y3afDoSAEFjbCPzrX/+aBS/nrL8w4euII46Y/fOf/+wNHAlevUG5sjN63OMelzY2lhdPPfXUrOGFuF0ru2IT4D54zmV42od8wQUXTICz1ccCnSMYb9q0KascS7eGe3ASSc8Hh4b03LXXXpvdpwMhIASEAAgE57J0Ymb9BmmIaTkLdmG9ACTBqxcYV3YmIThqNjCFMBKpjZEtN5JOaRksRNifYZNjP2zT8ghbKrvHUtTJY9P73ve+DFv7iEOw2rHZWDPlXXPNNSne3/3ud+fqfNxxx2Xvgfb+3ve+N132ZYYrEgJCQAjECHjzG+u7Q3Dm+LZW/yV4tYJtdT0Uonxng9IXvvCFtHIhWn12DtXrVIilIRMK+RhCmIBc1kJgzLn7eCbEIsu9d6iT3rDbPtypCbJD1X1Z+dI+MJQNruJzLJiGi/dw9tlnz573vOfNgifj3D36IwSEgBDwCLzyla/MxkHrw2MnHX9/3WOFkwhornU6/PDDs61TwrJMwv6L7GUXjOvTLVTA5+c//3nysIc9bBJQsa8k++8ZxWEC7HzwakuCrU8af+zjH//4qHtKsnXNwx/+8CyOlPFEaIPDDjvM/irtGYH9998/xZ3YczE961nPSuOkEVKCEB7bt29PXv7yl8e36b8QEAJCIEXg7rvvTtgaLw75c9ttt6X9TGuY6kpoum91IoBmwFxpQzT6uUqyzh0aVvpD8p8K3XvvvRlf8IcXWx7hLcj1EJQ07/Kg597+9rfP8Wg4yllhONhtyfzCCy/MLSTsRTr3TkKnmnufTgoBISAEDIEQ12uu36Av7zoejrrUyKCDfQuG3GMv+xiISucR+Na3vpU1qjwPr7DfXXadmCdTIBtAbckxDhNgPGKvhlA5th3P7373u0yYNYGLFFdl0XAImPfiT37yk9xCMLy3NoPHrkgICAEhUIVAnsc//fkPf/jDqkcLr48meBFTh8CcfiAKe/4VMqYL4yAQNhDO3kmeLdd73vOe7PojHvGIGcbtyyYMHBk4veG6hQnwvOGhSciGsSksYWWY+fZeZI82Nn+rsbyg+p/RPsE7bO5eWEXsu7gnb5JR+JAuCAEhsGYR8DbQvj/H+78tjSJ43XXXXWmgQphmoH/nO9+ZDUy42o+tkWgL1mp67tZbb80CS/rGxPvBUzDYTc2e+cxnZu/J7kGDtAxhxmOP1gLj+T/84Q8ZfxYmwO4LNmnptWDbZadGS1/84hdnfBlupG9729tG42EtFRS2WJrDmzZahLUtR6LpFQkBISAEqhAg8LXvx+2YiV5bGlzwwq3b1PtHH330zFz6f/CDH2QzVAZ7RUhv+wrX1nM33XRT+hFgfwaFTb2zj4JB1ci8Mln2G5OwmbMPM05PP/30MVlRWQUIYPvXZzDEgmJ0WggIgVWAgNkKx/05/9merA0NKngFD65sENq8efMCfwyKZriGcKZghgsQ6USEAPGXaPB/+9vf0is/+tGPsjbGMpIRxo/LsKliApH3gXKurxgwVscpp8R+27JlS6qVnDKf4k0ICIHlI4BCZqrOLkVLjfTphAxqQ4MIXmzHwf5GNgCxB1IRYXDv78VAViQEihAg/hIRhD3ZkijCO5oM2hTLTXgWjk1eELT2b+kyBMGx648tJ3aB4E+9WfIVCQEhIASKECDYNX04fQZmA1OjvC2ErE+/9NJLW7Hbu+CFfRCBCWGMgbCu5f+XvvSlrLN+1ateNaMDFwkBjwBti3Z1/vnn+9Mz7LjsQ8Bhg2Vs/pOOTcyAjJc4pWNhUrJa6fLLL09tOU3oov4SvFbr21a9hEB3BDAdwVbqmGOOmbFCRt9xzjnnTGqP4CKbXfo3VmDaUK+CF1uzhKCb6cCD19kvf/nLRjzdfvvtmRE+ISfarp82KlQ3rxgEzMgxFuZ9mADsBVnWRuhfRswsH/ssFrxWuyDC94qNmzd2l+C1Yj4vMSoERkXAlC1oyI0IBUN4oIMPPnj2+9//3k4vNcU7Pq8v51zbFbreBC8GwxDZPGUQCbGtxgqtxvr169N8kH4xbBMJARDAfZc2kSdQ+eVqhC7a0DLo+uuvL/xI+VCnqErvGyccaqyjkuDVN7rKTwisfARsr1Tbos7XCPtQVs3QhNGfLpPYFNv6srz0sssua8VeL4IXkcGNKdz8+1hO+dCHPpTlWRSZvFWN9dCKRYCZR9H+ehYmwNphnRhxhDFhxnLnnXf2hknMh/FjqZ/d9VboxDKS4DWxFyJ2hMBEEGB1gmVFlDTYw5aRCWeXXHJJ2W2DXiuz2aVPR2vXhnoRvNB2oYnoGyA6cNSOirnT5tWurmfsA7jooosKK0YsLxNw6oSRINAm97/pTW8qzLPphaoZ0lrwbJTg1bTV6H4hsDYQYCcUtqarGz6KsZ8VjKK4fEOjht2qjSl5aVuNXC+C19CVV/5rG4GwcffskEMOST+AMo0R0ev5OOp6D6LxYqf5n/3sZ70CjH1i3kfKuXg/zF4LnkhmErwm8iLEhhCYGALY6VoooLqsYTt+yy231L291/uYlBf15Zy3uKRNC13HAyGDXil4diVhqSd517veley666695q3M1hYCwdU42WuvveZ2hz/00EOT4Mm4AARN+bGPfWxy+OGHJ2FHhIXrY5147Wtfm1xwwQW5xQXNcBJ2Bci9tsyTQQBNgjFrIxYOO+ywZO+991545nvf+16y3377peeDjVcSlhUW7tEJIbCaEAhLaOn3s+eee66maq35urzkJS9JgtYrF4d99903oa9rQ4MIXvvvv3+yY8eO5JOf/GTywhe+sA1femYABL74xS8mYTm495yD+jgJtle959smQz4EOr+ywT5oupKwbVUSNmpPwnJjEjb/Tnbfffc2xeU+s3Xr1iTs0pB7jZPBOSDZZZddCq/HF4LNJJrp+HTlpIZneDaPKH/dunXZJTqYYJ+W/a9zELSPyT777LNw61iC16tf/eokeFEulN/1RLD7S/bYY4+u2Sw8X/Y+Fm6uODHkhJb2WUZx2ym7P763LN8u1+pgWxezoJVJwj6wuZO7PB7D/rVJ2Ic4HYRROBx44IF5t+lchACT6qDJSpicMRllEhdMi9L+O7q10d/gaJXstttuycknn1zZR1Zl/OxnPzuhPeRRcOBKgv153qXqc6HB9ko+jhEhJUTTQaBs64PQUkpVqmXXjzzyyOlUsiYnO3fuTL1mhmij1113XSmW7DHZhF72spfl5nfeeeeVZoM9XNF7I+zLUDTWUmOYcRbWr6jedc6zj+kQ5D1v6/BRds9Qe5D6/ruofOxejPBeL7qP8wSfHIPe/e53l/IBLzhslREe9SeddFKaz4knnlh2a3ZtJcShypidwAHhZq644ooZgbCL2g02XfRdbZ30Lr744jRvQgv5beTaVB8b8yI+w6pGmyzTZ3q38SKquGcUuxvRdBDAuNu/H47xzLj33ntr/YgyTGBS8zjheT6ilUa4LMP7aaed1jvrFug1xtn+Nw2xQGd18803p7FtLA9SPIOqNpgnlh4BCf1zmzZtmvEeh6KxBK+gsZyrF3UkAnbQQNRqy7R5PFoJ8eH7LQTnIQj7Pv8eeH9smYZgHQdpxFnp9a9//ey4446bhaX1bL9be54BfyiibYSl+jleKRf7yqJJgzm/GH/YOWLPU9U++6wDDjUmOBkfpNiH0lbKBvKgccliUNY15F4pcaj6xLhLXvRh/hugjW/YsCEVxK666qp0izFvH8u30TQWqPFn2/wgxN1xxx12unHK874t+eMu/USvghfejZ4xjgFPNB0EmKHGUjzxUuh4mhIfEu94CK1RU16a3s+gAO9h+bXpo7XuL9PGIBi0oTztQp04d/65M844o03RjZ4ZS/CCqe3bty/0OW3Dzxx11FGDtQmCQVrfiGdXLHwTz8iukyKMecIg2e8IUiZE+OfaHiMwxYMOE4AiYrJh9xP2pa7XWlF+bc/jKONx5BjnnDLi+0Bg5966QpdNPKceh6qs3mNei7Xvxx9//CzP85x3EZZts3fI2NJWeDePRNrjPffc06q6jI1xe7L/tPm21KvgxcbExpRPq+J1tGVez7VD4MYbb1x4T7y7NnT66aenmpc2zy7zGdt/q2gG35W3PM2ifRN5AWDrlHfXXXctvLd438q8fPyEqKvqPS//+NyYghdls9Rt2Fraps9BA8bzzJb7JvarJW8mosQyigntlvFOum3btviW1LOLawiIYxCDo+eJ91pE9j2hxeiiYSjKv+75eOeIqn4NYdK0nWjuq77NlRaH6tprr50FJ7c0dhZLb2hZEZCrfn16erPFm29HLLmXEV6M/n6WkdsSE03yQoBDqGtKeMh7XuyYiXUX6k3w4mMzpuL0iCOO6MKjnh0AAVsH9++qzYBjwXMHYHHQLJnhDqmpY4Nuj60dM4NqS6Y+t7wsZZuNMtqyZUvKS90wG2V51blG7B3jrc8OvKhslo3jbT3Q6rbpaGkX8V6gReU2OY8Qg1BStHQS84+QHRMzf3BtGy07zq/qf6yFO/fcc3Mf8cIOx8uks846K2t7YFW1ibG3DaujFVkpcaioC+OufYc+rRK6uN7Xd4sdqi/7DW94Q63mYdpne7bOu8nLmGVzy4OdT5pScJLKnrd8SFn+70K9CV4YI8IQnXte7ItYtd6FaT3bDwK2dOEbFLYQTYjBrYshZJOy+rqXZRvqPOSy24c//OHcD/bYY49tXQ17X2wi799ZVWfGvmfcXxYDrTVTOQ+effbZGX/eEDvn1t5Oea2eYdMmWC1CI4F1+yYGkiIhIJ60IjTmkdkOohEYg/i2DUvSvBh0aFAZqLnO8tuyicmU57kMK2JD2b3x0m5RPTBRmHocKswnEPKpG+k73vGONEp82yW7IiyqzsOH4UvK5KIuduy3659tGyEeHrHjtbyCx3UV23PX2bjbnvVpl36cAnoRvLBZMaawObn77ruz/3b+lFNOmauQ/iwfAWwf4jVsVKh1P47l16AdB6aRGXLfxOCOvvAN8C3k2YTUqQVLItaZojXx743zRdodAvzZNzj0pvOf/vSnM1sZKxPemB2y5DE0EQzXyrWUjnPqFNu/lM3M85Yph6xf7H3m7XLoJ/COBeuxlj/L6uq1G/CEoXYZeY3QULaeZeUPcQ1NqfUTCAdtNUVdebv99tszPuxb/OY3v1k7W3bBsedIER7bEvaGlhfLrU2I9m7P+hQ7tC7Ui+D11re+NWUOidbWyNmPyTPKsf9ouzCtZ/tDwLbN8e8Ke5PVTNgY0Dm1jTpcBxvfqRu2aDLKDJTL8jW7KRtM0F5ZvqQsHefRNddck97XZYkzL9+pnsvzDrztttumym7KV8wzHl5ToXgCQSgAI2vjCF9TmKzF2okyjTaaOvt+0Ni11QbRf2KQj3Y1BE6esUdslTG/4dd3isMFS6HUa9mKjlhgx7O0CcV2YbS1LmRaf7BBO96EwNLaiqXYynWhzoIXM22TsFleMcLuxJi0FLsX0fQQYInA3pGlGAOvJmLJIQT0TauEVg+38yHJa6T6wJTOnXxMGxLPxIqMPc3mZej6Dollk7zxHqRTNMxJEQzG1hTV5ZkB3/pP43kKQozx75fj4C8EjUwv2VIQQssYDhvGT1kKb4YhKfanReRtMLG/a0osWdmyJgoHH2uPPWOXQWiUqDd9wZCTyqq6md2vfxdNlwrR1vnnq5wkqnjyIXWamiCwwhB/o/DWpd13FrxMvc8HyAbBnrxbKIxyz5Q6Fc+rP2ZNGM1C19/QSzue5y7HzJRsL0Rr7DS0PAPfLuUs81kz8mQmhVCEB9tQhPGz4Wgps+EuZO7ufnk09pzMW86zpSD/XBc+VsKz3/nOdxbwHyJeWx9YxLxOMfyOn0TQh3uj+7w21wcuTfOgD4M3+95I4/HI53nQQQdl9+L114RwMrBysG02od5/j0VOFE3KaXpv2I0j5WsI55AmvPhYXeDERKippt+/H/IgrlwXQvCzd0Y7aUrYZ9rzlnYRBjsJXszW7KPMW4MNW7EsMFsVPbgpIEPcD6AGbpe0S4C1IepVlieCSKwpYLBvq4IvK2sZ19CE4MGEdm9Iu4dYQ0D7weGkbBCowoOQF9YO8eAzIpCtnSfF6N6T14p1iTnj81wpx6aR8fhMUfg0Mw3jEw/UqRHOG8afT7FNmwrdcMMNczyytFREfIu+HkVOD/HzmNH4EBvYtfl4ahaWgryX4UxmS9bsaoA2psuvbb8fO4qARZuVrliI7ipMIgj7d04/3ZQIZuzz4LitzW4nwcuvqTOw5VEcBwO1bFPpNy/fIc+hnUDj0/U3hHfUkPU2o3PfuMba8qOqXp6nIY+r+Ci7jqCDmt/zhxDfVeixGFB5hqHx7NIHrvzUpz6V8lI2CJXVZyVfY5CM7UzozHH8mRLF76+pV/EYdbnyyivn2jTtu6tXV998+7AQ8Bf22ywsIt7xgG2S6pD31kcj44UTdhKw776r/U8dXvLuYWw1HrqmGMe3IQtd48tvqv2Ld0Egr66TJlbaPE9tnG4Qss10w/KiTyEuZlPqJHjZIFOmBszbH5ABQTRNBLwa3RrXFLZ9Ml6GTtu+FWY+8SyNmZ6fEbfN22bZeTPHeFnT32N2EpgDrEVCsxm/E4yPzQFo2Zj4aPa062UN2FU4sK2S/+4Y4G15rerZsa57bRO8lsW2iyeYdQZOnAo8BmjYjGhnfrVgqH00rbyi1GzOWKnAk7jLz0/gisrLOx+bF2Hq0JQsNJXHu8uKgZXv8ysTzO3+opSdMXxeHG/durXo9tzzrQUv23KFQsvUqnRysSTe5mXkcq+TvSOANtI8Y6xxMXgVhSvonYGCDI2XodOC4gtPo1qPg/3BY5/OCbacn7d0TYfkDT8ZAGwmboPBUJs+F4IyoQt5hr5TWc4j/IZvz1PTItlrjF37zzzzTLs0iTSON1YlwMbKgCrTA7wUvQDPTglGeA378a3LxsmWZ9vUDPzbLn+1Ldc/Z8Kfteum3oisDvj+jHzqxljzfOQd+3yroufnPe/Pxd8ufLLUi9NFnYlda8HLZhi+EXrG/DGu7vYiLG0S08PnpePhEcAD0Hc0U7D1snYzdFoHXUI7oHLG+SLmBxf2PjegNpsxOg0TqGIe/RII/CD04djBMYPCWifbzsbeFUtnUyDTSBpfdOZTJAvca3yiMZoSsQxlvJFWCbDe0Jr7q5af4+UlliqZBHljepwisLlcJhmfpMsiP26ALcGUm1CeEXtfDhw2EYUvvBy7Ekuo8VZf5A0GtEFiwxX12a0EL2/UW2dPNGbl8QuZovdO1xexmp63Pa4Y8FeTd2Mf7yhuy3xsGzZsmLXd/LqMJ/MaLgvYF28MjMbywgsvTAejZcfzKavbWNfYAJ53xI+9RadCfiCAt6H2De1SXwYOrykomwB0KafLs3GcpSoB1mLbWZso8z7HVCB+T4YHKRqdpnGhutS17Fk/SStbhSrLo+s1w8awreu4YOXGNuFl/Z49Uzf1vNE/9kWMj3mG92BQtHNCK8HLvP4wXq1LmzZtyjo/eyl+nbwoH7QHaMf4mLjf1nqRJoda/sJmjWCXXX9tPCeKcBjzvH3AvCfU8qJ5BPI0XXQQdSYh8zlV/7MwH9gVlFFsW2HLH221Ozt27Jg1scUcMjxHWb3rXLNgzhiyT8U2yX9jfGcMOH0RTh59BfGM7aFiz9k+eO7Kr7V1G1eKHL2M15tvvnluLCoTnOJ7WfZiz1RsyNrYb7Y1Wjfeq1ILFMoqxVDjYxkPsbMIQm5dMicie4+kVWMo8cp27txZWUS8HN1nkGImJ9u3b59rU1aHImeZxoIXDccyLQtQFyMRG5KSR9k2E3SQaBGsLD4uk1ht4BtCwwDfJlha2W3TPJucGJep/cf7w5wm1krQzabvANU3wrnZXvn2Qcdc9LE1Lcd3FmXGwuTrbS49P03j5uGJa5133UCD5sbedHbbFI8295vnNX0H9nhTIdNk2rsiQG4fZIbw1LcPYdj3wfDa996bXfmNtb11BFgGasOdtMx5KF7GLNOOVb2/E044IS13yKVAQkiYhg7NPELimFEEYnvXuvZm9HXGt72bKicF7MFs9aGqf/RyC/n3YfeKjELQ+Hgc4NsDBwJ2FwWybSx42ZomRnRNicHKQLW0aBkL4zfuQeuEqy6E0ZrFvaGyQxE2M7zQrr+VFk4CPO39InyZdnEonLvmy6weVa79ipZq4vu4v84sqQ5/zIjz2jXttCuZETAdUhUx+45n/k3CSKB1QIDyedQVvKzzm1qQUrYKsskaW7lMiUyTaf0gdoN9EKERLM8++h+b5FqetJM+qSu/8Wb0eGXXIa+ZKfP6je3ByurPQFzUB8GTLaM13T6nTn38PWjevRDDGEq/jtYcL2j6FQSisl9bbVm8CwpbTtUhG3esndURTn1YkKq9NuOYokW2V3V45R4EcHA1fkn5VhDi6wi6jQQvvxl2lTSaVwHWnT2jHDNoxYQka/chsceE0NeXp0Oc91r+7z01qlS8U8CJNmjaOdoLH0IeMUv393FvmzgueXnbOQYQE0Cs7WJgWecjtDzi1PbCq+PAwrPxvnpNXJwRRG0LDBuU6gpen/nMZ2bwOKU9EZlpmodVlbF1jPvQ/70mk7aCcNilnXh+GVCYtGLa0ZXQClhbJh3CG70rv/ESe13Df5vYU6+y9uHtmbn36quvzoUVbSr44IhQRAjXlDWESUJcJm1s48aN2cTDv8c6x22XRAnw7PtB+t2qJdk4hBEyQR3PQOqMppjwE1WxEv3YVkcrGuPp/7PSZxM6wxJlTZF2yz9rx40ELzwUKAhtU1uJ0QYTY5g0nkX4xp4nyRJJuWskWwNA6f8hgMrfGtMUl4yK3hMDlgkKtKWiZQNb6qZTMAGjKM+257FHxLbCt20GhqbLfZSPJtjyqeuB4yPc82zbzhPHF56vK3i1xWvI50wLySy0qlMeko+8vGMBuWjCkPfsmOdiLUSZUDEmX1YW3oj2jVhaN1int90qEyhjIZl35fsPxkHCbdB3ElS1L9s6q2PXFP7YlgobJEJdsHUagk7Vr8pOroyvbdu2zb0XohrkEd8lwqG9O9KhQr34tlzlfJHHq50zpyXPcxuvy9qCFw3QBmYKZUmizc/nYczTGDwRvM2uYcAfzwZZO+1Dje7LXMvHfJwmvJTZ3U0VI6/NKtKEItzTpoaOxI/AFLdxhLG6gz9tnQ/ZL/HQ2dPe6yz9MqO277Pt+2ojeNWtX1uemjzHEor1H2am0OT5Ie5l1s+AHQtdxidaQ78dVFce6moM4nJof2gu4xAlxieD+BDvugm/9Fc4f8T7+cEjk/K6Hn0EGLV6lQVRJdim3Wcpy/KMTbakd/TRR9f6PuOxLMZ/NfynrXuNIpgh+KDxox++/vrrZwhjhh3X6cPbCDDgVdUe6TetT0Zp1PYdmOLJ2gBp2y0QawteLNf4Avs8zrPXYhZiZRDaQDQcAhY6AkG6z85/OI7/P2eb9Xr1tp+R2p2mai7zYLJ7u6Ys8VnbtZSOpw7FNiv2PCnCWBWxjMG9XRwj6ghedKIMzgi6fL9533AVr0Nc591bJ0vnPhXyM27/TuNjQoG0IbQuvHPiK1J/NBxtKA6eHPNn/3Gf70Jd+GV3BuOjKMWWqYoQ3ux5BKciQihkL2K716fgXdanoHVmwGalh+93Kt9JUV37PI9dpbcZ9bjZMeM8e34iTNclFC9otOmnaOts8VdG8GHlNTG/8HnmeVzihNeWagtebQto+5wNIAbYUCrItvytlue8105ZB1JV36p1/Krn2163gHuEHLG2khe0j4GijpF6Wz78c3TUeR1O29AOPu86xwxMXTTCdQQvBhTs2hhIwH39+vV1WBv0HjpvW+rtsiy2rLbcBRy0BQxA9g0UOS11KaPPZ6fCrxeIi2IuWb2JcI+xPUtp2JLVWY7D/IBJn00GqoQEK2s1pRvdcqKZCWFjW8cmNE8TiikSwpe19apJhvVn9P+s3DUl3qG9PyuTtItn5GQFL8CJtWzMkkT9IUBHY5qiLntXmR3EGEajce2ZSbJMCpnXEHWK4zUhIHSdpcdll/1nBu0/Uo7beAKXlTHUNeuo6th4mYBZ1fkNxavP1yJ386672NpgU1e0ZO3Lm9qxbRQ9VZuxGK8p8MsyFXjxfaLBHYKYENjA3SQ23hC8jJ0nttjWDxLypgkxuePd5NlzM3ZZvmVhYhDw7b624Z1OPfXULA/Li/fZhSYteFGxzZs3z1W6L7frLqCthmdZ5zYbCWwduszy8S6kQcZOEkPjxGwIIQt7AshiNsGLDx9g3lljBoON4zTZB9tn4L6h8K0reNkyL3XDOWOZ5Pdk7NJH0Kbo7JmZrzTC5oh3MaXo/GUYToVf7ABNMGoS8LOsbv4adnH2/dfRkvlnV+oxDkVeA0v9mXjibIQnItryotAbfIPYafJOWIrMC/1jm5aXrWIQhcEUC22XGJnA2bvzaZNQPXnv8H8AAAD//2D+CF0AABP4SURBVO2dB8wUxRfAB2sURMSI2IgtisSAwUYsEVREBFsURI09KogYsWus2LEQI1hoakjAhigqRFEUFRFRgoqV2BAVEZSIIjb2P2/8z/nd3e5+u9d2bu43yZf7bqe9+b252bezb2ZUUAfh8ssvD5RS5q9Tp051ILH7It56662G57bbbhv89NNPJQv87bffBrvsskuwySablFxGqRnfe+8904bZs2ebIv7444+gXbt25trBBx+cK3bcuHHm2qpVq3LXqv3P5MmTTZ2239rPIUOGVLvqsss/8sgjjewDBw6MLWvKlCkmnfShLMOyZctyeh8xYkRZotx9992mTWPHji2rnFpnXr16da6/vfrqq7WuPnV9rsk7f/58M4bJ77TS/G655Rajm+7du6fmVI8ZPv/886Br1665/mjHvrBPuZ+fddZZwbBhwwLhdP755wc77rijydu5c+fIe9PQoUNNmnPOOScUkdzTDjzwQJNmzJgxoWmSXJw6dWpoO0477bQk2SPTqMiYDCJ+++23YPHixUU1r127NpBOaxX3zTffFKXhQnICYqhYltZoSZ7735RicI0fPz43WGUxqNx1112mHWJw2SA/YNu2Dz/80Fzu379/0KtXL5ukJp/3339/Tg4rj3xmbaQkaXxSw+uiiy4ybYwa/JLUVW6af/75J+jTp4+RQz5lrEgbxAiQG69tt+hp1qxZaYvJNP3MmTNz/W3NmjWZypKkchflnTNnTm48e+mll5I0I1EaawAMHz48Ufp6TvT000/nGNrxrun4l/R/MaziJgTEKJOyJk6cWIRLbAgx6CT+iSeeKIpPc0EelMNk7tmzZ5piitI6ZXjJYCcWb1h44403cgBmzJgRloRrCQisWLHC3PxtZ5IOmubPzm7Z/PYzSm8JRCo5iXR+uVk2DWIQWpkuvPDC4K+//jIDwciRI5smq/r/V155ZU4OK4/9/P3336tefzkVWAOkuRkvO/hNmjSpnOrKymuNb2Ers65p+rKktU/XVjf2U/pRPYWrrrrK9Ldjjz22LsR2Vd7vvvvOjCkyc16JsHLlytw4sGDBgkoU6XQZMiaffvrp5kFGHook/P3334E8BD/00EOBPKTJPcT+zpp+ykPpKaecEnzyySexbVy+fHkuf9hEzdFHH20mar788svYcpJE2oe6pnLK/zJulBOcMrxkej/KkhTAtvEfffRROW1u2LwyGyCd0nKs5Oe9995bU66//vqraUeYQSUzXLZt8uQq/zf3Y6608NKPrQyFn0uWLKl0dRUtL4nhJU+jtl1hg19FBYoobO7cuTkZrCyV+ixl5ixCzJpctq927GuVprPANREgZSUuyyu6X7p0acoWhSd/7rnnTB8VQ04MkXqYjQxvSWWv/vzzz8G7775rZpbldxw3u1VYs2UaZfz8+OOPxtgrzFfKd9tPw8aVNDIX1u2U4WV9uRYtWlQoZyBP1dJ46cAyi0FIT+DRRx+t2o1q+vTp6QUqI4fUJ/1h4cKFRaXIzKn9oUh/qfXrPRlgbf1hn7U2AosANXNBXsuK3HEzXtOmTTNpoga/ZqqoSHTUbFUY8zTX9t5774rIV6tC5EZj2/fFF18E4qcW9kBSK3maq6fe5G2uPXHx9nW8+DFJu6VvYXzFEWs+ztoJceNT86UkSxE1Oye/N3ldXmpoIRl1IU6Eo446Sj377LNKd0711FNPKX3DNHLpG5U66KCDlHaiVY8//rjq16+fE/LWmxCffvqpeuyxx6oitv4RKG3kVKXssEL1u3fTF/STqWrRokVeEunSu+++u9Izo+a6fuWotNN1XppqfhHOHTt2jKzi+++/V+3bt4+MzzJC+zupli1bGhH0oKPkt1fIVyL1q1R12223KdG79mfLRGTtjKv0a4yK173bbrvV1RijZw7UXnvtZTjom5Lp93rhg1p33XUrzqYSBdabvOW02d7TBgwYYPQyatQodcABB5RTZMPn3WeffdS8efOUnoxRwrWawdYVVsedd96pLr744rCo5q+VarFVI588wcpqNPHV0JIHvXv3Nn/yv/yNHj26GtVSZp0R+PPPP83Mp/gDRIUHH3wwNwsgszO1DM3NLLr4xCt+GHoQy/P/k9+cPKHLU3thkOsS/+STTxZG8b3GBOS1ux0z5RW3+BW5HOpN3nJYikO9vX+V6+hdjhy+5G3qMyf+eNUOYoNY/RV+ynhZanBqxktmY0444QSlXyWap4P3339faSdXM3sglufWW2+t205oZAIywyFPGjLjoh2JzcxoGA+9dYRq3bq1idIDfW4WJyxtpa9px2Glt+sILVbfINUvv/wSGlcvF7V/hmrbtq0RVzi3atWqXkT3Vk7tb6Jk9ldm68JmKF1reL3JWw4/uY916NBBtWnTppxiyKsJPP/886pv375KrxRVr732WtWZnHrqqWrChAmh9cgbOb3DQmhcsxdLtdjIB4EsCOy///55TyDyPWp7EVnVWOttJIRJ3FNSt27dssBW0Tq1G4DRgX7lX9FyKQwCEIBAHAEZ07VRE9x3331xySoWd/311+fdb6Tupn+yS0ApwakZr2atRBJAIAUBmS0Vfyrr/5Iia1lJZaZNZoLCQpY+UWHyJLmml2+r8847z/heXnrppWZWWnwt9fJ41aVLlyRFkAYCEIBAagJvv/220nszKr1dkTrkkENyft96qwi18cYbpy4vbQZ5CxfnR6Y32zX+52nLxfBKS4z0EIghoPfoih0Q9GozNXjw4JgS3IuSBS/iJKz37VJ601xjyIoBpv1X3BMWiSAAAW8IXHLJJUrv1ae0n6nSPuDGANObtCq9LVJN2qhPR1F77LFHZF2lOvhjeEUiJQIC6QnIDFucL+LLL7+s9AKS9AVnmEN8LvVyeCUD3g477KD0BolKVorWgy9RhtioGgIQKJOAPn5IDRo0SL311lvmwe/qq69Whx9+eJmlJs/edJV3WK5SH6QxvMJoenBNnFetA3SWzdHHQKlHHnlEnXzyyWrTTTfNUpSa1P3xxx8rvSN6ZF3y+jPOMIvM2OARekNQpf0pMmfXaP25wbsdzYeA2mmnnZTeHy+UxA033KCuvfba0LjYi6U4hpHHbQLieKiVHmi/nMwElXM3ZTd7u8w9ygE+MwGrVLE9uFv4F/4Ji3rbEb1KmFIVK8zkeB/hWckz9NII0aj9OQ0j0kLARwJxp71oo6ukJju1c31JLSBTEQHZ30puUrLXUhZBO16b0+mt0SWyNIrhJWf8FRpc9ruswCSkJ2CPhxKOtT6aSqRt5P6cXlvkgIBfBPTrzcgxvdQTInjVqEdz34I+kNTsdSV7kPTo0aPmzZP38ttvv7165ZVXlN7Q0dQv+53YkwhqLlANKxR/qA022CC0RvGN0gfFhsZxMZ6ArC6SHc9l/7bNNtssPnGFYxu5P1cYJcVBoO4IPPzww+qMM84IlVtviquOP/740Li4ixhecXSIK4tA06NBGsXwEmBbbrmlOd6qEJ6szJEVOoT6JNCo/bk+tYXUEKgMAbuqO6w0cfrfd999w6Jir2F4xeKp30h9rI5ab7311DrrrJNZIxr1RiV7XoWdX3jdddcpvSFfZvqo94r14eOZnj/YqP253vsN8kOgHAKyV1fYmyM5hUQWscl9Nm3A8EpLzNH0so2BrB6U1Rdz585VckzFkiVL1DbbbJOZxI16o5o+fbo64ogjirjLlgxjx44tus6FcAKzZ89WL7zwgjmoe86cOYapPoMzPHENrjZqf64BWqqAgLMEojZRLcd1BMPLWXWnE0zOz1u4cKEaMWKEmjJlivGnktd7ceH1119XegVkXJKiOEmf1MemUW9Usomq+LgtW7Ysj59e7KBkJ2ZCMgKLFi1SL774otk0UXLow8fNrvlRuenPUWS4DgEIlErgmmuuUTfddFNRdnkoPOyww4quJ7rg1/oDWjNkyBCzAuPss89uFsa4ceMiV2vozhMal+ZE+HfeeSdXRqOsarTQR48enWt7U5b6kG+bhM8EBObPn5/jqA3Z2Bz051g8REIAAiUQ6NOnT24MsmN59+7dSyjpvyzMeCUyT+sn0a677qo+++wzNXHiRHXiiSfGCr506dLIjeGiMsq5h1Gr9grzNOqMl3CQ1Y0dO3Ys4lvWU1Ih4Ab4LosR5NiQJLOF9OcG6BA0EQI1JPDVV1+Z0zoKq5Q3FzImlRowvEol52A+ebUlK+okfP3116pDhw6ZStnIhpeAlx9n4YoXOW9MXglvtNFGmeqmXiqXqfwZM2aYg3Jlyj/L0Oj9OUv21A2BLAj0799fyZYRTYMcW3TjjTc2vZT6fwyv1MjczTB16lRzeKjc3GXvoawDNypl9u0688wz81QhKxtlhSMhnoAcz9OqVSuTSJzru3XrFp+hyrH05yoDpngIOERAztU99NBD8ySSw7knT55c9upqDK88rPX9xZ7kPnDgwNDtDApbJ6vvbr755sLLsd+feeYZtfnmm8emsZHcqP4lccUVV6jbb7/dYjGfDzzwgDr33HPzrvEln4A41vfq1UslXbZNf87nxzcIQKA0AvPmzVN9+/bNWyAlD37iKtK6devSCm2a6z93L/6rdwJdu3Y1ToCTJk1K1JRSnJG1H02isiVRIzvXF0K64447ihw0+/XrF+jVqIVJ+f5/ApdddplhdtJJJ5kra9asiWVDf47FQyQEIJCAwPjx44vGahmDVq9enSB3siTMeDW1Quv4f9lOom3btqYFSXeJX7VqlVq+fHniVstGcdttt13i9G+++abS5xOa9IsXL06VN3EldZRQtkMoXPAgxyjJrOMxxxxTmSepOuLRnKhdunQx+9FNmDBB7bzzzuqee+5R+qEiMhv9ORINERCAQAwBbS6pBQsWqFGjRin9AJeXctiwYUr8ulq0aJF3vawvyewzUrlOYNq0acZK1/5dzoiq9z7JPTnIQcOEIJBDtLXDZo6L/vHm/pfDzfVUdqBXRIJKE7Bshg4dGnTu3Dn44YcfMuVCf84UP5VDoKIE1q5dG+iFO8EFF1wQtGvXLjfe2HFHtozQRwJVtE5bmLL/8FnfBPThwabjyD5eWQftBxZIp7UdWD61n05w3HHHBdpJOmvxnKh/1qxZwYABA/IYWV4yGBCCoHfv3oaPPEx88MEHmSGhP2eGnoohUDUCeqPr0PFXv6UxD8BVq1gXzKtGfbfzIey5555KbzapZs6cGXqulA9t9LENK1asMCcNyGGr4tApRz3JUTn77befj81N1SbZC0223ujUqZPacMMNU+UlMQQgAIE4Atpn1Gzro2fTVc+ePZWeLDDjrnXZictbbhyGV7kEHcivX8Go9u3bJ1795YDIiBBBQO9sX9KhqxHFcRkCEIAABCIIyJY1LVu2jIit3mUMr+qxrWrJ2u9FrVy5Uo0ZM8YcvDxo0CA1cuRINXjw4KrWS+EQgAAEIAABCJROAMOrdHaZ5ZSViFtssYWpXw4S7tGjh9pqq63MK6r1118/M7moGAIQgAAEIACBeAIYXvF8nI0dPny4knPs2rRpY3bXle9ZTJk6CwjBIAABCEAAAg4SwPByUCmIBAEIQAACEICAnwQwvPzUK62CAAQgAAEIQMBBAhheDioFkSAAAQhAAAIQ8JMAhpefeqVVEIAABCAAAQg4SADDy0GlIBIEIAABCEAAAn4SwPDyU6+0CgIQgAAEIAABBwlgeDmoFESCAAQgAAEIQMBPAhhefuqVVkEAAhCAAAQg4CABDC8HlYJIEIAABCAAAQj4SQDDy0+90ioIQAACEIAABBwkgOHloFIQCQIQgAAEIAABPwlgePmpV1oFAQhAAAIQgICDBDC8HFQKIkEAAhCAAAQg4CcBDC8/9UqrIAABCEAAAhBwkACGl4NKQSQIQAACEIAABPwkgOHlp15pFQQgAAEIQAACDhLA8HJQKYgEAQhAAAIQgICfBDC8/NQrrYIABCAAAQhAwEECGF4OKgWRIAABCEAAAhDwkwCGl596pVUQgAAEIAABCDhIAMPLQaUgEgQgAAEIQAACfhLA8PJTr7QKAhCAAAQgAAEHCWB4OagURIIABCAAAQhAwE8CGF5+6pVWQQACEIAABCDgIAEMLweVgkgQgAAEIAABCPhJAMPLT73SKghAAAIQgAAEHCSA4eWgUhAJAhCAAAQgAAE/CWB4+alXWgUBCEAAAhCAgIMEMLwcVAoiQQACEIAABCDgJwEMLz/1SqsgAAEIQAACEHCQAIaXg0pBJAhAAAIQgAAE/CSA4eWnXmkVBCAAAQhAAAIOEsDwclApiAQBCEAAAhCAgJ8EMLz81CutggAEIAABCEDAQQIYXg4qBZEgAAEIQAACEPCTAIaXn3qlVRCAAAQgAAEIOEgAw8tBpSASBCAAAQhAAAJ+EsDw8lOvtAoCEIAABCAAAQcJYHg5qBREggAEIAABCEDATwIYXn7qlVZBAAIQgAAEIOAgAQwvB5WCSBCAAAQgAAEI+EkAw8tPvdIqCEAAAhCAAAQcJIDh5aBSEAkCEIAABCAAAT8JYHj5qVdaBQEIQAACEICAgwQwvBxUCiJBAAIQgAAEIOAnAQwvP/VKqyAAAQhAAAIQcJAAhpeDSkEkCEAAAhCAAAT8JIDh5adeaRUEIAABCEAAAg4SwPByUCmIBAEIQAACEICAnwQwvPzUK62CAAQgAAEIQMBBAhheDioFkSAAAQhAAAIQ8JMAhpefeqVVEIAABCAAAQg4SADDy0GlIBIEIAABCEAAAn4SwPDyU6+0CgIQgAAEIAABBwlgeDmoFESCAAQgAAEIQMBPAhhefuqVVkEAAhCAAAQg4CABDC8HlYJIEIAABCAAAQj4SeB/GTd7idcPNZkAAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for the Stratifictaion algorithm is:\n",
    "![Screen%20Shot%202020-12-01%20at%204.25.37%20PM.png](attachment:Screen%20Shot%202020-12-01%20at%204.25.37%20PM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to stratify data \n",
    "def stratify(df):\n",
    "    \n",
    "    Y = df['Y']\n",
    "    D = df['A']\n",
    "    scores = df['propensity_scores']\n",
    "    \n",
    "    # Create stratum and stratum limits\n",
    "    Q1 = np.quantile(scores, .20)\n",
    "    Q2 = np.quantile(scores, .40)\n",
    "    Q3 = np.quantile(scores, .60)\n",
    "    Q4 = np.quantile(scores, .80)\n",
    "    Q5 = np.quantile(scores, 1.0)\n",
    "    \n",
    "    quin1 = df[df['propensity_scores']<= Q1]\n",
    "    quin2 = df[(df['propensity_scores']> Q1) & (df['propensity_scores']<= Q2)]\n",
    "    quin3 = df[(df['propensity_scores']> Q2) & (df['propensity_scores']<= Q3)]\n",
    "    quin4 = df[(df['propensity_scores']> Q3) & (df['propensity_scores']<= Q4)]\n",
    "    quin5 = df[df['propensity_scores']> Q4]\n",
    "\n",
    "    quintiles = [quin1, quin2, quin3, quin4, quin5]\n",
    "    Q_ranges = [None, Q1, Q2, Q3, Q4, Q5]\n",
    "\n",
    "    return [quintiles, Q_ranges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to calc ATE\n",
    "def strat_ATE(quintiles, Q_ranges):\n",
    "    results = []\n",
    "    N = sum([len(quintiles[0]),len(quintiles[1]),len(quintiles[2]),len(quintiles[3]),len(quintiles[4])])\n",
    "    \n",
    "    for i, stratum in enumerate(quintiles): \n",
    "        i+=1\n",
    "        \n",
    "        Nj = len(stratum)                      # Number of ind in stratum\n",
    "        N1j = stratum['A'].value_counts()[1]   # Number of treated ind\n",
    "        N0j = stratum['A'].value_counts()[0]   # Number of control ind\n",
    "        \n",
    "        sum1 = 0\n",
    "        sum2 = 0\n",
    "\n",
    "        # Summation of treated samples within strata\n",
    "        sum1 = sum([Y*T for Y,T in zip(stratum['Y'],stratum['A'])])\n",
    "        # Summation of untreated samples within strata\n",
    "        sum2 = sum([(1-T)*Y for Y,T in zip(stratum['Y'],stratum['A'])]) \n",
    "\n",
    "        results.append(Nj/N * ((sum1/N1j)-(sum2/N0j)))\n",
    "\n",
    "    return sum(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowDim_dataset = pd.read_csv('../data/lowDim_dataset.csv')\n",
    "highDim_dataset = pd.read_csv('../data/highDim_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Low Dim data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowDim_scores = pd.read_csv('../output/low_dim_propensity_scores.csv') \n",
    "lowDim_scores.insert( 1 , \"Y\" , lowDim_dataset['Y']) \n",
    "lowDim_scores.insert( 2 , \"A\" , lowDim_dataset['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated ATE:  2.463529123502176\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "quintiles , Q_ranges = stratify(lowDim_scores) \n",
    "\n",
    "lowDim_stratification_ATE = strat_ATE(quintiles ,Q_ranges)\n",
    "end = time.time()\n",
    "lowdim_strat_runtime = end - start\n",
    "print( \"Estimated ATE: \" , lowDim_stratification_ATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.036\n",
      "0.030\n"
     ]
    }
   ],
   "source": [
    "lowdim_strat_runtime = \"{:,.3f}\".format(lowdim_strat_runtime)\n",
    "#lowDim_stratification_ATE = \"{:,.3f}\".format(lowDim_stratification_ATE)\n",
    "#print(lowDim_stratification_ATE)\n",
    "lowDim_stratification_error = abs(lowDim_stratification_ATE-lowDim_true_ATE)\n",
    "lowDim_stratification_error = \"{:,.3f}\".format(lowDim_stratification_error)\n",
    "print(lowDim_stratification_error)\n",
    "print(lowdim_strat_runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. High Dim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Calculated Propensity Scores \n",
    "highDim_scores = pd.read_csv('../output/high_dim_propensity_scores.csv') \n",
    "highDim_scores.insert( 1 , \"Y\" , highDim_dataset['Y']) \n",
    "highDim_scores.insert( 2 , \"A\" , highDim_dataset['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "quintiles , Q_ranges = stratify(highDim_scores)\n",
    "\n",
    "highDim_stratification_ATE = strat_ATE(quintiles ,Q_ranges)\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010\n",
      "0.018\n"
     ]
    }
   ],
   "source": [
    "highdim_strat_runtime=\"{:,.3f}\".format(end-start)\n",
    "#highDim_stratification_ATE = \"{:,.3f}\".format(highDim_stratification_ATE)\n",
    "#print(highDim_stratification_ATE)\n",
    "highDim_stratification_error = abs(highDim_stratification_ATE-highDim_true_ATE)\n",
    "highDim_stratification_error = \"{:,.3f}\".format(highDim_stratification_error)\n",
    "print(highDim_stratification_error)\n",
    "print(highdim_strat_runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have a table of all of the absolute errors and the runtimes for each method across the low dimensional and high dimensional datasets. Note that this notebook was run on a Mac with 64 GB of RAM and an i9 CPU, so the runtimes may be different if you run this notebook on your system.\n",
    "\n",
    "We can see that the overall best method on the low dimensional dataset and the high dimensional dataset in terms of runtime and performance is stratification. The reason is because…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Metric            </th><th>Dimension  </th><th style=\"text-align: right;\">       Full Matching-\n",
       "Mahalanobis</th><th style=\"text-align: right;\">       Full Matching-\n",
       "Propensity score</th><th style=\"text-align: right;\">       Full Matching-\n",
       "Linear Propensity Score</th><th style=\"text-align: right;\">      Inverse Propensity\n",
       "Weighting</th><th style=\"text-align: right;\">  Stratification</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>ATE Absolute Error</td><td>Low Dim    </td><td style=\"text-align: right;\"> 0.414</td><td style=\"text-align: right;\"> 0.888</td><td style=\"text-align: right;\"> 0.976</td><td style=\"text-align: right;\">1.661</td><td style=\"text-align: right;\">           0.036</td></tr>\n",
       "<tr><td>                  </td><td>High Dim   </td><td style=\"text-align: right;\"> 1.446</td><td style=\"text-align: right;\"> 0.292</td><td style=\"text-align: right;\"> 0.232</td><td style=\"text-align: right;\">1.153</td><td style=\"text-align: right;\">           0.01 </td></tr>\n",
       "<tr><td>Run Time (sec)    </td><td>Low Dim    </td><td style=\"text-align: right;\"> 0.633</td><td style=\"text-align: right;\">28.145</td><td style=\"text-align: right;\"> 1.924</td><td style=\"text-align: right;\">0.145</td><td style=\"text-align: right;\">           0.03 </td></tr>\n",
       "<tr><td>                  </td><td>High Dim   </td><td style=\"text-align: right;\">79.957</td><td style=\"text-align: right;\"> 6.956</td><td style=\"text-align: right;\">48.447</td><td style=\"text-align: right;\">0.779</td><td style=\"text-align: right;\">           0.018</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table = [[\"ATE Absolute Error\",'Low Dim',lowDim_mahalanobis_error,lowDim_propensity_error,\n",
    "         lowDim_linear_propensity_error,lowDim_ipw_error,lowDim_stratification_error],\n",
    "        [\"\",'High Dim',highDim_mahalanobis_error,highDim_propensity_error,\n",
    "         highDim_linear_propensity_error,highDim_ipw_error,highDim_stratification_error],\n",
    "        [\"Run Time (sec)\",'Low Dim',lowDim_mahalanobis_runtime,lowDim_propensity_runtime,lowDim_linear_propensity_runtime,\n",
    "         runtime_low_ipw, lowdim_strat_runtime],\n",
    "        [\"\",'High Dim',highDim_mahalanobis_runtime,highDim_propensity_runtime,highDim_linear_propensity_runtime,\n",
    "         runtime_high_ipw, highdim_strat_runtime]]\n",
    "\n",
    "display(HTML(tabulate.tabulate(table, headers=[\"Metric\",\"Dimension\", \"Full Matching-\\nMahalanobis\",\n",
    "                                               \"Full Matching-\\nPropensity score\", \"Full Matching-\\nLinear Propensity Score\",\n",
    "                                               \"Inverse Propensity\\nWeighting\", 'Stratification'],\n",
    "                                tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stuart E. A. (2010). Matching methods for causal inference: A review and a look forward. Statistical science : a review journal of the Institute of Mathematical Statistics, 25(1), 1–21. https://doi.org/10.1214/09-STS313\n",
    "\n",
    "Chan, D., Ge, R., Gershony, O., Hesterberg, T., & Lambert, D. (2010). Evaluating online ad campaigns in a pipeline: causal models at scale. KDD '10.\n",
    "\n",
    "Stuart, E. A., & Green, K. M. (2008). Using full matching to estimate causal effects in nonexperimental studies: examining the relationship between adolescent marijuana use and adult outcomes. Developmental psychology, 44(2), 395–406. https://doi.org/10.1037/0012-1649.44.2.395\n",
    "\n",
    "McCaffrey, D. F., Ridgeway, G., & Morral, A. R. (2004). Propensity score estimation with boosted regression for evaluating causal effects in observational studies. Psychological methods, 9(4), 403–425. https://doi.org/10.1037/1082-989X.9.4.403\n",
    "\n",
    "Raad, H., Cornelius, V., Chan, S., Williamson, E. and Cro, S., 2020. An evaluation of inverse probability weighting using the propensity score for baseline covariate adjustment in smaller population randomised controlled trials with a continuous outcome. BMC Medical Research Methodology, 20(1), pp.1-12. https://link.springer.com/article/10.1186/s12874-020-00947-7\n",
    "\n",
    "Hansen, Ben B. Full matching in an observational study of coaching for the SAT. J. Amer. Statist. Assoc. 99 (2004), no. 467, 609–618."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
