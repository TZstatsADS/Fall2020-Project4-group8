{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Import Required Packages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the cell below doesn't run then do 'pip install rpy2' or 'conda install -c r rpy2' and 'conda install tzlocal' in Anaconda Prompt\n",
    "#### Also, run pip install CausalInference or conda install -c conda-forge dowhy if you're using Anaconda Prompt\n",
    "#### Change the paths for os.environ below to match your R folder directory and version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rpy2\n",
    "\n",
    "try:\n",
    "    import rpy2.robjects as robjects\n",
    "except:\n",
    "    os.environ[\"R_HOME\"] = r\"C:\\Program Files\\R\\R-4.0.2\"\n",
    "    os.environ[\"PATH\"]   = r\"C:\\Program Files\\R\\R-4.0.2\\bin\\x64\" + \";\" + os.environ[\"PATH\"]\n",
    "    import rpy2.robjects as robjects\n",
    "    \n",
    "import rpy2.robjects.packages as rpackages\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "from rpy2.robjects.vectors import StrVector\n",
    "from rpy2.robjects import FloatVector, Formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run pip install tabulate \n",
    "#### Run 'from causalinference import CausalModel' below if you used pip install, or 'from dowhy import CausalModel' if you used conda install above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "import math\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "from causalinference import CausalModel \n",
    "#from dowhy import CausalModel\n",
    "from IPython.display import HTML, display\n",
    "import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowDim_dataset = pd.read_csv('../data/lowDim_dataset.csv')\n",
    "highDim_dataset = pd.read_csv('../data/highDim_dataset.csv')\n",
    "\n",
    "lowDim_true_ATE = 2.5\n",
    "highDim_true_ATE = -3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Calculate Propensity and Linear Propensity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#low dim grid search (commented out since it takes a few minutes to run)\n",
    "\n",
    "#X=lowDim_dataset.iloc[:,2:].values\n",
    "#A=lowDim_dataset['A'].values\n",
    "#Y=lowDim_dataset['Y'].values\n",
    "\n",
    "#params = {'learning_rate':[0.01,0.05,0.1,0.5], 'max_depth': [1,2,3,4], 'n_estimators':[50,100,150],\n",
    "#          'min_samples_leaf':[1,3,5],'min_samples_split':[2,4,6]}\n",
    "#gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=5).fit(X,A)\n",
    "#gscv.best_params_\n",
    "\n",
    "#output: {'learning_rate': 0.01,\n",
    "# 'max_depth': 2,\n",
    "# 'min_samples_leaf': 1,\n",
    "# 'min_samples_split': 2,\n",
    "# 'n_estimators': 150}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#high dim grid search (commented out since it takes a few minutes to run)\n",
    "\n",
    "#X=highDim_dataset.iloc[:,2:].values\n",
    "#A=highDim_dataset['A'].values\n",
    "#Y=highDim_dataset['Y'].values\n",
    "\n",
    "#params = {'learning_rate':[0.01,0.05,0.1,0.5], 'max_depth': [1,2,3,4], 'n_estimators':[50,100,150],\n",
    "#          'min_samples_leaf':[1,3,5],'min_samples_split':[2,4,6]}\n",
    "#gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=5).fit(X,A)\n",
    "#gscv.best_params_\n",
    "\n",
    "\n",
    "#output: {'learning_rate': 0.05,\n",
    "# 'max_depth': 1,\n",
    "# 'min_samples_leaf': 5,\n",
    "# 'min_samples_split': 2,\n",
    "# 'n_estimators': 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(x):\n",
    "    return math.log(x/(1-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Dimensional Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=lowDim_dataset.iloc[:,2:].values\n",
    "A=lowDim_dataset['A'].values\n",
    "Y=lowDim_dataset['Y'].values\n",
    "\n",
    "gbm = GradientBoostingClassifier(learning_rate = 0.01, max_depth = 2, min_samples_leaf = 1,\n",
    "                                min_samples_split = 2, n_estimators = 150).fit(X,A)\n",
    "\n",
    "low_dim_propensity_scores = [x[1] for x in gbm.predict_proba(X)]\n",
    "low_dim_linear_propensity_scores = [logit(x) for x in low_dim_propensity_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowDim_dataset_propensity = lowDim_dataset.copy(deep=True)\n",
    "lowDim_dataset_propensity['propensity_score'] = low_dim_propensity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowDim_dataset_linear_propensity = lowDim_dataset.copy(deep=True)\n",
    "lowDim_dataset_linear_propensity['linear_propensity_score'] = low_dim_linear_propensity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'propensity_scores':low_dim_propensity_scores}).to_csv('../output/low_dim_propensity_scores.csv')\n",
    "pd.DataFrame({'linear_propensity_scores':low_dim_linear_propensity_scores}).to_csv('../output/low_dim_linear_propensity_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Dimensional Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=highDim_dataset.iloc[:,2:].values\n",
    "A=highDim_dataset['A'].values\n",
    "Y=highDim_dataset['Y'].values\n",
    "\n",
    "gbm = GradientBoostingClassifier(learning_rate = 0.05, max_depth = 1, min_samples_leaf = 5,\n",
    "                                min_samples_split = 2, n_estimators = 100).fit(X,A)\n",
    "\n",
    "high_dim_propensity_scores = [x[1] for x in gbm.predict_proba(X)]\n",
    "high_dim_linear_propensity_scores = [logit(x) for x in high_dim_propensity_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "highDim_dataset_propensity = highDim_dataset.copy(deep=True)\n",
    "highDim_dataset_propensity['propensity_score'] = high_dim_propensity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "highDim_dataset_linear_propensity = highDim_dataset.copy(deep=True)\n",
    "highDim_dataset_linear_propensity['linear_propensity_score'] = high_dim_linear_propensity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'propensity_scores':high_dim_propensity_scores}).to_csv('../output/high_dim_propensity_scores.csv')\n",
    "pd.DataFrame({'linear_propensity_scores':high_dim_linear_propensity_scores}).to_csv('../output/high_dim_linear_propensity_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Perform Full Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True ATE: 2.5 for low dim and -3 for high dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up rpy2 (Python Interface to R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "utils = importr('utils')\n",
    "utils.chooseCRANmirror(ind=1)\n",
    "packnames = ('optmatch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_to_install = [x for x in packnames if not rpackages.isinstalled(x)]\n",
    "if len(names_to_install) > 0:\n",
    "    utils.install_packages(StrVector(names_to_install))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "utils.chooseCRANmirror(ind=1)\n",
    "robjects.r(f'install.packages(\"{\"optmatch\"}\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optmatch = rpackages.importr('optmatch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Python Dataframes to Python Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with localconverter(robjects.default_converter + pandas2ri.converter):\n",
    "    try:\n",
    "        lowDim_R_runtime = time.time()\n",
    "        lowDim_dataset_R = robjects.conversion.py2rpy(lowDim_dataset)\n",
    "        lowDim_R_runtime = time.time()-lowDim_R_runtime\n",
    "        \n",
    "        lowDim_propensity_R_runtime = time.time()\n",
    "        lowDim_dataset_propensity_R = robjects.conversion.py2rpy(lowDim_dataset_propensity)\n",
    "        lowDim_propensity_R_runtime = time.time()-lowDim_propensity_R_runtime\n",
    "        \n",
    "        lowDim_linear_propensity_R_runtime = time.time()\n",
    "        lowDim_dataset_linear_propensity_R = robjects.conversion.py2rpy(lowDim_dataset_linear_propensity)\n",
    "        lowDim_linear_propensity_R_runtime = time.time()-lowDim_linear_propensity_R_runtime\n",
    "        \n",
    "    except:\n",
    "        lowDim_R_runtime = time.time()\n",
    "        lowDim_dataset_R = pandas2ri.py2ri(lowDim_dataset)\n",
    "        lowDim_R_runtime = time.time()-lowDim_R_runtime\n",
    "        \n",
    "        lowDim_propensity_R_runtime = time.time()\n",
    "        lowDim_dataset_propensity_R = pandas2ri.py2ri(lowDim_dataset_propensity)\n",
    "        lowDim_propensity_R_runtime = time.time()-lowDim_propensity_R_runtime\n",
    "        \n",
    "        lowDim_linear_propensity_R_runtime = time.time()\n",
    "        lowDim_dataset_linear_propensity_R = pandas2ri.py2ri(lowDim_dataset_linear_propensity)\n",
    "        lowDim_linear_propensity_R_runtime = time.time()-lowDim_linear_propensity_R_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with localconverter(robjects.default_converter + pandas2ri.converter):\n",
    "    try:\n",
    "        highDim_R_runtime = time.time()\n",
    "        highDim_dataset_R = robjects.conversion.py2rpy(highDim_dataset)\n",
    "        highDim_R_runtime = time.time()-highDim_R_runtime\n",
    "        \n",
    "        highDim_propensity_R_runtime = time.time()\n",
    "        highDim_dataset_propensity_R = robjects.conversion.py2rpy(highDim_dataset_propensity)\n",
    "        highDim_propensity_R_runtime = time.time()-highDim_propensity_R_runtime\n",
    "        \n",
    "        highDim_linear_propensity_R_runtime = time.time()\n",
    "        highDim_dataset_linear_propensity_R = robjects.conversion.py2rpy(highDim_dataset_linear_propensity)\n",
    "        highDim_linear_propensity_R_runtime = time.time()-highDim_linear_propensity_R_runtime\n",
    "        \n",
    "    except:\n",
    "        highDim_R_runtime = time.time()\n",
    "        highDim_dataset_R = pandas2ri.py2ri(highDim_dataset)\n",
    "        highDim_R_runtime = time.time()-highDim_R_runtime\n",
    "        \n",
    "        highDim_propensity_R_runtime = time.time()\n",
    "        highDim_dataset_propensity_R = pandas2ri.py2ri(highDim_dataset_propensity)\n",
    "        highDim_propensity_R_runtime = time.time()-highDim_propensity_R_runtime\n",
    "        \n",
    "        highDim_linear_propensity_R_runtime = time.time()\n",
    "        highDim_dataset_linear_propensity_R = pandas2ri.py2ri(highDim_dataset_linear_propensity)\n",
    "        highDim_linear_propensity_R_runtime = time.time()-highDim_linear_propensity_R_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Mahalanobis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mahalanobis distance is \n",
    "$$D_{ij} = (X_i-X_j)^T\\Sigma^{-1}(X_i-X_j)$$\n",
    "where $\\Sigma$ is the covariance matrix of $X$ in the pooled treatment and full control groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Low Dim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "full_match_Mahalanobis_factor = optmatch.fullmatch(optmatch.match_on(Formula('A~.-Y'),data=lowDim_dataset_R,method='mahalanobis'),data=lowDim_dataset_R)\n",
    "lowDim_dataset['assign'] = list(full_match_Mahalanobis_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute ATE\n",
    "ATE_vec = []\n",
    "weights = []\n",
    "\n",
    "for i in range(max(list(full_match_Mahalanobis_factor))):\n",
    "    temp = lowDim_dataset.loc[lowDim_dataset['assign']==i+1]\n",
    "    \n",
    "    treatment_Y = temp.loc[temp['A']==1]['Y'].values\n",
    "    control_Y = temp.loc[temp['A']==0]['Y'].values\n",
    "    \n",
    "    ATE_vec.append(np.mean(treatment_Y)-np.mean(control_Y))\n",
    "    weights.append(len(treatment_Y)+len(control_Y))\n",
    "\n",
    "Mahalanobis_lowDim_est_ATE = np.average(ATE_vec, weights = weights)\n",
    "\n",
    "end = time.time()\n",
    "lowDim_mahalanobis_match_runtime = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.409'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#runtime is time to convert to R data frame + time to do matching\n",
    "lowDim_mahalanobis_runtime = \"{:,.3f}\".format(lowDim_R_runtime+lowDim_mahalanobis_match_runtime)\n",
    "lowDim_mahalanobis_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.906\n"
     ]
    }
   ],
   "source": [
    "Mahalanobis_lowDim_est_ATE =\"{:,.3f}\".format(Mahalanobis_lowDim_est_ATE)\n",
    "print(Mahalanobis_lowDim_est_ATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. High Dim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "full_match_Mahalanobis_factor = optmatch.fullmatch(optmatch.match_on(Formula('A~.-Y'),data=highDim_dataset_R,method='mahalanobis'),data=highDim_dataset_R)\n",
    "highDim_dataset['assign'] = list(full_match_Mahalanobis_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute ATE\n",
    "ATE_vec = []\n",
    "weights = []\n",
    "\n",
    "for i in range(max(list(full_match_Mahalanobis_factor))):\n",
    "    temp = highDim_dataset.loc[highDim_dataset['assign']==i+1]\n",
    "    \n",
    "    treatment_Y = temp.loc[temp['A']==1]['Y'].values\n",
    "    control_Y = temp.loc[temp['A']==0]['Y'].values\n",
    "    \n",
    "    ATE_vec.append(np.mean(treatment_Y)-np.mean(control_Y))\n",
    "    weights.append(len(treatment_Y)+len(control_Y))\n",
    "    \n",
    "highDim_Mahalanobis_est_ATE = np.average(ATE_vec, weights=weights)\n",
    "    \n",
    "end = time.time()\n",
    "highDim_mahalanobis_match_runtime = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'48.466'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highDim_mahalanobis_runtime = \"{:,.3f}\".format(highDim_R_runtime+highDim_mahalanobis_match_runtime)\n",
    "highDim_mahalanobis_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.553\n"
     ]
    }
   ],
   "source": [
    "highDim_Mahalanobis_est_ATE= \"{:,.3f}\".format(highDim_Mahalanobis_est_ATE)\n",
    "print(highDim_Mahalanobis_est_ATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Propensity Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Low Dim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "full_match_propensity_factor = optmatch.fullmatch(optmatch.match_on(Formula('A~propensity_score'),data=lowDim_dataset_propensity_R,method='euclidean'),data=lowDim_dataset_propensity_R)\n",
    "lowDim_dataset_propensity['assign'] = list(full_match_propensity_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute ATE\n",
    "ATE_vec = []\n",
    "weights = []\n",
    "\n",
    "for i in range(max(list(full_match_propensity_factor))):\n",
    "    temp = lowDim_dataset_propensity.loc[lowDim_dataset_propensity['assign']==i+1]\n",
    "    \n",
    "    treatment_Y = temp.loc[temp['A']==1]['Y'].values\n",
    "    control_Y = temp.loc[temp['A']==0]['Y'].values\n",
    "    \n",
    "    ATE_vec.append(np.mean(treatment_Y)-np.mean(control_Y))\n",
    "    weights.append(len(treatment_Y)+len(control_Y))\n",
    "\n",
    "lowDim_propensity_est_ATE = np.average(ATE_vec, weights=weights)\n",
    "    \n",
    "end = time.time()\n",
    "lowDim_propensity_match_runtime = end-start\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.336'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowDim_propensity_runtime = \"{:,.3f}\".format(lowDim_propensity_R_runtime+lowDim_propensity_match_runtime)\n",
    "lowDim_propensity_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.388\n"
     ]
    }
   ],
   "source": [
    "lowDim_propensity_est_ATE = \"{:,.3f}\".format(lowDim_propensity_est_ATE)\n",
    "print(lowDim_propensity_est_ATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. High Dim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "full_match_propensity_factor = optmatch.fullmatch(optmatch.match_on(Formula('A~propensity_score'),data=highDim_dataset_propensity_R,method='euclidean'),data=highDim_dataset_propensity_R)\n",
    "highDim_dataset_propensity['assign'] = list(full_match_propensity_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute ATE\n",
    "ATE_vec = []\n",
    "weights = []\n",
    "\n",
    "for i in range(max(list(full_match_propensity_factor))):\n",
    "    temp = highDim_dataset_propensity.loc[highDim_dataset_propensity['assign']==i+1]\n",
    "    \n",
    "    treatment_Y = temp.loc[temp['A']==1]['Y'].values\n",
    "    control_Y = temp.loc[temp['A']==0]['Y'].values\n",
    "    \n",
    "    ATE_vec.append(np.mean(treatment_Y)-np.mean(control_Y))\n",
    "    weights.append(len(treatment_Y)+len(control_Y))\n",
    "\n",
    "highDim_propensity_est_ATE = np.average(ATE_vec, weights=weights)    \n",
    "\n",
    "end = time.time()\n",
    "highDim_propensity_match_runtime = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.467'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highDim_propensity_runtime = \"{:,.3f}\".format(highDim_propensity_R_runtime+highDim_propensity_match_runtime)\n",
    "highDim_propensity_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.292\n"
     ]
    }
   ],
   "source": [
    "highDim_propensity_est_ATE = \"{:,.3f}\".format(highDim_propensity_est_ATE)\n",
    "print(highDim_propensity_est_ATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: Linear Propensity Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Low Dim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "full_match_linear_propensity_factor = optmatch.fullmatch(optmatch.match_on(Formula('A~linear_propensity_score'),data=lowDim_dataset_linear_propensity_R,method='euclidean'),data=lowDim_dataset_linear_propensity_R)\n",
    "lowDim_dataset_linear_propensity['assign'] = list(full_match_linear_propensity_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute ATE\n",
    "ATE_vec = []\n",
    "weights = []\n",
    "\n",
    "for i in range(max(list(full_match_linear_propensity_factor))):\n",
    "    temp = lowDim_dataset_linear_propensity.loc[lowDim_dataset_linear_propensity['assign']==i+1]\n",
    "    \n",
    "    treatment_Y = temp.loc[temp['A']==1]['Y'].values\n",
    "    control_Y = temp.loc[temp['A']==0]['Y'].values\n",
    "    \n",
    "    ATE_vec.append(np.mean(treatment_Y)-np.mean(control_Y))\n",
    "    weights.append(len(treatment_Y)+len(control_Y))\n",
    "\n",
    "lowDim_linear_propensity_est_ATE = np.average(ATE_vec, weights=weights)\n",
    "\n",
    "end = time.time()\n",
    "lowDim_linear_propensity_match_runtime = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.306'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowDim_linear_propensity_runtime = \"{:,.3f}\".format(lowDim_linear_propensity_R_runtime+lowDim_linear_propensity_match_runtime)\n",
    "lowDim_linear_propensity_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.476\n"
     ]
    }
   ],
   "source": [
    "lowDim_linear_propensity_est_ATE = \"{:,.3f}\".format(lowDim_linear_propensity_est_ATE)\n",
    "print(lowDim_linear_propensity_est_ATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. High Dim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "full_match_linear_propensity_factor = optmatch.fullmatch(optmatch.match_on(Formula('A~linear_propensity_score'),data=highDim_dataset_linear_propensity_R,\n",
    "                                                                           method='euclidean'),data=highDim_dataset_linear_propensity_R)\n",
    "highDim_dataset_linear_propensity['assign'] = list(full_match_linear_propensity_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute ATE\n",
    "ATE_vec = []\n",
    "weights = []\n",
    "\n",
    "for i in range(max(list(full_match_linear_propensity_factor))):\n",
    "    temp = highDim_dataset_linear_propensity.loc[highDim_dataset_linear_propensity['assign']==i+1]\n",
    "    \n",
    "    treatment_Y = temp.loc[temp['A']==1]['Y'].values\n",
    "    control_Y = temp.loc[temp['A']==0]['Y'].values\n",
    "    \n",
    "    ATE_vec.append(np.mean(treatment_Y)-np.mean(control_Y))\n",
    "    weights.append(len(treatment_Y)+len(control_Y))\n",
    "    \n",
    "highDim_linear_propensity_est_ATE=np.average(ATE_vec, weights=weights)\n",
    "\n",
    "end = time.time()\n",
    "highDim_linear_propensity_match_runtime = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.301'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highDim_linear_propensity_runtime = \"{:,.3f}\".format(highDim_linear_propensity_R_runtime+highDim_linear_propensity_match_runtime)\n",
    "highDim_linear_propensity_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.232\n"
     ]
    }
   ],
   "source": [
    "highDim_linear_propensity_est_ATE= \"{:,.3f}\".format(highDim_linear_propensity_est_ATE)\n",
    "print(highDim_linear_propensity_est_ATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Inverse Propensity Weighting Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reset data & Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowDim_dataset = pd.read_csv('../data/lowDim_dataset.csv')\n",
    "highDim_dataset = pd.read_csv('../data/highDim_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipw_ate(dataset):\n",
    "    treated = 0\n",
    "    controlled = 0\n",
    "    for i in range(dataset.shape[0]):\n",
    "        if dataset['A'][i] == 1:\n",
    "            treated += dataset['Y'][i] * dataset['weight'][i]\n",
    "        else:\n",
    "            controlled += dataset['Y'][i] * dataset['weight'][i]\n",
    "\n",
    "    print(treated - controlled)\n",
    "    ate = (treated - controlled)/dataset.shape[0]\n",
    "    return ate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_params(X, A, params, dataset):\n",
    "    runtime = time.time()\n",
    "    gscv = GridSearchCV(GradientBoostingClassifier(),params,cv=5).fit(X, A)\n",
    "    print('best_param: ', gscv.best_params_)\n",
    "    print('best_score: ', gscv.best_score_)\n",
    "    gbm_best = gscv.best_estimator_\n",
    "    gbm_best.fit(X, A)\n",
    "    propensity_new = [x[1] for x in gbm_best.predict_proba(X)]\n",
    "    dataset_temp = dataset\n",
    "    dataset_temp['score'] = propensity_new\n",
    "    dataset_temp['weight'] = dataset_temp['A']/dataset_temp['score'] + (1 - dataset_temp['A'])/(1 - dataset_temp['score'])\n",
    "    runtime = time.time()-runtime\n",
    "    runtime_str = \"{:,.3f}\".format(runtime)\n",
    "    print('runtime: ', runtime_str )\n",
    "    return dataset_temp, runtime_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Low Dim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_param:  {'learning_rate': 0.05, 'max_depth': 1, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "best_score:  0.7642105263157896\n",
      "runtime:  53.239\n"
     ]
    }
   ],
   "source": [
    "params = {'learning_rate':[0.001,0.01,0.05,0.1], 'max_depth': [1,2,3], 'n_estimators':[50,100,200],\n",
    "          'min_samples_leaf':[1,2],'min_samples_split':[2,4]}\n",
    "X=lowDim_dataset.iloc[:,2:].values\n",
    "A=lowDim_dataset['A'].values\n",
    "dataset_test, runtime = train_params(X, A, params, lowDim_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "852.5985560478684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.795'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ate_low = \"{:,.3f}\".format(ipw_ate(dataset_test))\n",
    "ate_low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. High Dim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_param:  {'learning_rate': 0.1, 'max_depth': 1, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "best_score:  0.595\n",
      "runtime:  345.671\n"
     ]
    }
   ],
   "source": [
    "params = {'learning_rate':[0.05,0.1,0.5,1], 'max_depth': [1,2,3], 'n_estimators':[50,100,150],\n",
    "          'min_samples_leaf':[1,2],'min_samples_split':[2]}\n",
    "X=highDim_dataset.iloc[:,2:].values\n",
    "A=highDim_dataset['A'].values\n",
    "dataset_test_high, runtime_high = train_params(X, A, params, highDim_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4105.192745053388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'-2.053'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ate_high = \"{:,.3f}\".format(ipw_ate(dataset_test_high))\n",
    "ate_high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowDim_dataset = pd.read_csv('../data/lowDim_dataset.csv')\n",
    "highDim_dataset = pd.read_csv('../data/highDim_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Low Dim data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = lowDim_dataset['Y']\n",
    "D = lowDim_dataset['A']\n",
    "X = lowDim_dataset.iloc[:,2:].values\n",
    "N = lowDim_dataset.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stratification Summary\n",
      "\n",
      "              Propensity Score         Sample Size     Ave. Propensity   Outcome\n",
      "   Stratum      Min.      Max.  Controls   Treated  Controls   Treated  Raw-diff\n",
      "--------------------------------------------------------------------------------\n",
      "         1     0.001     0.995       363       112     0.184     0.404     4.994\n",
      "\n",
      "\n",
      "Treatment Effect Estimates: Blocking\n",
      "\n",
      "                     Est.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "           ATE      2.467      0.113     21.793      0.000      2.246      2.689\n",
      "           ATC      2.467      0.113     21.793      0.000      2.246      2.689\n",
      "           ATT      2.467      0.113     21.793      0.000      2.246      2.689\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohan/opt/anaconda3/lib/python3.7/site-packages/causalinference/estimators/ols.py:21: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  olscoef = np.linalg.lstsq(Z, Y)[0]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "cm = CausalModel(Y=Y, D=D, X=X)\n",
    "    \n",
    "cm.est_propensity_s()\n",
    "cm.stratify_s()\n",
    "print(cm.strata)\n",
    "cm.est_via_blocking()\n",
    "print(cm.estimates)\n",
    "lowDim_stratification_ATE = cm.estimates['blocking']['ate']\n",
    "end = time.time()\n",
    "lowdim_strat_runtime = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.467\n",
      "3.707\n"
     ]
    }
   ],
   "source": [
    "lowdim_strat_runtime = \"{:,.3f}\".format(lowdim_strat_runtime)\n",
    "lowDim_stratification_ATE = \"{:,.3f}\".format(lowDim_stratification_ATE)\n",
    "print(lowDim_stratification_ATE)\n",
    "print(lowdim_strat_runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. High Dim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = highDim_dataset['Y']\n",
    "D = highDim_dataset['A']\n",
    "X = highDim_dataset.iloc[:,2:].values\n",
    "N = highDim_dataset.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stratification Summary\n",
      "\n",
      "              Propensity Score         Sample Size     Ave. Propensity   Outcome\n",
      "   Stratum      Min.      Max.  Controls   Treated  Controls   Treated  Raw-diff\n",
      "--------------------------------------------------------------------------------\n",
      "         1     0.045     0.431       707       294     0.276     0.326    -2.362\n",
      "         2     0.431     0.996       396       603     0.570     0.630    -1.843\n",
      "\n",
      "\n",
      "Treatment Effect Estimates: Blocking\n",
      "\n",
      "                     Est.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "           ATE     -2.948      0.047    -63.044      0.000     -3.039     -2.856\n",
      "           ATC     -2.939      0.050    -58.573      0.000     -3.037     -2.840\n",
      "           ATT     -2.959      0.047    -62.347      0.000     -3.052     -2.866\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohan/opt/anaconda3/lib/python3.7/site-packages/causalinference/estimators/ols.py:21: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  olscoef = np.linalg.lstsq(Z, Y)[0]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "cm = CausalModel(Y=Y, D=D, X=X)\n",
    "cm.est_propensity_s()\n",
    "cm.stratify_s()\n",
    "print(cm.strata)\n",
    "cm.est_via_blocking()\n",
    "print(cm.estimates)\n",
    "highDim_stratification_ATE = cm.estimates['blocking']['ate']\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.948\n",
      "636.758\n"
     ]
    }
   ],
   "source": [
    "highdim_strat_runtime=\"{:,.3f}\".format(end-start)\n",
    "highDim_stratification_ATE = \"{:,.3f}\".format(highDim_stratification_ATE)\n",
    "print(highDim_stratification_ATE)\n",
    "print(highdim_strat_runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Metric          </th><th>Dimension  </th><th>Full Matching-\n",
       "Mahalanobis       </th><th>Full Matching-\n",
       "Propensity score       </th><th>Full Matching-\n",
       "Linear Propensity Score       </th><th>Inverse Propensity\n",
       "Weighting        </th><th>Stratification  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>Best ATE score  </td><td>Low Dim    </td><td>2.906 </td><td>3.388 </td><td>3.476 </td><td>1.795  </td><td>2.467           </td></tr>\n",
       "<tr><td>                </td><td>High Dim   </td><td>-1.553</td><td>-3.292</td><td>-3.232</td><td>-2.053 </td><td>-2.948          </td></tr>\n",
       "<tr><td>Run Time (sec)  </td><td>Low Dim    </td><td>0.409 </td><td>0.336 </td><td>0.306 </td><td>53.239 </td><td>3.707           </td></tr>\n",
       "<tr><td>                </td><td>High Dim   </td><td>48.466</td><td>5.467 </td><td>5.301 </td><td>345.671</td><td>636.758         </td></tr>\n",
       "<tr><td>Computer Used   </td><td>           </td><td>PC    </td><td>PC    </td><td>PC    </td><td>PC     </td><td>PC              </td></tr>\n",
       "<tr><td>Stable/Nonstable</td><td>           </td><td>Stable</td><td>Stable</td><td>Stable</td><td>Stable </td><td>Stable          </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table = [[\"Best ATE score\",'Low Dim',Mahalanobis_lowDim_est_ATE,lowDim_propensity_est_ATE,\n",
    "          lowDim_linear_propensity_est_ATE,ate_low,lowDim_stratification_ATE],\n",
    "        [\"\",'High Dim',highDim_Mahalanobis_est_ATE,highDim_propensity_est_ATE,\n",
    "          highDim_linear_propensity_est_ATE,ate_high,highDim_stratification_ATE],\n",
    "        [\"Run Time (sec)\",'Low Dim',lowDim_mahalanobis_runtime,lowDim_propensity_runtime,lowDim_linear_propensity_runtime,\n",
    "         runtime,lowdim_strat_runtime],\n",
    "        [\"\",'High Dim',highDim_mahalanobis_runtime,highDim_propensity_runtime,highDim_linear_propensity_runtime,\n",
    "         runtime_high,highdim_strat_runtime],\n",
    "        [\"Computer Used\",'','PC','PC','PC','PC','PC'],\n",
    "        [\"Stable/Nonstable\",'','Stable','Stable','Stable','Stable','Stable']]\n",
    "\n",
    "display(HTML(tabulate.tabulate(table, headers=[\"Metric\",\"Dimension\", \"Full Matching-\\nMahalanobis\",\n",
    "                                               \"Full Matching-\\nPropensity score\", \"Full Matching-\\nLinear Propensity Score\",\n",
    "                                               \"Inverse Propensity\\nWeighting\", 'Stratification'],\n",
    "                                tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://projecteuclid.org/download/pdfview_1/euclid.ss/1280841730\n",
    "https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36552.pdf\n",
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5784842/\n",
    "https://www.researchgate.net/publication/8132035_Propensity_Score_Estimation_With_Boosted_Regression_for_Evaluating_Causal_Effects_in_Observational_Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
